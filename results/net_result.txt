C:\Users\dell-pc\Anaconda3.6\python.exe C:/Users/dell-pc/Quant/Quant/scribble_script.py

test period:20160701-20170101
dataset range: 20130101 20160701
2013-01-01 2016-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 102536
<class 'pandas.core.frame.DataFrame'>
Int64Index: 102536 entries, 20130607 to 20160309
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 425.1 MB
None
(0, 8)
Empty DataFrame
Columns: [y_l_rise, y_l_decline, y_l_avg, y_s_rise, y_s_decline, y_s_avg, y_l, y_l_r]
Index: []
Train dates:20130104-20160527
(99886, 2036)

----------Train layer 0----------

Train custom_revenue_y_l_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 38.87s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
2035                                   vol0              10
1536           p9mv_(3k_high-3k_low)/3k_avg              10
1457        p60mv_(20k_avg/p20mv_20k_avg-1)              10
132           (20k_close/p80mv_20k_close-1)              10
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1000            p120mv_upper_shadow/60k_avg              10
1138             p20mv_lower_shadow/20k_avg              10
1061       p180mv_(60k_amt/p60mv_60k_amt-1)              10
927        p100mv_(20k_avg/p20mv_20k_avg-1)              10
1705                                 sh_vol              10
1865                               sz50_vol              10
2025                                 sz_vol              10
1449  p600mv_(120k_low-120k_close)/120k_avg              10
1247      p30mv_(10k_high/p10mv_10k_high-1)               8
509                      (low/p60max_low-1)               2

      importance_percent
2035                7.14
1536                7.14
1457                7.14
132                 7.14
1321                7.14
1000                7.14
1138                7.14
1061                7.14
927                 7.14
1705                7.14
1865                7.14
2025                7.14
1449                7.14
1247                5.71
509                 1.43
Among 2036 features, 2021 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 39.80s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
2025                              sz_vol              16               11.43
895                            hs300_vol              14               10.00
1865                            sz50_vol              11                7.86
342           (60k_high-60k_low)/60k_avg               8                5.71
1705                              sh_vol               5                3.57
2035                                vol0               5                3.57
22         (10k_close/p30mv_10k_close-1)               4                2.86
567                                  amt               4                2.86
1125        p20mv_(5k_avg/p5mv_5k_avg-1)               4                2.86
453                     (high-close)/avg               3                2.14
430              (close/p120max_close-1)               3                2.14
1489     p6mv_(3k_close/p3mv_3k_close-1)               3                2.14
454                       (high-low)/avg               3                2.14
1309  p40mv_(10k_close-10k_open)/10k_avg               3                2.14
468               (high/p250mean_high-1)               2                1.43
1495        p6mv_(3k_low-3k_open)/3k_avg               2                1.43
1107    p20mv_(10k_low-10k_open)/10k_avg               2                1.43
132        (20k_close/p80mv_20k_close-1)               2                1.43
303               (5k_low/p5mv_5k_low-1)               2                1.43
1490      p6mv_(3k_high-3k_close)/3k_avg               2                1.43
Among 2036 features, 1980 features are not used in the model

Train custom_revenue_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 41.02s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
1865                        sz50_vol              26               18.57
342       (60k_high-60k_low)/60k_avg              20               14.29
895                        hs300_vol              14               10.00
454                   (high-low)/avg              10                7.14
468           (high/p250mean_high-1)              10                7.14
1490  p6mv_(3k_high-3k_close)/3k_avg              10                7.14
2035                            vol0               7                5.00
243          (3k_low-3k_open)/3k_avg               7                5.00
1415    p5mv_(5k_high-5k_low)/5k_avg               7                5.00
234          (3k_high-3k_low)/3k_avg               7                5.00
22     (10k_close/p30mv_10k_close-1)               7                5.00
134       (20k_high-20k_low)/20k_avg               6                4.29
1125    p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
1222           p2mv_(vol/p1mv_vol-1)               3                2.14
567                              amt               3                2.14
Among 2036 features, 2021 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 41.06s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                  feature  importance_raw  importance_percent
2025                               sz_vol              28               20.00
895                             hs300_vol              12                8.57
1138           p20mv_lower_shadow/20k_avg              10                7.14
22          (10k_close/p30mv_10k_close-1)              10                7.14
1251    p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
991    p120mv_(60k_high/p60mv_60k_high-1)              10                7.14
988   p120mv_(60k_high-60k_close)/60k_avg              10                7.14
248                (3k_low/p6mv_3k_low-1)               8                5.71
1107     p20mv_(10k_low-10k_open)/10k_avg               8                5.71
548                   (vol/p20mean_vol-1)               6                4.29
1490       p6mv_(3k_high-3k_close)/3k_avg               6                4.29
1865                             sz50_vol               6                4.29
243               (3k_low-3k_open)/3k_avg               6                4.29
1495         p6mv_(3k_low-3k_open)/3k_avg               2                1.43
303                (5k_low/p5mv_5k_low-1)               2                1.43
1219                  p2mv_(low-open)/avg               2                1.43
2035                                 vol0               2                1.43
547                    (vol/p20max_vol-1)               2                1.43
Among 2036 features, 2018 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 39.64s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 50.21s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
80             (120k_high-120k_low)/120k_avg              20
1062        p180mv_(60k_avg/p60mv_60k_avg-1)              12
1321         p40mv_(20k_avg/p20mv_20k_avg-1)              10
125                (20k_avg/p80mv_20k_avg-1)              10
567                                      amt              10
1865                                sz50_vol              10
2025                                  sz_vol              10
88            (120k_low-120k_close)/120k_avg              10
1526         p80mv_(20k_low/p20mv_20k_low-1)               9
1235        p300mv_(60k_low/p60mv_60k_low-1)               9
1511     p750mv_(250k_low/p250mv_250k_low-1)               8
1705                                  sh_vol               6
123                (20k_avg/p40mv_20k_avg-1)               4
988      p120mv_(60k_high-60k_close)/60k_avg               4
184            (250k_high-250k_low)/250k_avg               3
1466         p60mv_(20k_low/p20mv_20k_low-1)               2
343              (60k_high-60k_open)/60k_avg               1
975   p120mv_(120k_high-120k_close)/120k_avg               1
901                    lower_shadow/120k_avg               1

      importance_percent
80                 14.29
1062                8.57
1321                7.14
125                 7.14
567                 7.14
1865                7.14
2025                7.14
88                  7.14
1526                6.43
1235                6.43
1511                5.71
1705                4.29
123                 2.86
988                 2.86
184                 2.14
1466                1.43
343                 0.71
975                 0.71
901                 0.71
Among 2036 features, 2017 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 43.68s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                      feature  importance_raw  \
1865                                 sz50_vol              13
2035                                     vol0              10
342                (60k_high-60k_low)/60k_avg              10
2027                    upper_shadow/120k_avg               9
133              (20k_high-20k_close)/20k_avg               9
82                    (120k_high/250k_high-1)               8
1112          p20mv_(20k_avg/p20mv_20k_avg-1)               7
1480        p60mv_(60k_open/p60mv_60k_open-1)               6
124                 (20k_avg/p60mv_20k_avg-1)               6
2025                                   sz_vol               5
1061         p180mv_(60k_amt/p60mv_60k_amt-1)               5
496                       (low/p250max_low-1)               5
1519      p80mv_(20k_close/p20mv_20k_close-1)               5
1350             p480mv_lower_shadow/120k_avg               5
1525         p80mv_(20k_low-20k_open)/20k_avg               5
1268             p360mv_lower_shadow/120k_avg               5
1705                                   sh_vol               3
430                   (close/p120max_close-1)               3
438                  (close/p250mean_close-1)               3
1005  p1250mv_(250k_high-250k_close)/250k_avg               2

      importance_percent
1865                9.29
2035                7.14
342                 7.14
2027                6.43
133                 6.43
82                  5.71
1112                5.00
1480                4.29
124                 4.29
2025                3.57
1061                3.57
496                 3.57
1519                3.57
1350                3.57
1525                3.57
1268                3.57
1705                2.14
430                 2.14
438                 2.14
1005                1.43
Among 2036 features, 2006 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 40.46s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 44.42s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              20               14.29
2035                                vol0              10                7.14
2025                              sz_vol              10                7.14
432              (close/p120min_close-1)              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
933   p100mv_(20k_high/p20mv_20k_high-1)              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
491                  (low/p120min_low-1)               8                5.71
345         (60k_high/p120mv_60k_high-1)               8                5.71
486                   (low/p10max_low-1)               7                5.00
1865                            sz50_vol               7                5.00
1705                              sh_vol               6                4.29
946     p10mv_(10k_high-10k_low)/10k_avg               5                3.57
1138          p20mv_lower_shadow/20k_avg               4                2.86
399                   (avg/p10max_avg-1)               2                1.43
303               (5k_low/p5mv_5k_low-1)               1                0.71
1250     p30mv_(10k_low/p10mv_10k_low-1)               1                0.71
347         (60k_high/p240mv_60k_high-1)               1                0.71
Among 2036 features, 2017 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 41.60s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
1865                            sz50_vol              15               10.71
2025                              sz_vol              11                7.86
454                       (high-low)/avg               9                6.43
342           (60k_high-60k_low)/60k_avg               8                5.71
567                                  amt               6                4.29
895                            hs300_vol               5                3.57
1418       p5mv_(5k_low-5k_close)/5k_avg               4                2.86
234              (3k_high-3k_low)/3k_avg               4                2.86
1705                              sh_vol               4                2.86
2035                                vol0               4                2.86
908                               market               3                2.14
134           (20k_high-20k_low)/20k_avg               3                2.14
1539       p9mv_(3k_low-3k_close)/3k_avg               3                2.14
1415        p5mv_(5k_high-5k_low)/5k_avg               3                2.14
468               (high/p250mean_high-1)               3                2.14
1490      p6mv_(3k_high-3k_close)/3k_avg               2                1.43
1125        p20mv_(5k_avg/p5mv_5k_avg-1)               2                1.43
1107    p20mv_(10k_low-10k_open)/10k_avg               2                1.43
26            (10k_high-10k_low)/10k_avg               2                1.43
1311  p40mv_(10k_high-10k_close)/10k_avg               2                1.43
Among 2036 features, 1974 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 45.08s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                feature  importance_raw  importance_percent
567                                 amt              10                7.14
1865                           sz50_vol               9                6.43
1705                             sh_vol               7                5.00
1112    p20mv_(20k_avg/p20mv_20k_avg-1)               5                3.57
2025                             sz_vol               5                3.57
496                 (low/p250max_low-1)               4                2.86
124           (20k_avg/p60mv_20k_avg-1)               4                2.86
2027              upper_shadow/120k_avg               4                2.86
1268       p360mv_lower_shadow/120k_avg               4                2.86
133        (20k_high-20k_close)/20k_avg               4                2.86
430             (close/p120max_close-1)               3                2.14
1283      p3mv_(3k_low-3k_close)/3k_avg               3                2.14
1480  p60mv_(60k_open/p60mv_60k_open-1)               3                2.14
342          (60k_high-60k_low)/60k_avg               3                2.14
351          (60k_low-60k_open)/60k_avg               2                1.43
1423            p5mv_(60MA/p1mv_60MA-1)               2                1.43
908                              market               2                1.43
191      (250k_low-250k_close)/250k_avg               2                1.43
454                      (high-low)/avg               2                1.43
1466    p60mv_(20k_low/p20mv_20k_low-1)               2                1.43
Among 2036 features, 1962 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 44.06s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2025                                  sz_vol              14
1865                                sz50_vol               9
1705                                  sh_vol               8
567                                      amt               4
1268            p360mv_lower_shadow/120k_avg               4
2035                                    vol0               4
988      p120mv_(60k_high-60k_close)/60k_avg               3
986      p120mv_(60k_close-60k_open)/60k_avg               3
1449   p600mv_(120k_low-120k_close)/120k_avg               3
1138              p20mv_lower_shadow/20k_avg               3
1241         p30mv_(10k_avg/p10mv_10k_avg-1)               3
124                (20k_avg/p60mv_20k_avg-1)               3
430                  (close/p120max_close-1)               2
133             (20k_high-20k_close)/20k_avg               2
921     p1000mv_(250k_low/p250mv_250k_low-1)               2
1525        p80mv_(20k_low-20k_open)/20k_avg               2
1321         p40mv_(20k_avg/p20mv_20k_avg-1)               2
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
125                (20k_avg/p80mv_20k_avg-1)               2
1350            p480mv_lower_shadow/120k_avg               2

      importance_percent
2025               10.00
1865                6.43
1705                5.71
567                 2.86
1268                2.86
2035                2.86
988                 2.14
986                 2.14
1449                2.14
1138                2.14
1241                2.14
124                 2.14
430                 1.43
133                 1.43
921                 1.43
1525                1.43
1321                1.43
1009                1.43
125                 1.43
1350                1.43
Among 2036 features, 1960 features are not used in the model
2013-01-01 2016-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 102669
<class 'pandas.core.frame.DataFrame'>
Int64Index: 102669 entries, 20130807 to 20160325
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 425.6 MB
None
(0, 8)
Empty DataFrame
Columns: [y_l_rise, y_l_decline, y_l_avg, y_s_rise, y_s_decline, y_s_avg, y_l, y_l_r]
Index: []
Train dates:20130104-20160527
(99931, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_decline
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 56.23s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
1705                                      sh_vol              47
895                                    hs300_vol              47
2025                                      sz_vol              47
1865                                    sz50_vol              43
2048         layer0_custom_revenue2_y_s_avg_pred              32
2148         layer0_custom_revenue2_y_l_avg_pred              18
2112      layer0_custom_revenue_y_s_decline_pred              16
2086      layer0_custom_revenue_y_l_decline_pred              15
732                                      cyb_vol              10
937           p100mv_(20k_open/p20mv_20k_open-1)               7
433                         (close/p1mv_close-1)               5
936             p100mv_(20k_low/p20mv_20k_low-1)               5
1026                p12mv_(3k_low/p3mv_3k_low-1)               5
938             p100mv_(20k_vol/p20mv_20k_vol-1)               5
2146  layer0_custom_revenue2_y_l_rise_tree8_leaf               4
34                   (10k_low-10k_close)/10k_avg               4
955                 p10mv_(5k_avg/p5mv_5k_avg-1)               4
1101         p20mv_(10k_close/p10mv_10k_close-1)               4
567                                          amt               4
1250             p30mv_(10k_low/p10mv_10k_low-1)               4

      importance_percent
1705                6.27
895                 6.27
2025                6.27
1865                5.73
2048                4.27
2148                2.40
2112                2.13
2086                2.00
732                 1.33
937                 0.93
433                 0.67
936                 0.67
1026                0.67
938                 0.67
2146                0.53
34                  0.53
955                 0.53
1101                0.53
567                 0.53
1250                0.53
Among 2160 features, 1846 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 138.92s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              42
2025                                      sz_vol              20
2048         layer0_custom_revenue2_y_s_avg_pred              18
1865                                    sz50_vol              17
2112      layer0_custom_revenue_y_s_decline_pred              17
2137      layer0_custom_revenue2_y_l_rise_output              16
1705                                      sh_vol              15
895                                    hs300_vol              15
2086      layer0_custom_revenue_y_l_decline_pred              15
567                                          amt              14
568                                         area              11
732                                      cyb_vol              10
2136        layer0_custom_revenue2_y_l_rise_pred              10
2098         layer0_custom_revenue_y_l_rise_pred               9
2146  layer0_custom_revenue2_y_l_rise_tree8_leaf               8
1526             p80mv_(20k_low/p20mv_20k_low-1)               8
1065         p180mv_(60k_high-60k_close)/60k_avg               6
1238                 p300mv_lower_shadow/60k_avg               6
1466             p60mv_(20k_low/p20mv_20k_low-1)               6
1457             p60mv_(20k_avg/p20mv_20k_avg-1)               6

      importance_percent
2148                5.60
2025                2.67
2048                2.40
1865                2.27
2112                2.27
2137                2.13
1705                2.00
895                 2.00
2086                2.00
567                 1.87
568                 1.47
732                 1.33
2136                1.33
2098                1.20
2146                1.07
1526                1.07
1065                0.80
1238                0.80
1466                0.80
1457                0.80
Among 2160 features, 1800 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 56.69s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2148        layer0_custom_revenue2_y_l_avg_pred              43
568                                        area              20
2124       layer0_custom_revenue2_y_s_rise_pred              19
2048        layer0_custom_revenue2_y_s_avg_pred              15
567                                         amt              14
2137     layer0_custom_revenue2_y_l_rise_output              13
2136       layer0_custom_revenue2_y_l_rise_pred              11
1865                                   sz50_vol              10
2098        layer0_custom_revenue_y_l_rise_pred              10
2035                                       vol0               9
1235           p300mv_(60k_low/p60mv_60k_low-1)               8
2025                                     sz_vol               7
1705                                     sh_vol               6
2112     layer0_custom_revenue_y_s_decline_pred               6
2156  layer0_custom_revenue2_y_l_avg_tree6_leaf               5
453                            (high-close)/avg               5
1241            p30mv_(10k_avg/p10mv_10k_avg-1)               4
71                 (120k_avg/p600mv_120k_avg-1)               4
984            p120mv_(60k_amt/p60mv_60k_amt-1)               4
895                                   hs300_vol               4

      importance_percent
2148                5.73
568                 2.67
2124                2.53
2048                2.00
567                 1.87
2137                1.73
2136                1.47
1865                1.33
2098                1.33
2035                1.20
1235                1.07
2025                0.93
1705                0.80
2112                0.80
2156                0.67
453                 0.67
1241                0.53
71                  0.53
984                 0.53
895                 0.53
Among 2160 features, 1766 features are not used in the model

Train l2_y_s_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 59.50s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              49
2025                                  sz_vol              38
895                                hs300_vol              38
1705                                  sh_vol              35
2048     layer0_custom_revenue2_y_s_avg_pred              29
2112  layer0_custom_revenue_y_s_decline_pred              22
2148     layer0_custom_revenue2_y_l_avg_pred              18
732                                  cyb_vol              13
2124    layer0_custom_revenue2_y_s_rise_pred              10
453                         (high-close)/avg               8
2086  layer0_custom_revenue_y_l_decline_pred               7
2072      layer0_custom_revenue_y_s_avg_pred               6
1198            p25mv_(5k_low/p5mv_5k_low-1)               6
47               (10k_open/p50mv_10k_open-1)               6
1215                     p2mv_(high-low)/avg               5
519                       (open/p1mv_open-1)               5
1411             p5mv_(5k_avg/p5mv_5k_avg-1)               4
1400       p50mv_(10k_open/p10mv_10k_open-1)               4
1526         p80mv_(20k_low/p20mv_20k_low-1)               4
937       p100mv_(20k_open/p20mv_20k_open-1)               4

      importance_percent
1865                6.53
2025                5.07
895                 5.07
1705                4.67
2048                3.87
2112                2.93
2148                2.40
732                 1.73
2124                1.33
453                 1.07
2086                0.93
2072                0.80
1198                0.80
47                  0.80
1215                0.67
519                 0.67
1411                0.53
1400                0.53
1526                0.53
937                 0.53
Among 2160 features, 1830 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 58.80s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2124    layer0_custom_revenue2_y_s_rise_pred              30
2048     layer0_custom_revenue2_y_s_avg_pred              25
1705                                  sh_vol              21
2148     layer0_custom_revenue2_y_l_avg_pred              20
1865                                sz50_vol              17
2025                                  sz_vol              14
2137  layer0_custom_revenue2_y_l_rise_output              13
895                                hs300_vol              13
2112  layer0_custom_revenue_y_s_decline_pred              10
454                           (high-low)/avg               9
732                                  cyb_vol               8
2136    layer0_custom_revenue2_y_l_rise_pred               8
453                         (high-close)/avg               7
568                                     area               7
1254              p30mv_upper_shadow/10k_avg               6
1088                     p1mv_(high-low)/avg               5
2033                        upper_shadow/avg               5
2060     layer0_custom_revenue_y_s_rise_pred               5
965           p10mv_(5k_open/p5mv_5k_open-1)               4
528                     (open/p3mean_open-1)               4

      importance_percent
2124                4.00
2048                3.33
1705                2.80
2148                2.67
1865                2.27
2025                1.87
2137                1.73
895                 1.73
2112                1.33
454                 1.20
732                 1.07
2136                1.07
453                 0.93
568                 0.93
1254                0.80
1088                0.67
2033                0.67
2060                0.67
965                 0.53
528                 0.53
Among 2160 features, 1756 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 59.42s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2148     layer0_custom_revenue2_y_l_avg_pred              60
1865                                sz50_vol              31
1705                                  sh_vol              24
732                                  cyb_vol              20
2025                                  sz_vol              18
2112  layer0_custom_revenue_y_s_decline_pred              17
1526         p80mv_(20k_low/p20mv_20k_low-1)              17
2086  layer0_custom_revenue_y_l_decline_pred              17
895                                hs300_vol              15
567                                      amt               9
1238             p300mv_lower_shadow/60k_avg               9
1108         p20mv_(10k_low/p10mv_10k_low-1)               8
2048     layer0_custom_revenue2_y_s_avg_pred               7
2036      layer0_custom_revenue_y_l_avg_pred               6
1341  p480mv_(120k_high-120k_close)/120k_avg               6
1116        p20mv_(20k_high-20k_low)/20k_avg               6
1350            p480mv_lower_shadow/120k_avg               5
2137  layer0_custom_revenue2_y_l_rise_output               5
1466         p60mv_(20k_low/p20mv_20k_low-1)               5
81            (120k_high-120k_open)/120k_avg               5

      importance_percent
2148                8.00
1865                4.13
1705                3.20
732                 2.67
2025                2.40
2112                2.27
1526                2.27
2086                2.27
895                 2.00
567                 1.20
1238                1.20
1108                1.07
2048                0.93
2036                0.80
1341                0.80
1116                0.80
1350                0.67
2137                0.67
1466                0.67
81                  0.67
Among 2160 features, 1834 features are not used in the model
2013-01-01 2016-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 102608
<class 'pandas.core.frame.DataFrame'>
Int64Index: 102608 entries, 20130819 to 20160217
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 425.4 MB
None
(0, 8)
Empty DataFrame
Columns: [y_l_rise, y_l_decline, y_l_avg, y_s_rise, y_s_decline, y_s_avg, y_l, y_l_r]
Index: []
Train dates:20130104-20160527
(99933, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 83.45s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=50,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2186                     layer1_l2_y_l_avg_pred             112
2212                    layer1_l2_y_l_rise_pred              97
2290                 layer1_l2_y_l_decline_pred              76
568                                        area              48
2148        layer0_custom_revenue2_y_l_avg_pred              44
2238                     layer1_l2_y_s_avg_pred              34
2160                 layer1_l2_y_s_decline_pred              28
2025                                     sz_vol              19
1705                                     sh_vol              17
1865                                   sz50_vol              16
895                                   hs300_vol              16
2112     layer0_custom_revenue_y_s_decline_pred              11
2098        layer0_custom_revenue_y_l_rise_pred              10
2137     layer0_custom_revenue2_y_l_rise_output              10
2306          layer1_l2_y_l_decline_tree15_leaf               9
2158  layer0_custom_revenue2_y_l_avg_tree8_leaf               8
2264                    layer1_l2_y_s_rise_pred               8
2222              layer1_l2_y_l_rise_tree9_leaf               7
2136       layer0_custom_revenue2_y_l_rise_pred               7
2207              layer1_l2_y_l_avg_tree20_leaf               6

      importance_percent
2186                7.47
2212                6.47
2290                5.07
568                 3.20
2148                2.93
2238                2.27
2160                1.87
2025                1.27
1705                1.13
1865                1.07
895                 1.07
2112                0.73
2098                0.67
2137                0.67
2306                0.60
2158                0.53
2264                0.53
2222                0.47
2136                0.47
2207                0.40
Among 2316 features, 1720 features are not used in the model
test range: 20160701 20170101
2016-07-01 2016-12-31
Time slice keys in hdf5: 2016/0101-0101

Current key: 2016/0101-0101
Current slice size(length): 313767

Total concatenating size: 313767
Result dataset size: 160893

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]     12     -0.122       -0.140     0.080     0.090    -0.215
(-0.25,-0.20]    161     -0.065       -0.091     0.102     0.300    -0.235
(-0.20,-0.15]    985     -0.035       -0.073     0.107     0.449    -0.299
(-0.15,-0.10]   3220     -0.023       -0.059     0.106     0.889    -0.251
(-0.10,-0.05]   9140     -0.004       -0.039     0.100     0.878    -0.266
(-0.05,0.00]   36558      0.013        0.000     0.086     0.902    -0.335
(0.00,0.05]    73097      0.017        0.018     0.079     1.064    -0.323
(0.05,0.10]    26318      0.012        0.023     0.089     0.928    -0.291
(0.10,0.15]     8209     -0.006       -0.037     0.095     0.821    -0.259
(0.15,0.20]     2204     -0.029       -0.058     0.094     0.520    -0.297
(0.20,0.25]      575     -0.015       -0.053     0.115     0.912    -0.231
(0.25,0.30]      161     -0.030       -0.066     0.107     0.264    -0.232
(0.30,0.35]       76     -0.040       -0.070     0.115     0.293    -0.230
(0.35,0.40]       29     -0.036       -0.067     0.104     0.198    -0.214
(0.40,0.45]        3     -0.097       -0.094     0.011    -0.088    -0.109
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]     12     -0.163       -0.183     0.105     0.123    -0.284
(-0.25,-0.20]    161     -0.094       -0.132     0.144     0.423    -0.341
(-0.20,-0.15]    985     -0.052       -0.111     0.152     0.609    -0.385
(-0.15,-0.10]   3220     -0.036       -0.095     0.154     1.642    -0.337
(-0.10,-0.05]   9140     -0.009       -0.067     0.145     1.569    -0.373
(-0.05,0.00]   36558      0.016        0.000     0.124     1.274    -0.391
(0.00,0.05]    73097      0.021        0.023     0.113     1.336    -0.393
(0.05,0.10]    26318      0.013        0.035     0.127     1.255    -0.372
(0.10,0.15]     8209     -0.011       -0.062     0.132     1.168    -0.360
(0.15,0.20]     2204     -0.042       -0.088     0.133     0.692    -0.364
(0.20,0.25]      575     -0.022       -0.083     0.160     1.213    -0.299
(0.25,0.30]      161     -0.042       -0.098     0.150     0.361    -0.302
(0.30,0.35]       76     -0.054       -0.100     0.164     0.371    -0.326
(0.35,0.40]       29     -0.049       -0.099     0.154     0.286    -0.293
(0.40,0.45]        3     -0.141       -0.141     0.005    -0.136    -0.147
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]     12     -0.080       -0.097     0.057     0.057    -0.147
(-0.25,-0.20]    161     -0.037       -0.042     0.063     0.178    -0.161
(-0.20,-0.15]    985     -0.020       -0.032     0.064     0.294    -0.213
(-0.15,-0.10]   3220     -0.013       -0.021     0.062     0.617    -0.182
(-0.10,-0.05]   9140     -0.003       -0.009     0.058     0.584    -0.168
(-0.05,0.00]   36558      0.008        0.000     0.052     0.587    -0.283
(0.00,0.05]    73097      0.011        0.002     0.048     0.792    -0.254
(0.05,0.10]    26318      0.009        0.004     0.055     0.613    -0.211
(0.10,0.15]     8209     -0.003       -0.011     0.059     0.588    -0.182
(0.15,0.20]     2204     -0.018       -0.026     0.058     0.349    -0.230
(0.20,0.25]      575     -0.009       -0.022     0.071     0.612    -0.182
(0.25,0.30]      161     -0.018       -0.029     0.067     0.182    -0.162
(0.30,0.35]       76     -0.024       -0.036     0.070     0.216    -0.169
(0.35,0.40]       29     -0.022       -0.032     0.060     0.109    -0.135
(0.40,0.45]        3     -0.054       -0.053     0.018    -0.036    -0.071
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]     12      0.033        0.024     0.037     0.123     0.000
(-0.25,-0.20]    161      0.063        0.047     0.068     0.423     0.000
(-0.20,-0.15]    985      0.072        0.045     0.081     0.609     0.000
(-0.15,-0.10]   3220      0.074        0.048     0.092     1.642     0.000
(-0.10,-0.05]   9140      0.076        0.051     0.088     1.569     0.000
(-0.05,0.00]   36558      0.071        0.050     0.078     1.274     0.000
(0.00,0.05]    73097      0.066        0.046     0.073     1.336     0.000
(0.05,0.10]    26318      0.074        0.053     0.076     1.255     0.000
(0.10,0.15]     8209      0.067        0.043     0.077     1.168     0.000
(0.15,0.20]     2204      0.061        0.037     0.070     0.692     0.000
(0.20,0.25]      575      0.080        0.048     0.096     1.213     0.000
(0.25,0.30]      161      0.074        0.040     0.080     0.361     0.000
(0.30,0.35]       76      0.075        0.046     0.086     0.371     0.000
(0.35,0.40]       29      0.078        0.042     0.079     0.286     0.003
(0.40,0.45]        3      0.042        0.020     0.048     0.098     0.009
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

test period:20170101-20170701
dataset range: 20130101 20170101
2013-01-01 2016-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 118807
<class 'pandas.core.frame.DataFrame'>
Int64Index: 118807 entries, 20130607 to 20160715
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 492.5 MB
None
(15, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
Train dates:20130104-20161201
(115939, 2036)

----------Train layer 0----------

Train custom_revenue_y_l_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 54.87s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
1705                                 sh_vol              10
1536           p9mv_(3k_high-3k_low)/3k_avg              10
1138             p20mv_lower_shadow/20k_avg              10
1449  p600mv_(120k_low-120k_close)/120k_avg              10
1865                               sz50_vol              10
1061       p180mv_(60k_amt/p60mv_60k_amt-1)              10
24            (10k_close/p50mv_10k_close-1)              10
2025                                 sz_vol              10
454                          (high-low)/avg              10
988     p120mv_(60k_high-60k_close)/60k_avg              10
132           (20k_close/p80mv_20k_close-1)              10
1457        p60mv_(20k_avg/p20mv_20k_avg-1)              10
927        p100mv_(20k_avg/p20mv_20k_avg-1)              10
1347    p480mv_(120k_low/p120mv_120k_low-1)               9
1152   p240mv_(120k_low-120k_open)/120k_avg               1

      importance_percent
1705                7.14
1536                7.14
1138                7.14
1449                7.14
1865                7.14
1061                7.14
24                  7.14
2025                7.14
454                 7.14
988                 7.14
132                 7.14
1457                7.14
927                 7.14
1347                6.43
1152                0.71
Among 2036 features, 2021 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 57.55s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
2025                                 sz_vol              18
895                               hs300_vol              13
1865                               sz50_vol              12
342              (60k_high-60k_low)/60k_avg               8
454                          (high-low)/avg               6
567                                     amt               5
1705                                 sh_vol               4
1309     p40mv_(10k_close-10k_open)/10k_avg               4
430                 (close/p120max_close-1)               3
22            (10k_close/p30mv_10k_close-1)               3
2035                                   vol0               3
1449  p600mv_(120k_low-120k_close)/120k_avg               3
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               3
21            (10k_close/p20mv_10k_close-1)               2
1138             p20mv_lower_shadow/20k_avg               2
132           (20k_close/p80mv_20k_close-1)               2
453                        (high-close)/avg               2
1102     p20mv_(10k_high-10k_close)/10k_avg               2
1491           p6mv_(3k_high-3k_low)/3k_avg               2
1490         p6mv_(3k_high-3k_close)/3k_avg               2

      importance_percent
2025               12.86
895                 9.29
1865                8.57
342                 5.71
454                 4.29
567                 3.57
1705                2.86
1309                2.86
430                 2.14
22                  2.14
2035                2.14
1449                2.14
1125                2.14
21                  1.43
1138                1.43
132                 1.43
453                 1.43
1102                1.43
1491                1.43
1490                1.43
Among 2036 features, 1977 features are not used in the model

Train custom_revenue_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 52.65s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
1865                        sz50_vol              26               18.57
342       (60k_high-60k_low)/60k_avg              20               14.29
468           (high/p250mean_high-1)              10                7.14
1125    p20mv_(5k_avg/p5mv_5k_avg-1)              10                7.14
454                   (high-low)/avg              10                7.14
2035                            vol0               8                5.71
134       (20k_high-20k_low)/20k_avg               8                5.71
895                        hs300_vol               7                5.00
234          (3k_high-3k_low)/3k_avg               6                4.29
567                              amt               6                4.29
110               (20MA/p2mv_20MA-1)               4                2.86
243          (3k_low-3k_open)/3k_avg               4                2.86
1490  p6mv_(3k_high-3k_close)/3k_avg               4                2.86
1415    p5mv_(5k_high-5k_low)/5k_avg               4                2.86
2025                          sz_vol               4                2.86
1222           p2mv_(vol/p1mv_vol-1)               3                2.14
109               (20MA/p1mv_20MA-1)               2                1.43
453                 (high-close)/avg               2                1.43
1495    p6mv_(3k_low-3k_open)/3k_avg               2                1.43
Among 2036 features, 2017 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 116.98s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                  feature  importance_raw  importance_percent
2025                               sz_vol              30               21.43
1107     p20mv_(10k_low-10k_open)/10k_avg              10                7.14
303                (5k_low/p5mv_5k_low-1)              10                7.14
988   p120mv_(60k_high-60k_close)/60k_avg              10                7.14
1251    p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
895                             hs300_vol              10                7.14
1138           p20mv_lower_shadow/20k_avg              10                7.14
22          (10k_close/p30mv_10k_close-1)              10                7.14
1865                             sz50_vol               9                6.43
1495         p6mv_(3k_low-3k_open)/3k_avg               6                4.29
547                    (vol/p20max_vol-1)               5                3.57
503                      (low/p3mv_low-1)               4                2.86
1219                  p2mv_(low-open)/avg               4                2.86
247                (3k_low/p3mv_3k_low-1)               3                2.14
2035                                 vol0               3                2.14
991    p120mv_(60k_high/p60mv_60k_high-1)               2                1.43
548                   (vol/p20mean_vol-1)               1                0.71
243               (3k_low-3k_open)/3k_avg               1                0.71
1                      (10MA/p1mv_10MA-1)               1                0.71
1490       p6mv_(3k_high-3k_close)/3k_avg               1                0.71
Among 2036 features, 2016 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 51.19s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 57.54s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
88           (120k_low-120k_close)/120k_avg              20
80            (120k_high-120k_low)/120k_avg              20
2025                                 sz_vol              10
927        p100mv_(20k_avg/p20mv_20k_avg-1)              10
1226       p300mv_(60k_avg/p60mv_60k_avg-1)              10
132           (20k_close/p80mv_20k_close-1)              10
567                                     amt              10
26               (10k_high-10k_low)/10k_avg              10
1466        p60mv_(20k_low/p20mv_20k_low-1)              10
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1263  p360mv_(120k_low-120k_close)/120k_avg               7
972     p120mv_(120k_avg/p120mv_120k_avg-1)               6
1000            p120mv_upper_shadow/60k_avg               6
934      p100mv_(20k_low-20k_close)/20k_avg               1

      importance_percent
88                 14.29
80                 14.29
2025                7.14
927                 7.14
1226                7.14
132                 7.14
567                 7.14
26                  7.14
1466                7.14
1321                7.14
1263                5.00
972                 4.29
1000                4.29
934                 0.71
Among 2036 features, 2022 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 57.50s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                   feature  importance_raw  importance_percent
1865                              sz50_vol              16               11.43
133           (20k_high-20k_close)/20k_avg              10                7.14
342             (60k_high-60k_low)/60k_avg              10                7.14
454                         (high-low)/avg               9                6.43
124              (20k_avg/p60mv_20k_avg-1)               8                5.71
567                                    amt               8                5.71
2027                 upper_shadow/120k_avg               7                5.00
1525      p80mv_(20k_low-20k_open)/20k_avg               6                4.29
430                (close/p120max_close-1)               5                3.57
496                    (low/p250max_low-1)               4                2.86
1268          p360mv_lower_shadow/120k_avg               4                2.86
346           (60k_high/p180mv_60k_high-1)               4                2.86
1152  p240mv_(120k_low-120k_open)/120k_avg               4                2.86
1112       p20mv_(20k_avg/p20mv_20k_avg-1)               4                2.86
1061      p180mv_(60k_amt/p60mv_60k_amt-1)               4                2.86
2035                                  vol0               4                2.86
1519   p80mv_(20k_close/p20mv_20k_close-1)               4                2.86
895                              hs300_vol               3                2.14
1705                                sh_vol               3                2.14
83          (120k_high/p120mv_120k_high-1)               3                2.14
Among 2036 features, 2006 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 56.83s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 59.48s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              30               21.43
486                   (low/p10max_low-1)              10                7.14
538                 (open/p60min_open-1)              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
345         (60k_high/p120mv_60k_high-1)              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
2025                              sz_vol              10                7.14
134           (20k_high-20k_low)/20k_avg              10                7.14
432              (close/p120min_close-1)              10                7.14
1138          p20mv_lower_shadow/20k_avg               6                4.29
946     p10mv_(10k_high-10k_low)/10k_avg               4                2.86
Among 2036 features, 2023 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 53.49s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              12
342               (60k_high-60k_low)/60k_avg               9
454                           (high-low)/avg               9
2025                                  sz_vol               9
567                                      amt               8
234                  (3k_high-3k_low)/3k_avg               6
895                                hs300_vol               6
134               (20k_high-20k_low)/20k_avg               5
26                (10k_high-10k_low)/10k_avg               5
2035                                    vol0               5
1418           p5mv_(5k_low-5k_close)/5k_avg               3
468                   (high/p250mean_high-1)               3
1705                                  sh_vol               3
1125            p20mv_(5k_avg/p5mv_5k_avg-1)               3
1449   p600mv_(120k_low-120k_close)/120k_avg               2
1415            p5mv_(5k_high-5k_low)/5k_avg               2
975   p120mv_(120k_high-120k_close)/120k_avg               2
908                                   market               2
1311      p40mv_(10k_high-10k_close)/10k_avg               2
164                     (250MA/p3mv_250MA-1)               1

      importance_percent
1865                8.57
342                 6.43
454                 6.43
2025                6.43
567                 5.71
234                 4.29
895                 4.29
134                 3.57
26                  3.57
2035                3.57
1418                2.14
468                 2.14
1705                2.14
1125                2.14
1449                1.43
1415                1.43
975                 1.43
908                 1.43
1311                1.43
164                 0.71
Among 2036 features, 1973 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 50.27s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
567                                      amt              12
2025                                  sz_vol               6
1865                                sz50_vol               6
1705                                  sh_vol               5
342               (60k_high-60k_low)/60k_avg               5
83            (120k_high/p120mv_120k_high-1)               5
133             (20k_high-20k_close)/20k_avg               4
2027                   upper_shadow/120k_avg               4
1283           p3mv_(3k_low-3k_close)/3k_avg               4
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               4
430                  (close/p120max_close-1)               4
1009  p1250mv_(250k_low-250k_close)/250k_avg               3
124                (20k_avg/p60mv_20k_avg-1)               3
895                                hs300_vol               3
1263   p360mv_(120k_low-120k_close)/120k_avg               3
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               3
454                           (high-low)/avg               3
351               (60k_low-60k_open)/60k_avg               2
921     p1000mv_(250k_low/p250mv_250k_low-1)               2
1000             p120mv_upper_shadow/60k_avg               2

      importance_percent
567                 8.57
2025                4.29
1865                4.29
1705                3.57
342                 3.57
83                  3.57
133                 2.86
2027                2.86
1283                2.86
1112                2.86
430                 2.86
1009                2.14
124                 2.14
895                 2.14
1263                2.14
1061                2.14
454                 2.14
351                 1.43
921                 1.43
1000                1.43
Among 2036 features, 1965 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 59.48s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
2025                                 sz_vol              13
1865                               sz50_vol              12
567                                     amt               7
988     p120mv_(60k_high-60k_close)/60k_avg               6
1705                                 sh_vol               5
430                 (close/p120max_close-1)               4
1449  p600mv_(120k_low-120k_close)/120k_avg               4
1321        p40mv_(20k_avg/p20mv_20k_avg-1)               4
1457        p60mv_(20k_avg/p20mv_20k_avg-1)               4
454                          (high-low)/avg               3
986     p120mv_(60k_close-60k_open)/60k_avg               3
1061       p180mv_(60k_amt/p60mv_60k_amt-1)               3
1014          p1250mv_lower_shadow/250k_avg               3
1268           p360mv_lower_shadow/120k_avg               3
1241        p30mv_(10k_avg/p10mv_10k_avg-1)               2
125               (20k_avg/p80mv_20k_avg-1)               2
124               (20k_avg/p60mv_20k_avg-1)               2
732                                 cyb_vol               2
1138             p20mv_lower_shadow/20k_avg               2
341            (60k_high-60k_close)/60k_avg               2

      importance_percent
2025                9.29
1865                8.57
567                 5.00
988                 4.29
1705                3.57
430                 2.86
1449                2.86
1321                2.86
1457                2.86
454                 2.14
986                 2.14
1061                2.14
1014                2.14
1268                2.14
1241                1.43
125                 1.43
124                 1.43
732                 1.43
1138                1.43
341                 1.43
Among 2036 features, 1964 features are not used in the model
2013-01-01 2016-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 118807
<class 'pandas.core.frame.DataFrame'>
Int64Index: 118807 entries, 20130807 to 20160325
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 492.5 MB
None
(10, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
Train dates:20130104-20161201
(116070, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_decline
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 66.41s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
1705                                     sh_vol              52
1865                                   sz50_vol              40
2025                                     sz_vol              37
895                                   hs300_vol              36
2048        layer0_custom_revenue2_y_s_avg_pred              26
2148        layer0_custom_revenue2_y_l_avg_pred              21
732                                     cyb_vol              20
937          p100mv_(20k_open/p20mv_20k_open-1)              13
2112     layer0_custom_revenue_y_s_decline_pred              12
2072         layer0_custom_revenue_y_s_avg_pred              11
2058  layer0_custom_revenue2_y_s_avg_tree8_leaf              10
46                  (10k_open/p40mv_10k_open-1)               8
2124       layer0_custom_revenue2_y_s_rise_pred               7
2086     layer0_custom_revenue_y_l_decline_pred               7
2036         layer0_custom_revenue_y_l_avg_pred               6
938            p100mv_(20k_vol/p20mv_20k_vol-1)               5
940                 p100mv_upper_shadow/20k_avg               5
955                p10mv_(5k_avg/p5mv_5k_avg-1)               5
47                  (10k_open/p50mv_10k_open-1)               5
968                   p10mv_lower_shadow/5k_avg               5

      importance_percent
1705                6.93
1865                5.33
2025                4.93
895                 4.80
2048                3.47
2148                2.80
732                 2.67
937                 1.73
2112                1.60
2072                1.47
2058                1.33
46                  1.07
2124                0.93
2086                0.93
2036                0.80
938                 0.67
940                 0.67
955                 0.67
47                  0.67
968                 0.67
Among 2160 features, 1860 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 73.98s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              42
1705                                      sh_vol              21
1865                                    sz50_vol              20
567                                          amt              19
2025                                      sz_vol              16
732                                      cyb_vol              14
2048         layer0_custom_revenue2_y_s_avg_pred              13
2136        layer0_custom_revenue2_y_l_rise_pred              12
895                                    hs300_vol              11
2112      layer0_custom_revenue_y_s_decline_pred              11
1526             p80mv_(20k_low/p20mv_20k_low-1)              11
453                             (high-close)/avg              10
568                                         area              10
2137      layer0_custom_revenue2_y_l_rise_output               9
2146  layer0_custom_revenue2_y_l_rise_tree8_leaf               9
2098         layer0_custom_revenue_y_l_rise_pred               9
2100   layer0_custom_revenue_y_l_rise_tree0_leaf               8
1234           p300mv_(60k_low-60k_open)/60k_avg               7
2086      layer0_custom_revenue_y_l_decline_pred               7
2036          layer0_custom_revenue_y_l_avg_pred               6

      importance_percent
2148                5.60
1705                2.80
1865                2.67
567                 2.53
2025                2.13
732                 1.87
2048                1.73
2136                1.60
895                 1.47
2112                1.47
1526                1.47
453                 1.33
568                 1.33
2137                1.20
2146                1.20
2098                1.20
2100                1.07
1234                0.93
2086                0.93
2036                0.80
Among 2160 features, 1818 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 68.89s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              42
568                                         area              23
2136        layer0_custom_revenue2_y_l_rise_pred              19
2124        layer0_custom_revenue2_y_s_rise_pred              13
2137      layer0_custom_revenue2_y_l_rise_output              13
567                                          amt              13
2048         layer0_custom_revenue2_y_s_avg_pred              11
1705                                      sh_vol              11
1865                                    sz50_vol              10
2035                                        vol0               9
2098         layer0_custom_revenue_y_l_rise_pred               9
1235            p300mv_(60k_low/p60mv_60k_low-1)               7
2025                                      sz_vol               7
2146  layer0_custom_revenue2_y_l_rise_tree8_leaf               6
566                                   adj_factor               5
1238                 p300mv_lower_shadow/60k_avg               5
1456             p60mv_(20k_amt/p20mv_20k_amt-1)               5
1517             p80mv_(20k_avg/p20mv_20k_avg-1)               5
937           p100mv_(20k_open/p20mv_20k_open-1)               4
1111             p20mv_(20k_amt/p20mv_20k_amt-1)               4

      importance_percent
2148                5.60
568                 3.07
2136                2.53
2124                1.73
2137                1.73
567                 1.73
2048                1.47
1705                1.47
1865                1.33
2035                1.20
2098                1.20
1235                0.93
2025                0.93
2146                0.80
566                 0.67
1238                0.67
1456                0.67
1517                0.67
937                 0.53
1111                0.53
Among 2160 features, 1766 features are not used in the model

Train l2_y_s_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 69.53s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
895                                hs300_vol              41
1865                                sz50_vol              38
1705                                  sh_vol              36
2025                                  sz_vol              32
2048     layer0_custom_revenue2_y_s_avg_pred              21
2148     layer0_custom_revenue2_y_l_avg_pred              21
732                                  cyb_vol              13
2112  layer0_custom_revenue_y_s_decline_pred              13
2072      layer0_custom_revenue_y_s_avg_pred              10
2124    layer0_custom_revenue2_y_s_rise_pred               8
453                         (high-close)/avg               8
1198            p25mv_(5k_low/p5mv_5k_low-1)               6
1317         p40mv_(10k_low/p10mv_10k_low-1)               5
2036      layer0_custom_revenue_y_l_avg_pred               5
1088                     p1mv_(high-low)/avg               5
2136    layer0_custom_revenue2_y_l_rise_pred               5
47               (10k_open/p50mv_10k_open-1)               5
1545                p9mv_upper_shadow/3k_avg               5
937       p100mv_(20k_open/p20mv_20k_open-1)               4
936         p100mv_(20k_low/p20mv_20k_low-1)               4

      importance_percent
895                 5.47
1865                5.07
1705                4.80
2025                4.27
2048                2.80
2148                2.80
732                 1.73
2112                1.73
2072                1.33
2124                1.07
453                 1.07
1198                0.80
1317                0.67
2036                0.67
1088                0.67
2136                0.67
47                  0.67
1545                0.67
937                 0.53
936                 0.53
Among 2160 features, 1805 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 68.21s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2124    layer0_custom_revenue2_y_s_rise_pred              29
2048     layer0_custom_revenue2_y_s_avg_pred              27
1705                                  sh_vol              24
1865                                sz50_vol              21
2025                                  sz_vol              19
2148     layer0_custom_revenue2_y_l_avg_pred              18
895                                hs300_vol              13
2136    layer0_custom_revenue2_y_l_rise_pred              11
454                           (high-low)/avg               7
732                                  cyb_vol               7
1159    p240mv_(60k_close/p60mv_60k_close-1)               6
453                         (high-close)/avg               6
2137  layer0_custom_revenue2_y_l_rise_output               6
1545                p9mv_upper_shadow/3k_avg               6
2060     layer0_custom_revenue_y_s_rise_pred               5
2033                        upper_shadow/avg               5
568                                     area               5
2034                                     vol               5
1198            p25mv_(5k_low/p5mv_5k_low-1)               5
2035                                    vol0               4

      importance_percent
2124                3.87
2048                3.60
1705                3.20
1865                2.80
2025                2.53
2148                2.40
895                 1.73
2136                1.47
454                 0.93
732                 0.93
1159                0.80
453                 0.80
2137                0.80
1545                0.80
2060                0.67
2033                0.67
568                 0.67
2034                0.67
1198                0.67
2035                0.53
Among 2160 features, 1751 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 71.95s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2148        layer0_custom_revenue2_y_l_avg_pred              57
1865                                   sz50_vol              34
2025                                     sz_vol              25
1705                                     sh_vol              24
732                                     cyb_vol              19
2086     layer0_custom_revenue_y_l_decline_pred              17
1526            p80mv_(20k_low/p20mv_20k_low-1)              16
2112     layer0_custom_revenue_y_s_decline_pred              10
895                                   hs300_vol              10
1350               p480mv_lower_shadow/120k_avg               9
567                                         amt               9
1466            p60mv_(20k_low/p20mv_20k_low-1)               8
1238                p300mv_lower_shadow/60k_avg               8
1341     p480mv_(120k_high-120k_close)/120k_avg               7
2154  layer0_custom_revenue2_y_l_avg_tree4_leaf               7
2036         layer0_custom_revenue_y_l_avg_pred               7
1065        p180mv_(60k_high-60k_close)/60k_avg               6
937          p100mv_(20k_open/p20mv_20k_open-1)               6
1108            p20mv_(10k_low/p10mv_10k_low-1)               6
924               p1000mv_lower_shadow/250k_avg               5

      importance_percent
2148                7.60
1865                4.53
2025                3.33
1705                3.20
732                 2.53
2086                2.27
1526                2.13
2112                1.33
895                 1.33
1350                1.20
567                 1.20
1466                1.07
1238                1.07
1341                0.93
2154                0.93
2036                0.93
1065                0.80
937                 0.80
1108                0.80
924                 0.67
Among 2160 features, 1846 features are not used in the model
2013-01-01 2016-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 118807
<class 'pandas.core.frame.DataFrame'>
Int64Index: 118807 entries, 20130819 to 20161109
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 492.5 MB
None
(9, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
Train dates:20130104-20161201
(116043, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 93.47s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=50,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2186                  layer1_l2_y_l_avg_pred             112
2212                 layer1_l2_y_l_rise_pred             102
2290              layer1_l2_y_l_decline_pred              85
2148     layer0_custom_revenue2_y_l_avg_pred              43
568                                     area              36
2238                  layer1_l2_y_s_avg_pred              31
2160              layer1_l2_y_s_decline_pred              23
895                                hs300_vol              21
1705                                  sh_vol              16
2193            layer1_l2_y_l_avg_tree6_leaf              14
2025                                  sz_vol              14
1865                                sz50_vol              13
2308       layer1_l2_y_l_decline_tree17_leaf              12
2098     layer0_custom_revenue_y_l_rise_pred              12
2315       layer1_l2_y_l_decline_tree24_leaf              11
2224          layer1_l2_y_l_rise_tree11_leaf               8
2086  layer0_custom_revenue_y_l_decline_pred               8
2282          layer1_l2_y_s_rise_tree17_leaf               7
2310       layer1_l2_y_l_decline_tree19_leaf               7
2304       layer1_l2_y_l_decline_tree13_leaf               7

      importance_percent
2186                7.47
2212                6.80
2290                5.67
2148                2.87
568                 2.40
2238                2.07
2160                1.53
895                 1.40
1705                1.07
2193                0.93
2025                0.93
1865                0.87
2308                0.80
2098                0.80
2315                0.73
2224                0.53
2086                0.53
2282                0.47
2310                0.47
2304                0.47
Among 2316 features, 1726 features are not used in the model
test range: 20170101 20170701
2017-01-01 2017-06-30
Time slice keys in hdf5: 2017/0101-0101

Current key: 2017/0101-0101
Current slice size(length): 333371

Total concatenating size: 333371
Result dataset size: 158718

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      0        NaN          NaN       NaN       NaN       NaN
(-0.20,-0.15]      1     -0.211       -0.211       NaN    -0.211    -0.211
(-0.15,-0.10]     19     -0.011        0.000     0.121     0.145    -0.287
(-0.10,-0.05]    250     -0.037       -0.074     0.133     0.442    -0.342
(-0.05,0.00]    8284     -0.009        0.000     0.103     0.965    -0.339
(0.00,0.05]    76449      0.003        0.000     0.093     1.172    -0.450
(0.05,0.10]    65888      0.017        0.030     0.096     0.984    -0.449
(0.10,0.15]     7052      0.040        0.050     0.107     1.149    -0.423
(0.15,0.20]      567      0.070        0.078     0.142     1.105    -0.219
(0.20,0.25]       67      0.092        0.089     0.166     0.914    -0.212
(0.25,0.30]       20      0.023        0.062     0.113     0.206    -0.145
(0.30,0.35]        3      0.013        0.076     0.140     0.111    -0.148
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      0        NaN          NaN       NaN       NaN       NaN
(-0.20,-0.15]      1     -0.269       -0.269       NaN    -0.269    -0.269
(-0.15,-0.10]     19     -0.035       -0.089     0.175     0.249    -0.356
(-0.10,-0.05]    250     -0.060       -0.123     0.186     0.563    -0.431
(-0.05,0.00]    8284     -0.017        0.000     0.146     1.240    -0.458
(0.00,0.05]    76449     -0.001        0.000     0.131     1.561    -0.495
(0.05,0.10]    65888      0.019        0.045     0.135     1.210    -0.494
(0.10,0.15]     7052      0.049        0.076     0.150     1.514    -0.466
(0.15,0.20]      567      0.086        0.114     0.194     1.432    -0.304
(0.20,0.25]       67      0.119        0.136     0.218     1.130    -0.291
(0.25,0.30]       20      0.024        0.102     0.164     0.268    -0.218
(0.30,0.35]        3      0.033        0.122     0.189     0.161    -0.183
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      0        NaN          NaN       NaN       NaN       NaN
(-0.20,-0.15]      1     -0.154       -0.154       NaN    -0.154    -0.154
(-0.15,-0.10]     19     -0.021        0.000     0.073     0.082    -0.218
(-0.10,-0.05]    250     -0.021       -0.026     0.083     0.321    -0.252
(-0.05,0.00]    8284     -0.004        0.000     0.063     0.690    -0.296
(0.00,0.05]    76449      0.003        0.000     0.058     1.010    -0.405
(0.05,0.10]    65888      0.011        0.009     0.061     0.860    -0.403
(0.10,0.15]     7052      0.027        0.022     0.069     0.845    -0.381
(0.15,0.20]      567      0.049        0.039     0.096     0.778    -0.165
(0.20,0.25]       67      0.067        0.050     0.118     0.698    -0.133
(0.25,0.30]       20      0.021        0.023     0.064     0.144    -0.073
(0.30,0.35]        3     -0.007        0.029     0.092     0.061    -0.112
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      0        NaN          NaN       NaN       NaN       NaN
(-0.20,-0.15]      1      0.013        0.013       NaN     0.013     0.013
(-0.15,-0.10]     19      0.084        0.034     0.082     0.249     0.000
(-0.10,-0.05]    250      0.087        0.060     0.090     0.563     0.000
(-0.05,0.00]    8284      0.060        0.030     0.089     1.240     0.000
(0.00,0.05]    76449      0.065        0.044     0.079     1.561     0.000
(0.05,0.10]    65888      0.078        0.057     0.083     1.210     0.000
(0.10,0.15]     7052      0.103        0.081     0.096     1.514     0.000
(0.15,0.20]      567      0.146        0.118     0.132     1.432     0.000
(0.20,0.25]       67      0.171        0.136     0.160     1.130     0.003
(0.25,0.30]       20      0.111        0.113     0.067     0.268     0.010
(0.30,0.35]        3      0.097        0.122     0.080     0.161     0.008
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

test period:20170701-20180101
dataset range: 20130101 20170701
2013-01-01 2017-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 134597
<class 'pandas.core.frame.DataFrame'>
Int64Index: 134597 entries, 20130607 to 20170609
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 558.0 MB
None
(28, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170324       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170405       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170302       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170517       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170406       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
20170324    NaN
20170405    NaN
20170302    NaN
20170517    NaN
20170406    NaN
Train dates:20130104-20170531
(131732, 2036)

----------Train layer 0----------

Train custom_revenue_y_l_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 63.34s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                      feature  importance_raw  \
1865                                 sz50_vol              27
2025                                   sz_vol              18
1705                                   sh_vol              18
1457          p60mv_(20k_avg/p20mv_20k_avg-1)              10
1000              p120mv_upper_shadow/60k_avg              10
1061         p180mv_(60k_amt/p60mv_60k_amt-1)               9
1536             p9mv_(3k_high-3k_low)/3k_avg               9
2035                                     vol0               9
132             (20k_close/p80mv_20k_close-1)               9
1138               p20mv_lower_shadow/20k_avg               8
1321          p40mv_(20k_avg/p20mv_20k_avg-1)               2
1241          p30mv_(10k_avg/p10mv_10k_avg-1)               1
124                 (20k_avg/p60mv_20k_avg-1)               1
307                 (5k_open/p20mv_5k_open-1)               1
550                       (vol/p250max_vol-1)               1
993         p120mv_(60k_low-60k_open)/60k_avg               1
454                            (high-low)/avg               1
1347      p480mv_(120k_low/p120mv_120k_low-1)               1
1009   p1250mv_(250k_low-250k_close)/250k_avg               1
1340  p480mv_(120k_close/p120mv_120k_close-1)               1

      importance_percent
1865               19.29
2025               12.86
1705               12.86
1457                7.14
1000                7.14
1061                6.43
1536                6.43
2035                6.43
132                 6.43
1138                5.71
1321                1.43
1241                0.71
124                 0.71
307                 0.71
550                 0.71
993                 0.71
454                 0.71
1347                0.71
1009                0.71
1340                0.71
Among 2036 features, 2014 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 64.96s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                               feature  importance_raw  importance_percent
1865                          sz50_vol              19               13.57
2025                            sz_vol              12                8.57
895                          hs300_vol              12                8.57
342         (60k_high-60k_low)/60k_avg              11                7.86
454                     (high-low)/avg               7                5.00
1705                            sh_vol               6                4.29
1125      p20mv_(5k_avg/p5mv_5k_avg-1)               4                2.86
567                                amt               4                2.86
2035                              vol0               4                2.86
430            (close/p120max_close-1)               3                2.14
1490    p6mv_(3k_high-3k_close)/3k_avg               3                2.14
22       (10k_close/p30mv_10k_close-1)               3                2.14
1138        p20mv_lower_shadow/20k_avg               2                1.43
2034                               vol               2                1.43
732                            cyb_vol               2                1.43
132      (20k_close/p80mv_20k_close-1)               2                1.43
1303          p3mv_lower_shadow/3k_avg               1                0.71
1107  p20mv_(10k_low-10k_open)/10k_avg               1                0.71
487                (low/p10mean_low-1)               1                0.71
1212             p2mv_(close-open)/avg               1                0.71
Among 2036 features, 1976 features are not used in the model

Train custom_revenue_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 71.99s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                               feature  importance_raw  importance_percent
342         (60k_high-60k_low)/60k_avg              20               14.29
1865                          sz50_vol              19               13.57
895                          hs300_vol              16               11.43
2035                              vol0              13                9.29
234            (3k_high-3k_low)/3k_avg              10                7.14
454                     (high-low)/avg              10                7.14
424                 (avg/p60min_avg-1)              10                7.14
1125      p20mv_(5k_avg/p5mv_5k_avg-1)               7                5.00
1495      p6mv_(3k_low-3k_open)/3k_avg               6                4.29
243            (3k_low-3k_open)/3k_avg               4                2.86
1415      p5mv_(5k_high-5k_low)/5k_avg               3                2.14
110                 (20MA/p2mv_20MA-1)               3                2.14
109                 (20MA/p1mv_20MA-1)               3                2.14
453                   (high-close)/avg               3                2.14
370                     (amt/3k_amt-1)               2                1.43
1107  p20mv_(10k_low-10k_open)/10k_avg               2                1.43
133       (20k_high-20k_close)/20k_avg               2                1.43
1219               p2mv_(low-open)/avg               2                1.43
2                   (10MA/p2mv_10MA-1)               1                0.71
539                     (vol/3k_vol-1)               1                0.71
Among 2036 features, 2013 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 67.15s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
2025                              sz_vol              17               12.14
1865                            sz50_vol              16               11.43
895                            hs300_vol              14               10.00
22         (10k_close/p30mv_10k_close-1)              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
1102  p20mv_(10k_high-10k_close)/10k_avg              10                7.14
1251   p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
1138          p20mv_lower_shadow/20k_avg              10                7.14
248               (3k_low/p6mv_3k_low-1)              10                7.14
547                   (vol/p20max_vol-1)               7                5.00
1495        p6mv_(3k_low-3k_open)/3k_avg               7                5.00
1298                p3mv_(low-close)/avg               4                2.86
1490      p6mv_(3k_high-3k_close)/3k_avg               3                2.14
548                  (vol/p20mean_vol-1)               3                2.14
243              (3k_low-3k_open)/3k_avg               3                2.14
531                   (open/p4mv_open-1)               3                2.14
732                              cyb_vol               3                2.14
Among 2036 features, 2019 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 62.28s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 66.88s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                   feature  importance_raw  importance_percent
80           (120k_high-120k_low)/120k_avg              20               14.29
2025                                sz_vol              10                7.14
1466       p60mv_(20k_low/p20mv_20k_low-1)              10                7.14
26              (10k_high-10k_low)/10k_avg              10                7.14
1260  p360mv_(120k_high-120k_low)/120k_avg              10                7.14
1511   p750mv_(250k_low/p250mv_250k_low-1)              10                7.14
114                    (20k_amt/60k_amt-1)              10                7.14
1321       p40mv_(20k_avg/p20mv_20k_avg-1)              10                7.14
567                                    amt              10                7.14
1865                              sz50_vol              10                7.14
927       p100mv_(20k_avg/p20mv_20k_avg-1)              10                7.14
132          (20k_close/p80mv_20k_close-1)               8                5.71
491                    (low/p120min_low-1)               7                5.00
1000           p120mv_upper_shadow/60k_avg               2                1.43
350            (60k_low-60k_close)/60k_avg               2                1.43
88          (120k_low-120k_close)/120k_avg               1                0.71
Among 2036 features, 2020 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 66.15s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                   feature  importance_raw  importance_percent
1865                              sz50_vol              20               14.29
430                (close/p120max_close-1)              12                8.57
454                         (high-low)/avg              10                7.14
342             (60k_high-60k_low)/60k_avg              10                7.14
567                                    amt              10                7.14
1705                                sh_vol               7                5.00
2027                 upper_shadow/120k_avg               6                4.29
496                    (low/p250max_low-1)               6                4.29
133           (20k_high-20k_close)/20k_avg               6                4.29
1519   p80mv_(20k_close/p20mv_20k_close-1)               5                3.57
895                              hs300_vol               5                3.57
124              (20k_avg/p60mv_20k_avg-1)               5                3.57
1112       p20mv_(20k_avg/p20mv_20k_avg-1)               5                3.57
1350          p480mv_lower_shadow/120k_avg               5                3.57
1241       p30mv_(10k_avg/p10mv_10k_avg-1)               5                3.57
324             (60k_amt/p180mv_60k_amt-1)               4                2.86
1510  p750mv_(250k_low-250k_open)/250k_avg               3                2.14
437                (close/p250max_close-1)               3                2.14
450                 (close/p60max_close-1)               3                2.14
326             (60k_amt/p300mv_60k_amt-1)               2                1.43
Among 2036 features, 2008 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 65.34s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 66.07s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              30               21.43
26            (10k_high-10k_low)/10k_avg              20               14.29
80         (120k_high-120k_low)/120k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
432              (close/p120min_close-1)              10                7.14
345         (60k_high/p120mv_60k_high-1)              10                7.14
2025                              sz_vol              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
1241     p30mv_(10k_avg/p10mv_10k_avg-1)               7                5.00
1138          p20mv_lower_shadow/20k_avg               7                5.00
486                   (low/p10max_low-1)               5                3.57
303               (5k_low/p5mv_5k_low-1)               5                3.57
946     p10mv_(10k_high-10k_low)/10k_avg               3                2.14
538                 (open/p60min_open-1)               3                2.14
Among 2036 features, 2022 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 66.30s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
342           (60k_high-60k_low)/60k_avg              12                8.57
1865                            sz50_vol              11                7.86
895                            hs300_vol               9                6.43
454                       (high-low)/avg               8                5.71
567                                  amt               7                5.00
234              (3k_high-3k_low)/3k_avg               7                5.00
2025                              sz_vol               7                5.00
134           (20k_high-20k_low)/20k_avg               6                4.29
1418       p5mv_(5k_low-5k_close)/5k_avg               5                3.57
468               (high/p250mean_high-1)               4                2.86
1705                              sh_vol               3                2.14
80         (120k_high-120k_low)/120k_avg               3                2.14
2035                                vol0               3                2.14
1125        p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
88        (120k_low-120k_close)/120k_avg               2                1.43
288              (5k_high-5k_low)/5k_avg               2                1.43
1311  p40mv_(10k_high-10k_close)/10k_avg               2                1.43
1415        p5mv_(5k_high-5k_low)/5k_avg               2                1.43
142          (20k_low-20k_close)/20k_avg               2                1.43
1107    p20mv_(10k_low-10k_open)/10k_avg               2                1.43
Among 2036 features, 1980 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 156.15s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
567                                      amt              10
1865                                sz50_vol              10
1705                                  sh_vol               6
454                           (high-low)/avg               6
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               5
342               (60k_high-60k_low)/60k_avg               5
2025                                  sz_vol               5
468                   (high/p250mean_high-1)               4
1187            p250mv_upper_shadow/250k_avg               3
1517         p80mv_(20k_avg/p20mv_20k_avg-1)               3
2027                   upper_shadow/120k_avg               3
496                      (low/p250max_low-1)               3
430                  (close/p120max_close-1)               3
124                (20k_avg/p60mv_20k_avg-1)               2
133             (20k_high-20k_close)/20k_avg               2
22             (10k_close/p30mv_10k_close-1)               2
1229     p300mv_(60k_high-60k_close)/60k_avg               2
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               2
1283           p3mv_(3k_low-3k_close)/3k_avg               2

      importance_percent
567                 7.14
1865                7.14
1705                4.29
454                 4.29
1112                3.57
342                 3.57
2025                3.57
468                 2.86
1187                2.14
1517                2.14
2027                2.14
496                 2.14
430                 2.14
124                 1.43
133                 1.43
22                  1.43
1229                1.43
1009                1.43
1457                1.43
1283                1.43
Among 2036 features, 1963 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 69.52s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              14
2025                                  sz_vol              11
1009  p1250mv_(250k_low-250k_close)/250k_avg               5
1705                                  sh_vol               5
895                                hs300_vol               4
567                                      amt               4
430                  (close/p120max_close-1)               4
2035                                    vol0               3
1229     p300mv_(60k_high-60k_close)/60k_avg               3
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               3
454                           (high-low)/avg               3
986      p120mv_(60k_close-60k_open)/60k_avg               3
1138              p20mv_lower_shadow/20k_avg               3
124                (20k_avg/p60mv_20k_avg-1)               3
1268            p360mv_lower_shadow/120k_avg               2
1347     p480mv_(120k_low/p120mv_120k_low-1)               2
1010   p1250mv_(250k_low-250k_open)/250k_avg               2
1321         p40mv_(20k_avg/p20mv_20k_avg-1)               2
898                                      low               2
1449   p600mv_(120k_low-120k_close)/120k_avg               2

      importance_percent
1865               10.00
2025                7.86
1009                3.57
1705                3.57
895                 2.86
567                 2.86
430                 2.86
2035                2.14
1229                2.14
1112                2.14
454                 2.14
986                 2.14
1138                2.14
124                 2.14
1268                1.43
1347                1.43
1010                1.43
1321                1.43
898                 1.43
1449                1.43
Among 2036 features, 1959 features are not used in the model
2013-01-01 2017-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 134697
<class 'pandas.core.frame.DataFrame'>
Int64Index: 134697 entries, 20130807 to 20170306
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 558.4 MB
None
(21, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170110       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170303       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170425       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170227       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170109       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170607       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170518       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170222       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
20170110    NaN
20170303    NaN
20170425    NaN
20170125    NaN
20170227    NaN
20170117    NaN
20170109    NaN
20170607    NaN
20170518    NaN
20170222    NaN
Train dates:20130104-20170531
(131823, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_decline
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 84.94s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                      feature  importance_raw  \
895                                 hs300_vol              46
1865                                 sz50_vol              43
1705                                   sh_vol              42
2025                                   sz_vol              40
2048      layer0_custom_revenue2_y_s_avg_pred              20
2036       layer0_custom_revenue_y_l_avg_pred              14
2112   layer0_custom_revenue_y_s_decline_pred              14
2148      layer0_custom_revenue2_y_l_avg_pred              13
732                                   cyb_vol              12
2072       layer0_custom_revenue_y_s_avg_pred              11
2086   layer0_custom_revenue_y_l_decline_pred               9
1268             p360mv_lower_shadow/120k_avg               7
2037     layer0_custom_revenue_y_l_avg_output               7
1215                      p2mv_(high-low)/avg               4
1088                      p1mv_(high-low)/avg               4
2124     layer0_custom_revenue2_y_s_rise_pred               4
938          p100mv_(20k_vol/p20mv_20k_vol-1)               4
1005  p1250mv_(250k_high-250k_close)/250k_avg               4
1020          p12mv_(3k_high-3k_close)/3k_avg               4
567                                       amt               4

      importance_percent
895                 6.13
1865                5.73
1705                5.60
2025                5.33
2048                2.67
2036                1.87
2112                1.87
2148                1.73
732                 1.60
2072                1.47
2086                1.20
1268                0.93
2037                0.93
1215                0.53
1088                0.53
2124                0.53
938                 0.53
1005                0.53
1020                0.53
567                 0.53
Among 2160 features, 1825 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 83.55s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              34
567                                          amt              22
1705                                      sh_vol              15
2025                                      sz_vol              14
1865                                    sz50_vol              13
2048         layer0_custom_revenue2_y_s_avg_pred              13
732                                      cyb_vol              12
1526             p80mv_(20k_low/p20mv_20k_low-1)              12
2112      layer0_custom_revenue_y_s_decline_pred              11
2037        layer0_custom_revenue_y_l_avg_output              11
2136        layer0_custom_revenue2_y_l_rise_pred              10
2036          layer0_custom_revenue_y_l_avg_pred              10
2086      layer0_custom_revenue_y_l_decline_pred               9
2142  layer0_custom_revenue2_y_l_rise_tree4_leaf               7
1466             p60mv_(20k_low/p20mv_20k_low-1)               7
453                             (high-close)/avg               7
895                                    hs300_vol               7
2145  layer0_custom_revenue2_y_l_rise_tree7_leaf               6
2098         layer0_custom_revenue_y_l_rise_pred               6
2072          layer0_custom_revenue_y_s_avg_pred               6

      importance_percent
2148                4.53
567                 2.93
1705                2.00
2025                1.87
1865                1.73
2048                1.73
732                 1.60
1526                1.60
2112                1.47
2037                1.47
2136                1.33
2036                1.33
2086                1.20
2142                0.93
1466                0.93
453                 0.93
895                 0.93
2145                0.80
2098                0.80
2072                0.80
Among 2160 features, 1800 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 82.51s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              41
568                                         area              21
2137      layer0_custom_revenue2_y_l_rise_output              17
567                                          amt              14
2124        layer0_custom_revenue2_y_s_rise_pred              13
2025                                      sz_vol              13
2136        layer0_custom_revenue2_y_l_rise_pred              11
2048         layer0_custom_revenue2_y_s_avg_pred              10
2034                                         vol               8
2036          layer0_custom_revenue_y_l_avg_pred               8
2098         layer0_custom_revenue_y_l_rise_pred               7
2035                                        vol0               7
1865                                    sz50_vol               5
986          p120mv_(60k_close-60k_open)/60k_avg               5
1526             p80mv_(20k_low/p20mv_20k_low-1)               5
1521            p80mv_(20k_high-20k_low)/20k_avg               5
1111             p20mv_(20k_amt/p20mv_20k_amt-1)               4
1263       p360mv_(120k_low-120k_close)/120k_avg               4
1457             p60mv_(20k_avg/p20mv_20k_avg-1)               4
2143  layer0_custom_revenue2_y_l_rise_tree5_leaf               4

      importance_percent
2148                5.47
568                 2.80
2137                2.27
567                 1.87
2124                1.73
2025                1.73
2136                1.47
2048                1.33
2034                1.07
2036                1.07
2098                0.93
2035                0.93
1865                0.67
986                 0.67
1526                0.67
1521                0.67
1111                0.53
1263                0.53
1457                0.53
2143                0.53
Among 2160 features, 1765 features are not used in the model

Train l2_y_s_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 81.79s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
895                                hs300_vol              40
2025                                  sz_vol              37
1865                                sz50_vol              36
1705                                  sh_vol              33
2048     layer0_custom_revenue2_y_s_avg_pred              29
2148     layer0_custom_revenue2_y_l_avg_pred              20
2112  layer0_custom_revenue_y_s_decline_pred              14
732                                  cyb_vol              12
2072      layer0_custom_revenue_y_s_avg_pred               9
453                         (high-close)/avg               7
1437                p5mv_lower_shadow/5k_avg               7
1094                 p1mv_(open/p1mv_open-1)               6
1215                     p2mv_(high-low)/avg               5
1198            p25mv_(5k_low/p5mv_5k_low-1)               5
1221                 p2mv_(open/p1mv_open-1)               4
2036      layer0_custom_revenue_y_l_avg_pred               4
965           p10mv_(5k_open/p5mv_5k_open-1)               4
2136    layer0_custom_revenue2_y_l_rise_pred               4
47               (10k_open/p50mv_10k_open-1)               4
1120        p20mv_(20k_low-20k_open)/20k_avg               4

      importance_percent
895                 5.33
2025                4.93
1865                4.80
1705                4.40
2048                3.87
2148                2.67
2112                1.87
732                 1.60
2072                1.20
453                 0.93
1437                0.93
1094                0.80
1215                0.67
1198                0.67
1221                0.53
2036                0.53
965                 0.53
2136                0.53
47                  0.53
1120                0.53
Among 2160 features, 1813 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 83.69s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2124       layer0_custom_revenue2_y_s_rise_pred              28
1705                                     sh_vol              26
2048        layer0_custom_revenue2_y_s_avg_pred              24
895                                   hs300_vol              21
2148        layer0_custom_revenue2_y_l_avg_pred              19
2025                                     sz_vol              19
1865                                   sz50_vol              16
2136       layer0_custom_revenue2_y_l_rise_pred              10
568                                        area               9
2137     layer0_custom_revenue2_y_l_rise_output               8
454                              (high-low)/avg               8
453                            (high-close)/avg               6
2033                           upper_shadow/avg               5
2058  layer0_custom_revenue2_y_s_avg_tree8_leaf               5
2112     layer0_custom_revenue_y_s_decline_pred               5
2072         layer0_custom_revenue_y_s_avg_pred               5
2031                        upper_shadow/5k_avg               4
1198               p25mv_(5k_low/p5mv_5k_low-1)               4
943          p10mv_(10k_close-10k_open)/10k_avg               4
2151  layer0_custom_revenue2_y_l_avg_tree1_leaf               4

      importance_percent
2124                3.73
1705                3.47
2048                3.20
895                 2.80
2148                2.53
2025                2.53
1865                2.13
2136                1.33
568                 1.20
2137                1.07
454                 1.07
453                 0.80
2033                0.67
2058                0.67
2112                0.67
2072                0.67
2031                0.53
1198                0.53
943                 0.53
2151                0.53
Among 2160 features, 1766 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 84.37s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2148     layer0_custom_revenue2_y_l_avg_pred              45
2025                                  sz_vol              31
1705                                  sh_vol              24
1865                                sz50_vol              22
2086  layer0_custom_revenue_y_l_decline_pred              20
732                                  cyb_vol              18
1526         p80mv_(20k_low/p20mv_20k_low-1)              15
2036      layer0_custom_revenue_y_l_avg_pred              13
2037    layer0_custom_revenue_y_l_avg_output              11
895                                hs300_vol              11
2112  layer0_custom_revenue_y_s_decline_pred               9
567                                      amt               9
2048     layer0_custom_revenue2_y_s_avg_pred               8
1466         p60mv_(20k_low/p20mv_20k_low-1)               6
1341  p480mv_(120k_high-120k_close)/120k_avg               6
1269            p360mv_upper_shadow/120k_avg               6
1238             p300mv_lower_shadow/60k_avg               6
1454            p600mv_lower_shadow/120k_avg               6
1350            p480mv_lower_shadow/120k_avg               6
1186            p250mv_lower_shadow/250k_avg               5

      importance_percent
2148                6.00
2025                4.13
1705                3.20
1865                2.93
2086                2.67
732                 2.40
1526                2.00
2036                1.73
2037                1.47
895                 1.47
2112                1.20
567                 1.20
2048                1.07
1466                0.80
1341                0.80
1269                0.80
1238                0.80
1454                0.80
1350                0.80
1186                0.67
Among 2160 features, 1847 features are not used in the model
2013-01-01 2017-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 134730
<class 'pandas.core.frame.DataFrame'>
Int64Index: 134730 entries, 20130819 to 20170418
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 558.5 MB
None
(27, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170223       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170522       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170515       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170526       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170215       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170523       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170220       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170502       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170308       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170126       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
20170223    NaN
20170124    NaN
20170522    NaN
20170515    NaN
20170526    NaN
20170215    NaN
20170523    NaN
20170220    NaN
20170502    NaN
20170308    NaN
20170126    NaN
Train dates:20130104-20170531
(131992, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 108.64s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=50,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2290              layer1_l2_y_l_decline_pred             110
2212                 layer1_l2_y_l_rise_pred             104
2186                  layer1_l2_y_l_avg_pred             100
2238                  layer1_l2_y_s_avg_pred              52
568                                     area              33
2160              layer1_l2_y_s_decline_pred              28
2025                                  sz_vol              28
895                                hs300_vol              28
2148     layer0_custom_revenue2_y_l_avg_pred              27
1865                                sz50_vol              24
2306       layer1_l2_y_l_decline_tree15_leaf              12
1705                                  sh_vol              10
2264                 layer1_l2_y_s_rise_pred              10
2164        layer1_l2_y_s_decline_tree3_leaf               9
2112  layer0_custom_revenue_y_s_decline_pred               9
567                                      amt               8
2098     layer0_custom_revenue_y_l_rise_pred               8
905                      lower_shadow/5k_avg               7
2086  layer0_custom_revenue_y_l_decline_pred               7
2036      layer0_custom_revenue_y_l_avg_pred               7

      importance_percent
2290                7.33
2212                6.93
2186                6.67
2238                3.47
568                 2.20
2160                1.87
2025                1.87
895                 1.87
2148                1.80
1865                1.60
2306                0.80
1705                0.67
2264                0.67
2164                0.60
2112                0.60
567                 0.53
2098                0.53
905                 0.47
2086                0.47
2036                0.47
Among 2316 features, 1748 features are not used in the model
test range: 20170701 20180101
2017-07-01 2017-12-31
Time slice keys in hdf5: 2017/0101-0101

Current key: 2017/0101-0101
Current slice size(length): 333371

Total concatenating size: 333371
Result dataset size: 173294

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      4     -0.101       -0.089     0.030    -0.079    -0.145
(-0.25,-0.20]      8     -0.103       -0.121     0.082     0.075    -0.201
(-0.20,-0.15]     88     -0.057       -0.106     0.119     0.353    -0.230
(-0.15,-0.10]    581      0.008        0.024     0.126     0.614    -0.374
(-0.10,-0.05]  11522      0.037        0.043     0.113     0.780    -0.314
(-0.05,0.00]   53453      0.019        0.018     0.099     0.837    -0.352
(0.00,0.05]    93044      0.013        0.000     0.092     1.134    -0.365
(0.05,0.10]    12637      0.021        0.030     0.108     1.448    -0.315
(0.10,0.15]     1375      0.025        0.042     0.118     0.691    -0.322
(0.15,0.20]      290      0.018       -0.027     0.128     0.397    -0.251
(0.20,0.25]       67      0.023       -0.041     0.152     0.424    -0.191
(0.25,0.30]        7      0.046       -0.086     0.245     0.403    -0.138
(0.30,0.35]        1     -0.067       -0.067       NaN    -0.067    -0.067
(0.35,0.40]        1     -0.024       -0.024       NaN    -0.024    -0.024
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      4     -0.161       -0.154     0.025    -0.141    -0.197
(-0.25,-0.20]      8     -0.146       -0.183     0.128     0.147    -0.279
(-0.20,-0.15]     88     -0.076       -0.149     0.172     0.465    -0.299
(-0.15,-0.10]    581      0.012        0.000     0.180     0.832    -0.458
(-0.10,-0.05]  11522      0.053        0.070     0.158     1.127    -0.423
(-0.05,0.00]   53453      0.025        0.030     0.138     1.206    -0.427
(0.00,0.05]    93044      0.017        0.000     0.129     1.507    -0.454
(0.05,0.10]    12637      0.026        0.050     0.150     2.058    -0.388
(0.10,0.15]     1375      0.034        0.070     0.164     0.910    -0.422
(0.15,0.20]      290      0.024       -0.044     0.175     0.497    -0.341
(0.20,0.25]       67      0.032       -0.052     0.206     0.555    -0.241
(0.25,0.30]        7      0.039       -0.131     0.323     0.508    -0.208
(0.30,0.35]        1     -0.100       -0.100       NaN    -0.100    -0.100
(0.35,0.40]        1      0.052        0.052       NaN     0.052     0.052
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      4     -0.040       -0.024     0.036    -0.017    -0.093
(-0.25,-0.20]      8     -0.059       -0.055     0.042     0.003    -0.124
(-0.20,-0.15]     88     -0.032       -0.049     0.073     0.240    -0.161
(-0.15,-0.10]    581      0.004        0.001     0.076     0.447    -0.290
(-0.10,-0.05]  11522      0.022        0.015     0.071     0.503    -0.266
(-0.05,0.00]   53453      0.011        0.001     0.062     0.549    -0.281
(0.00,0.05]    93044      0.009        0.000     0.058     0.786    -0.275
(0.05,0.10]    12637      0.014        0.006     0.068     0.839    -0.249
(0.10,0.15]     1375      0.016        0.011     0.076     0.482    -0.237
(0.15,0.20]      290      0.012       -0.002     0.084     0.297    -0.187
(0.20,0.25]       67      0.020       -0.013     0.101     0.298    -0.142
(0.25,0.30]        7      0.052       -0.041     0.167     0.300    -0.069
(0.30,0.35]        1     -0.034       -0.034       NaN    -0.034    -0.034
(0.35,0.40]        1     -0.009       -0.009       NaN    -0.009    -0.009
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      4      0.109        0.119     0.033     0.136     0.061
(-0.25,-0.20]      8      0.078        0.075     0.038     0.147     0.015
(-0.20,-0.15]     88      0.083        0.062     0.079     0.465     0.000
(-0.15,-0.10]    581      0.106        0.081     0.101     0.832     0.000
(-0.10,-0.05]  11522      0.108        0.077     0.105     1.127     0.000
(-0.05,0.00]   53453      0.081        0.055     0.090     1.206     0.000
(0.00,0.05]    93044      0.073        0.048     0.083     1.507     0.000
(0.05,0.10]    12637      0.089        0.064     0.100     2.058     0.000
(0.10,0.15]     1375      0.104        0.080     0.101     0.910     0.000
(0.15,0.20]      290      0.108        0.073     0.105     0.497     0.000
(0.20,0.25]       67      0.123        0.083     0.136     0.555     0.000
(0.25,0.30]        7      0.162        0.035     0.237     0.508     0.000
(0.30,0.35]        1      0.027        0.027       NaN     0.027     0.027
(0.35,0.40]        1      0.052        0.052       NaN     0.052     0.052
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

test period:20180101-20180701
dataset range: 20130101 20180101
2013-01-01 2017-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 152144
<class 'pandas.core.frame.DataFrame'>
Int64Index: 152144 entries, 20130607 to 20170609
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 630.7 MB
None
(51, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170324       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170707       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170405       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
20170324    NaN
20170707    NaN
20170926    NaN
20170926    NaN
20170405    NaN
Train dates:20130104-20171130
(149126, 2036)

----------Train layer 0----------

Train custom_revenue_y_l_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 81.79s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              18
2025                                  sz_vol              16
1705                                  sh_vol              14
1000             p120mv_upper_shadow/60k_avg              10
1457         p60mv_(20k_avg/p20mv_20k_avg-1)              10
132            (20k_close/p80mv_20k_close-1)              10
430                  (close/p120max_close-1)              10
1536            p9mv_(3k_high-3k_low)/3k_avg              10
1138              p20mv_lower_shadow/20k_avg               8
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               6
2035                                    vol0               6
1009  p1250mv_(250k_low-250k_close)/250k_avg               4
1321         p40mv_(20k_avg/p20mv_20k_avg-1)               2
496                      (low/p250max_low-1)               2
454                           (high-low)/avg               2
1347     p480mv_(120k_low/p120mv_120k_low-1)               2
550                      (vol/p250max_vol-1)               2
946         p10mv_(10k_high-10k_low)/10k_avg               2
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               2
947        p10mv_(10k_high-10k_open)/10k_avg               2

      importance_percent
1865               12.86
2025               11.43
1705               10.00
1000                7.14
1457                7.14
132                 7.14
430                 7.14
1536                7.14
1138                5.71
1061                4.29
2035                4.29
1009                2.86
1321                1.43
496                 1.43
454                 1.43
1347                1.43
550                 1.43
946                 1.43
1112                1.43
947                 1.43
Among 2036 features, 2015 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 83.26s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
1865                               sz50_vol              17
2025                                 sz_vol              17
895                               hs300_vol              13
342              (60k_high-60k_low)/60k_avg              12
454                          (high-low)/avg               8
567                                     amt               6
430                 (close/p120max_close-1)               4
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               4
132           (20k_close/p80mv_20k_close-1)               3
1449  p600mv_(120k_low-120k_close)/120k_avg               3
22            (10k_close/p30mv_10k_close-1)               3
248                  (3k_low/p6mv_3k_low-1)               2
1705                                 sh_vol               2
2035                                   vol0               2
453                        (high-close)/avg               2
1489        p6mv_(3k_close/p3mv_3k_close-1)               2
1418          p5mv_(5k_low-5k_close)/5k_avg               2
1251      p30mv_(10k_open/p10mv_10k_open-1)               2
1219                    p2mv_(low-open)/avg               2
1490         p6mv_(3k_high-3k_close)/3k_avg               2

      importance_percent
1865               12.14
2025               12.14
895                 9.29
342                 8.57
454                 5.71
567                 4.29
430                 2.86
1125                2.86
132                 2.14
1449                2.14
22                  2.14
248                 1.43
1705                1.43
2035                1.43
453                 1.43
1489                1.43
1418                1.43
1251                1.43
1219                1.43
1490                1.43
Among 2036 features, 1986 features are not used in the model

Train custom_revenue_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 83.76s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
1865                        sz50_vol              25               17.86
342       (60k_high-60k_low)/60k_avg              20               14.29
895                        hs300_vol              14               10.00
134       (20k_high-20k_low)/20k_avg              11                7.86
454                   (high-low)/avg              10                7.14
1125    p20mv_(5k_avg/p5mv_5k_avg-1)              10                7.14
1490  p6mv_(3k_high-3k_close)/3k_avg               7                5.00
2035                            vol0               6                4.29
88    (120k_low-120k_close)/120k_avg               6                4.29
567                              amt               6                4.29
234          (3k_high-3k_low)/3k_avg               4                2.86
482             (high/p60min_high-1)               4                2.86
1499        p6mv_lower_shadow/3k_avg               4                2.86
453                 (high-close)/avg               2                1.43
243          (3k_low-3k_open)/3k_avg               2                1.43
1705                          sh_vol               2                1.43
1495    p6mv_(3k_low-3k_open)/3k_avg               2                1.43
1210           p2mv_(amt/p1mv_amt-1)               2                1.43
450           (close/p60max_close-1)               1                0.71
1415    p5mv_(5k_high-5k_low)/5k_avg               1                0.71
Among 2036 features, 2015 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 83.18s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
1865                            sz50_vol              15               10.71
895                            hs300_vol              13                9.29
2025                              sz_vol              10                7.14
1251   p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
732                              cyb_vol              10                7.14
454                       (high-low)/avg              10                7.14
1138          p20mv_lower_shadow/20k_avg              10                7.14
1219                 p2mv_(low-open)/avg               7                5.00
1490      p6mv_(3k_high-3k_close)/3k_avg               5                3.57
531                   (open/p4mv_open-1)               5                3.57
937   p100mv_(20k_open/p20mv_20k_open-1)               5                3.57
132        (20k_close/p80mv_20k_close-1)               5                3.57
1536        p9mv_(3k_high-3k_low)/3k_avg               5                3.57
22         (10k_close/p30mv_10k_close-1)               5                3.57
303               (5k_low/p5mv_5k_low-1)               5                3.57
248               (3k_low/p6mv_3k_low-1)               5                3.57
1495        p6mv_(3k_low-3k_open)/3k_avg               5                3.57
519                   (open/p1mv_open-1)               4                2.86
563                   (vol/p60max_vol-1)               3                2.14
2035                                vol0               2                1.43
Among 2036 features, 2015 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 82.75s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 90.18s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
80            (120k_high-120k_low)/120k_avg              20
88           (120k_low-120k_close)/120k_avg              19
567                                     amt              13
26               (10k_high-10k_low)/10k_avg              10
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1447  p600mv_(120k_high-120k_open)/120k_avg               8
1070      p180mv_(60k_low-60k_open)/60k_avg               8
2025                                 sz_vol               7
132           (20k_close/p80mv_20k_close-1)               7
1466        p60mv_(20k_low/p20mv_20k_low-1)               7
1865                               sz50_vol               7
927        p100mv_(20k_avg/p20mv_20k_avg-1)               7
1000            p120mv_upper_shadow/60k_avg               6
1511    p750mv_(250k_low/p250mv_250k_low-1)               2
1479        p60mv_(60k_low/p60mv_60k_low-1)               2
1260   p360mv_(120k_high-120k_low)/120k_avg               2
895                               hs300_vol               2
1526        p80mv_(20k_low/p20mv_20k_low-1)               1
134              (20k_high-20k_low)/20k_avg               1
972     p120mv_(120k_avg/p120mv_120k_avg-1)               1

      importance_percent
80                 14.29
88                 13.57
567                 9.29
26                  7.14
1321                7.14
1447                5.71
1070                5.71
2025                5.00
132                 5.00
1466                5.00
1865                5.00
927                 5.00
1000                4.29
1511                1.43
1479                1.43
1260                1.43
895                 1.43
1526                0.71
134                 0.71
972                 0.71
Among 2036 features, 2016 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 82.49s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                   feature  importance_raw  importance_percent
1865                              sz50_vol              21               15.00
342             (60k_high-60k_low)/60k_avg              18               12.86
430                (close/p120max_close-1)              12                8.57
567                                    amt              10                7.14
454                         (high-low)/avg              10                7.14
895                              hs300_vol               7                5.00
133           (20k_high-20k_close)/20k_avg               6                4.29
1705                                sh_vol               6                4.29
1112       p20mv_(20k_avg/p20mv_20k_avg-1)               5                3.57
1519   p80mv_(20k_close/p20mv_20k_close-1)               5                3.57
1241       p30mv_(10k_avg/p10mv_10k_avg-1)               4                2.86
2027                 upper_shadow/120k_avg               4                2.86
124              (20k_avg/p60mv_20k_avg-1)               4                2.86
496                    (low/p250max_low-1)               4                2.86
1350          p480mv_lower_shadow/120k_avg               3                2.14
1268          p360mv_lower_shadow/120k_avg               2                1.43
1061      p180mv_(60k_amt/p60mv_60k_amt-1)               2                1.43
346           (60k_high/p180mv_60k_high-1)               2                1.43
987   p120mv_(60k_close/p60mv_60k_close-1)               2                1.43
437                (close/p250max_close-1)               2                1.43
Among 2036 features, 2008 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 79.66s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 86.38s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              30               21.43
486                   (low/p10max_low-1)              10                7.14
2025                              sz_vol              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
134           (20k_high-20k_low)/20k_avg              10                7.14
432              (close/p120min_close-1)              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
491                  (low/p120min_low-1)               6                4.29
345         (60k_high/p120mv_60k_high-1)               6                4.29
1138          p20mv_lower_shadow/20k_avg               6                4.29
946     p10mv_(10k_high-10k_low)/10k_avg               4                2.86
1705                              sh_vol               4                2.86
538                 (open/p60min_open-1)               4                2.86
Among 2036 features, 2021 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 83.49s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
342       (60k_high-60k_low)/60k_avg              14               10.00
1865                        sz50_vol              11                7.86
454                   (high-low)/avg              10                7.14
2025                          sz_vol               9                6.43
567                              amt               9                6.43
1418   p5mv_(5k_low-5k_close)/5k_avg               6                4.29
234          (3k_high-3k_low)/3k_avg               6                4.29
134       (20k_high-20k_low)/20k_avg               5                3.57
2035                            vol0               5                3.57
895                        hs300_vol               5                3.57
1705                          sh_vol               4                2.86
80     (120k_high-120k_low)/120k_avg               4                2.86
468           (high/p250mean_high-1)               4                2.86
142      (20k_low-20k_close)/20k_avg               3                2.14
1125    p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
1490  p6mv_(3k_high-3k_close)/3k_avg               3                2.14
908                           market               2                1.43
26        (10k_high-10k_low)/10k_avg               2                1.43
88    (120k_low-120k_close)/120k_avg               2                1.43
288          (5k_high-5k_low)/5k_avg               2                1.43
Among 2036 features, 1989 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 89.47s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
567                                      amt              12
1865                                sz50_vol              11
342               (60k_high-60k_low)/60k_avg              10
454                           (high-low)/avg               8
2025                                  sz_vol               7
430                  (close/p120max_close-1)               5
1705                                  sh_vol               5
133             (20k_high-20k_close)/20k_avg               4
1283           p3mv_(3k_low-3k_close)/3k_avg               3
468                   (high/p250mean_high-1)               3
142              (20k_low-20k_close)/20k_avg               3
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               3
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
895                                hs300_vol               2
1366                 p4mv_(high/p1mv_high-1)               2
288                  (5k_high-5k_low)/5k_avg               2
1375     p500mv_(250k_avg/p250mv_250k_avg-1)               2
22             (10k_close/p30mv_10k_close-1)               2
1268            p360mv_lower_shadow/120k_avg               2
1235        p300mv_(60k_low/p60mv_60k_low-1)               2

      importance_percent
567                 8.57
1865                7.86
342                 7.14
454                 5.71
2025                5.00
430                 3.57
1705                3.57
133                 2.86
1283                2.14
468                 2.14
142                 2.14
1112                2.14
1009                1.43
895                 1.43
1366                1.43
288                 1.43
1375                1.43
22                  1.43
1268                1.43
1235                1.43
Among 2036 features, 1972 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 87.93s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              12
2025                                  sz_vol               7
1705                                  sh_vol               7
1009  p1250mv_(250k_low-250k_close)/250k_avg               5
430                  (close/p120max_close-1)               4
895                                hs300_vol               4
2035                                    vol0               4
1449   p600mv_(120k_low-120k_close)/120k_avg               3
1321         p40mv_(20k_avg/p20mv_20k_avg-1)               3
1229     p300mv_(60k_high-60k_close)/60k_avg               3
1241         p30mv_(10k_avg/p10mv_10k_avg-1)               3
2034                                     vol               3
1347     p480mv_(120k_low/p120mv_120k_low-1)               3
454                           (high-low)/avg               3
1268            p360mv_lower_shadow/120k_avg               3
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               2
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               2
567                                      amt               2
453                         (high-close)/avg               2
154              (20k_open/p60mv_20k_open-1)               2

      importance_percent
1865                8.57
2025                5.00
1705                5.00
1009                3.57
430                 2.86
895                 2.86
2035                2.86
1449                2.14
1321                2.14
1229                2.14
1241                2.14
2034                2.14
1347                2.14
454                 2.14
1268                2.14
1457                1.43
1061                1.43
567                 1.43
453                 1.43
154                 1.43
Among 2036 features, 1961 features are not used in the model
2013-01-01 2017-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 152144
<class 'pandas.core.frame.DataFrame'>
Int64Index: 152144 entries, 20130807 to 20170306
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 630.7 MB
None
(45, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170110       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170706       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170303       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170425       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171116       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170227       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170822       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
20171214    NaN
20171115    NaN
20170110    NaN
20170706    NaN
20170303    NaN
20170425    NaN
20170125    NaN
20171116    NaN
20170227    NaN
20170822    NaN
Train dates:20130104-20171130
(149201, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_decline
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 102.54s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2025                                  sz_vol              48
1865                                sz50_vol              39
895                                hs300_vol              36
1705                                  sh_vol              31
2048     layer0_custom_revenue2_y_s_avg_pred              24
732                                  cyb_vol              18
2112  layer0_custom_revenue_y_s_decline_pred              15
2036      layer0_custom_revenue_y_l_avg_pred              11
2148     layer0_custom_revenue2_y_l_avg_pred               9
2072      layer0_custom_revenue_y_s_avg_pred               9
2086  layer0_custom_revenue_y_l_decline_pred               8
2073    layer0_custom_revenue_y_s_avg_output               8
1088                     p1mv_(high-low)/avg               6
1526         p80mv_(20k_low/p20mv_20k_low-1)               6
2124    layer0_custom_revenue2_y_s_rise_pred               6
1390         p50mv_(10k_avg/p10mv_10k_avg-1)               5
937       p100mv_(20k_open/p20mv_20k_open-1)               5
938         p100mv_(20k_vol/p20mv_20k_vol-1)               5
1400       p50mv_(10k_open/p10mv_10k_open-1)               4
1487             p6mv_(3k_avg/p3mv_3k_avg-1)               4

      importance_percent
2025                6.40
1865                5.20
895                 4.80
1705                4.13
2048                3.20
732                 2.40
2112                2.00
2036                1.47
2148                1.20
2072                1.20
2086                1.07
2073                1.07
1088                0.80
1526                0.80
2124                0.80
1390                0.67
937                 0.67
938                 0.67
1400                0.53
1487                0.53
Among 2160 features, 1841 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 116.84s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              35
1865                                    sz50_vol              25
2025                                      sz_vol              22
567                                          amt              16
2112      layer0_custom_revenue_y_s_decline_pred              16
2098         layer0_custom_revenue_y_l_rise_pred              15
1705                                      sh_vol              14
732                                      cyb_vol              13
2136        layer0_custom_revenue2_y_l_rise_pred              12
1526             p80mv_(20k_low/p20mv_20k_low-1)              12
2036          layer0_custom_revenue_y_l_avg_pred              11
1466             p60mv_(20k_low/p20mv_20k_low-1)              11
2048         layer0_custom_revenue2_y_s_avg_pred              10
2086      layer0_custom_revenue_y_l_decline_pred              10
895                                    hs300_vol               9
2072          layer0_custom_revenue_y_s_avg_pred               8
986          p120mv_(60k_close-60k_open)/60k_avg               7
1463           p60mv_(20k_high/p20mv_20k_high-1)               7
2143  layer0_custom_revenue2_y_l_rise_tree5_leaf               7
2055   layer0_custom_revenue2_y_s_avg_tree5_leaf               6

      importance_percent
2148                4.67
1865                3.33
2025                2.93
567                 2.13
2112                2.13
2098                2.00
1705                1.87
732                 1.73
2136                1.60
1526                1.60
2036                1.47
1466                1.47
2048                1.33
2086                1.33
895                 1.20
2072                1.07
986                 0.93
1463                0.93
2143                0.93
2055                0.80
Among 2160 features, 1833 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 109.83s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              34
568                                         area              20
2136        layer0_custom_revenue2_y_l_rise_pred              19
2036          layer0_custom_revenue_y_l_avg_pred              18
567                                          amt              16
1865                                    sz50_vol              13
2098         layer0_custom_revenue_y_l_rise_pred              11
2048         layer0_custom_revenue2_y_s_avg_pred              11
2137      layer0_custom_revenue2_y_l_rise_output               9
1456             p60mv_(20k_amt/p20mv_20k_amt-1)               9
2025                                      sz_vol               9
895                                    hs300_vol               8
2124        layer0_custom_revenue2_y_s_rise_pred               7
1235            p300mv_(60k_low/p60mv_60k_low-1)               6
1263       p360mv_(120k_low-120k_close)/120k_avg               6
2034                                         vol               6
453                             (high-close)/avg               5
1526             p80mv_(20k_low/p20mv_20k_low-1)               5
993            p120mv_(60k_low-60k_open)/60k_avg               5
2143  layer0_custom_revenue2_y_l_rise_tree5_leaf               5

      importance_percent
2148                4.53
568                 2.67
2136                2.53
2036                2.40
567                 2.13
1865                1.73
2098                1.47
2048                1.47
2137                1.20
1456                1.20
2025                1.20
895                 1.07
2124                0.93
1235                0.80
1263                0.80
2034                0.80
453                 0.67
1526                0.67
993                 0.67
2143                0.67
Among 2160 features, 1765 features are not used in the model

Train l2_y_s_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 102.66s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2025                                     sz_vol              41
1865                                   sz50_vol              36
1705                                     sh_vol              32
895                                   hs300_vol              31
2048        layer0_custom_revenue2_y_s_avg_pred              26
2112     layer0_custom_revenue_y_s_decline_pred              17
2148        layer0_custom_revenue2_y_l_avg_pred              15
732                                     cyb_vol              13
2072         layer0_custom_revenue_y_s_avg_pred              11
1219                        p2mv_(low-open)/avg               8
2036         layer0_custom_revenue_y_l_avg_pred               7
453                            (high-close)/avg               7
2098        layer0_custom_revenue_y_l_rise_pred               5
1198               p25mv_(5k_low/p5mv_5k_low-1)               5
2058  layer0_custom_revenue2_y_s_avg_tree8_leaf               5
1215                        p2mv_(high-low)/avg               5
2073       layer0_custom_revenue_y_s_avg_output               5
1088                        p1mv_(high-low)/avg               5
935           p100mv_(20k_low-20k_open)/20k_avg               5
519                          (open/p1mv_open-1)               5

      importance_percent
2025                5.47
1865                4.80
1705                4.27
895                 4.13
2048                3.47
2112                2.27
2148                2.00
732                 1.73
2072                1.47
1219                1.07
2036                0.93
453                 0.93
2098                0.67
1198                0.67
2058                0.67
1215                0.67
2073                0.67
1088                0.67
935                 0.67
519                 0.67
Among 2160 features, 1818 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 105.38s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2124        layer0_custom_revenue2_y_s_rise_pred              31
2048         layer0_custom_revenue2_y_s_avg_pred              25
2025                                      sz_vol              20
2148         layer0_custom_revenue2_y_l_avg_pred              16
1705                                      sh_vol              14
1865                                    sz50_vol              12
895                                    hs300_vol              12
2136        layer0_custom_revenue2_y_l_rise_pred              12
454                               (high-low)/avg               9
2112      layer0_custom_revenue_y_s_decline_pred               8
2036          layer0_custom_revenue_y_l_avg_pred               7
2034                                         vol               7
2033                            upper_shadow/avg               7
1088                         p1mv_(high-low)/avg               7
2137      layer0_custom_revenue2_y_l_rise_output               6
2128  layer0_custom_revenue2_y_s_rise_tree2_leaf               6
1545                    p9mv_upper_shadow/3k_avg               5
2072          layer0_custom_revenue_y_s_avg_pred               5
732                                      cyb_vol               5
2035                                        vol0               5

      importance_percent
2124                4.13
2048                3.33
2025                2.67
2148                2.13
1705                1.87
1865                1.60
895                 1.60
2136                1.60
454                 1.20
2112                1.07
2036                0.93
2034                0.93
2033                0.93
1088                0.93
2137                0.80
2128                0.80
1545                0.67
2072                0.67
732                 0.67
2035                0.67
Among 2160 features, 1761 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 108.25s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2148        layer0_custom_revenue2_y_l_avg_pred              38
1865                                   sz50_vol              33
2025                                     sz_vol              26
2086     layer0_custom_revenue_y_l_decline_pred              21
1526            p80mv_(20k_low/p20mv_20k_low-1)              21
2036         layer0_custom_revenue_y_l_avg_pred              20
732                                     cyb_vol              19
1705                                     sh_vol              19
2112     layer0_custom_revenue_y_s_decline_pred              10
1350               p480mv_lower_shadow/120k_avg               7
1238                p300mv_lower_shadow/60k_avg               7
1062           p180mv_(60k_avg/p60mv_60k_avg-1)               6
1341     p480mv_(120k_high-120k_close)/120k_avg               6
2155  layer0_custom_revenue2_y_l_avg_tree5_leaf               6
1160        p240mv_(60k_high-60k_close)/60k_avg               6
567                                         amt               6
1466            p60mv_(20k_low/p20mv_20k_low-1)               6
1269               p360mv_upper_shadow/120k_avg               6
895                                   hs300_vol               6
1264       p360mv_(120k_low-120k_open)/120k_avg               6

      importance_percent
2148                5.07
1865                4.40
2025                3.47
2086                2.80
1526                2.80
2036                2.67
732                 2.53
1705                2.53
2112                1.33
1350                0.93
1238                0.93
1062                0.80
1341                0.80
2155                0.80
1160                0.80
567                 0.80
1466                0.80
1269                0.80
895                 0.80
1264                0.80
Among 2160 features, 1854 features are not used in the model
2013-01-01 2017-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 152144
<class 'pandas.core.frame.DataFrame'>
Int64Index: 152144 entries, 20130819 to 20170418
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 630.7 MB
None
(45, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170223       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170522       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170515       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170526       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170920       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171120       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170908       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
20170223    NaN
20171222    NaN
20170124    NaN
20170522    NaN
20170515    NaN
20170526    NaN
20170920    NaN
20171120    NaN
20171027    NaN
20171027    NaN
20170908    NaN
Train dates:20130104-20171130
(149058, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 143.02s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=50,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2186                  layer1_l2_y_l_avg_pred             113
2290              layer1_l2_y_l_decline_pred             106
2212                 layer1_l2_y_l_rise_pred              94
2148     layer0_custom_revenue2_y_l_avg_pred              38
568                                     area              35
2238                  layer1_l2_y_s_avg_pred              31
895                                hs300_vol              30
2025                                  sz_vol              25
2315       layer1_l2_y_l_decline_tree24_leaf              24
2160              layer1_l2_y_s_decline_pred              23
1705                                  sh_vol              19
1865                                sz50_vol              16
2098     layer0_custom_revenue_y_l_rise_pred              14
2264                 layer1_l2_y_s_rise_pred              12
2296        layer1_l2_y_l_decline_tree5_leaf              10
2169        layer1_l2_y_s_decline_tree8_leaf               9
2086  layer0_custom_revenue_y_l_decline_pred               9
1482              p60mv_lower_shadow/20k_avg               7
2313       layer1_l2_y_l_decline_tree22_leaf               7
2171       layer1_l2_y_s_decline_tree10_leaf               6

      importance_percent
2186                7.53
2290                7.07
2212                6.27
2148                2.53
568                 2.33
2238                2.07
895                 2.00
2025                1.67
2315                1.60
2160                1.53
1705                1.27
1865                1.07
2098                0.93
2264                0.80
2296                0.67
2169                0.60
2086                0.60
1482                0.47
2313                0.47
2171                0.40
Among 2316 features, 1759 features are not used in the model
test range: 20180101 20180701
2018-01-01 2018-06-30
Time slice keys in hdf5: 2018/0101-0101

Current key: 2018/0101-0101
Current slice size(length): 347712

Total concatenating size: 347712
Result dataset size: 169366

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      1      0.079        0.079       NaN     0.079     0.079
(-0.25,-0.20]      8     -0.024       -0.008     0.115     0.092    -0.160
(-0.20,-0.15]     53     -0.127       -0.128     0.067     0.106    -0.230
(-0.15,-0.10]    109     -0.090       -0.126     0.126     0.296    -0.262
(-0.10,-0.05]   1047     -0.067       -0.101     0.125     0.727    -0.392
(-0.05,0.00]   20392     -0.040       -0.058     0.109     0.726    -0.612
(0.00,0.05]    95730     -0.021       -0.041     0.102     0.762    -0.607
(0.05,0.10]    38531     -0.004       -0.036     0.123     1.003    -0.569
(0.10,0.15]    10175      0.019        0.041     0.142     0.872    -0.543
(0.15,0.20]     2396      0.045        0.067     0.167     0.691    -0.387
(0.20,0.25]      585      0.065        0.088     0.199     0.623    -0.369
(0.25,0.30]       94      0.068        0.089     0.190     0.450    -0.278
(0.30,0.35]       27      0.088        0.174     0.236     0.565    -0.280
(0.35,0.40]        3      0.246        0.113     0.256     0.541     0.085
(0.40,0.45]        1      0.490        0.490       NaN     0.490     0.490
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      1      0.127        0.127       NaN     0.127     0.127
(-0.25,-0.20]      8     -0.112       -0.151     0.164     0.143    -0.278
(-0.20,-0.15]     53     -0.194       -0.197     0.093     0.159    -0.335
(-0.15,-0.10]    109     -0.142       -0.200     0.177     0.369    -0.354
(-0.10,-0.05]   1047     -0.104       -0.158     0.178     0.950    -0.453
(-0.05,0.00]   20392     -0.062       -0.099     0.152     0.976    -0.639
(0.00,0.05]    95730     -0.037       -0.077     0.143     1.027    -0.635
(0.05,0.10]    38531     -0.016       -0.079     0.172     1.338    -0.605
(0.10,0.15]    10175      0.015        0.000     0.197     1.056    -0.563
(0.15,0.20]     2396      0.052        0.103     0.227     0.817    -0.473
(0.20,0.25]      585      0.079        0.132     0.267     0.839    -0.429
(0.25,0.30]       94      0.085        0.154     0.268     0.601    -0.349
(0.30,0.35]       27      0.120        0.241     0.319     0.702    -0.339
(0.35,0.40]        3      0.347        0.189     0.295     0.688     0.165
(0.40,0.45]        1      0.648        0.648       NaN     0.648     0.648
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      1      0.030        0.030       NaN     0.030     0.030
(-0.25,-0.20]      8     -0.004       -0.005     0.040     0.042    -0.045
(-0.20,-0.15]     53     -0.063       -0.068     0.038     0.053    -0.161
(-0.15,-0.10]    109     -0.047       -0.063     0.074     0.223    -0.181
(-0.10,-0.05]   1047     -0.036       -0.042     0.075     0.503    -0.337
(-0.05,0.00]   20392     -0.023       -0.017     0.067     0.477    -0.589
(0.00,0.05]    95730     -0.011       -0.009     0.064     0.609    -0.583
(0.05,0.10]    38531     -0.000       -0.004     0.078     0.668    -0.534
(0.10,0.15]    10175      0.016        0.007     0.092     0.688    -0.524
(0.15,0.20]     2396      0.034        0.023     0.111     0.594    -0.355
(0.20,0.25]      585      0.046        0.035     0.135     0.510    -0.314
(0.25,0.30]       94      0.048        0.028     0.118     0.313    -0.206
(0.30,0.35]       27      0.056        0.094     0.157     0.428    -0.223
(0.35,0.40]        3      0.145        0.037     0.216     0.393     0.004
(0.40,0.45]        1      0.333        0.333       NaN     0.333     0.333
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      1      0.127        0.127       NaN     0.127     0.127
(-0.25,-0.20]      8      0.099        0.106     0.040     0.143     0.034
(-0.20,-0.15]     53      0.047        0.039     0.036     0.159     0.000
(-0.15,-0.10]    109      0.075        0.050     0.076     0.369     0.000
(-0.10,-0.05]   1047      0.076        0.052     0.084     0.950     0.000
(-0.05,0.00]   20392      0.058        0.033     0.073     0.976     0.000
(0.00,0.05]    95730      0.064        0.041     0.072     1.027     0.000
(0.05,0.10]    38531      0.091        0.066     0.088     1.338     0.000
(0.10,0.15]    10175      0.118        0.089     0.108     1.056     0.000
(0.15,0.20]     2396      0.150        0.116     0.132     0.817     0.000
(0.20,0.25]      585      0.185        0.146     0.163     0.839     0.000
(0.25,0.30]       94      0.198        0.173     0.155     0.601     0.005
(0.30,0.35]       27      0.233        0.241     0.188     0.702     0.000
(0.35,0.40]        3      0.347        0.189     0.295     0.688     0.165
(0.40,0.45]        1      0.648        0.648       NaN     0.648     0.648
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

test period:20180701-20190101
dataset range: 20130101 20180701
2013-01-01 2018-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 169080
<class 'pandas.core.frame.DataFrame'>
Int64Index: 169080 entries, 20130607 to 20180604
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 700.9 MB
None
(70, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170324       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170707       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170405       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
20170324    NaN
20170707    NaN
20170926    NaN
20170926    NaN
20170405    NaN
Train dates:20130104-20180530
(166068, 2036)

----------Train layer 0----------

Train custom_revenue_y_l_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 108.24s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                  feature  importance_raw  importance_percent
1865                             sz50_vol              29               20.71
1705                               sh_vol              11                7.86
2025                               sz_vol              10                7.14
1268         p360mv_lower_shadow/120k_avg              10                7.14
430               (close/p120max_close-1)              10                7.14
489                   (low/p120max_low-1)              10                7.14
1375  p500mv_(250k_avg/p250mv_250k_avg-1)              10                7.14
124             (20k_avg/p60mv_20k_avg-1)              10                7.14
1000          p120mv_upper_shadow/60k_avg              10                7.14
1061     p180mv_(60k_amt/p60mv_60k_amt-1)              10                7.14
1457      p60mv_(20k_avg/p20mv_20k_avg-1)              10                7.14
454                        (high-low)/avg               9                6.43
234               (3k_high-3k_low)/3k_avg               1                0.71
Among 2036 features, 2023 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 108.28s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
895                               hs300_vol              19
342              (60k_high-60k_low)/60k_avg              13
454                          (high-low)/avg               9
1865                               sz50_vol               9
2025                                 sz_vol               9
1705                                 sh_vol               6
430                 (close/p120max_close-1)               5
2034                                    vol               4
22            (10k_close/p30mv_10k_close-1)               3
1449  p600mv_(120k_low-120k_close)/120k_avg               3
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               3
567                                     amt               3
2035                                   vol0               3
1284           p3mv_(3k_low-3k_open)/3k_avg               2
732                                 cyb_vol               2
1243    p30mv_(10k_close/p10mv_10k_close-1)               2
1495           p6mv_(3k_low-3k_open)/3k_avg               2
1102     p20mv_(10k_high-10k_close)/10k_avg               2
297                 (5k_low-5k_open)/5k_avg               2
404                     (avg/p120min_avg-1)               2

      importance_percent
895                13.57
342                 9.29
454                 6.43
1865                6.43
2025                6.43
1705                4.29
430                 3.57
2034                2.86
22                  2.14
1449                2.14
1125                2.14
567                 2.14
2035                2.14
1284                1.43
732                 1.43
1243                1.43
1495                1.43
1102                1.43
297                 1.43
404                 1.43
Among 2036 features, 1983 features are not used in the model

Train custom_revenue_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 99.15s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
342           (60k_high-60k_low)/60k_avg              20               14.29
895                            hs300_vol              20               14.29
2035                                vol0              12                8.57
1865                            sz50_vol              10                7.14
482                 (high/p60min_high-1)              10                7.14
234              (3k_high-3k_low)/3k_avg              10                7.14
454                       (high-low)/avg              10                7.14
1495        p6mv_(3k_low-3k_open)/3k_avg               7                5.00
453                     (high-close)/avg               4                2.86
110                   (20MA/p2mv_20MA-1)               4                2.86
243              (3k_low-3k_open)/3k_avg               4                2.86
1102  p20mv_(10k_high-10k_close)/10k_avg               4                2.86
133         (20k_high-20k_close)/20k_avg               3                2.14
134           (20k_high-20k_low)/20k_avg               3                2.14
1490      p6mv_(3k_high-3k_close)/3k_avg               3                2.14
1107    p20mv_(10k_low-10k_open)/10k_avg               3                2.14
1219                 p2mv_(low-open)/avg               3                2.14
1415        p5mv_(5k_high-5k_low)/5k_avg               3                2.14
370                       (amt/3k_amt-1)               3                2.14
1125        p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
Among 2036 features, 2015 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 102.93s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
1865                            sz50_vol              20               14.29
2025                              sz_vol              20               14.29
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
22         (10k_close/p30mv_10k_close-1)              10                7.14
1                     (10MA/p1mv_10MA-1)              10                7.14
1251   p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
895                            hs300_vol              10                7.14
1495        p6mv_(3k_low-3k_open)/3k_avg              10                7.14
1284        p3mv_(3k_low-3k_open)/3k_avg              10                7.14
454                       (high-low)/avg              10                7.14
1138          p20mv_lower_shadow/20k_avg              10                7.14
248               (3k_low/p6mv_3k_low-1)              10                7.14
Among 2036 features, 2024 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 93.58s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 108.93s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
80            (120k_high-120k_low)/120k_avg              20
567                                     amt              20
88           (120k_low-120k_close)/120k_avg              16
134              (20k_high-20k_low)/20k_avg              11
895                               hs300_vol              10
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1446   p600mv_(120k_high-120k_low)/120k_avg              10
1447  p600mv_(120k_high-120k_open)/120k_avg              10
26               (10k_high-10k_low)/10k_avg               9
1000            p120mv_upper_shadow/60k_avg               8
1509  p750mv_(250k_low-250k_close)/250k_avg               7
927        p100mv_(20k_avg/p20mv_20k_avg-1)               4
1526        p80mv_(20k_low/p20mv_20k_low-1)               4
1235       p300mv_(60k_low/p60mv_60k_low-1)               1

      importance_percent
80                 14.29
567                14.29
88                 11.43
134                 7.86
895                 7.14
1321                7.14
1446                7.14
1447                7.14
26                  6.43
1000                5.71
1509                5.00
927                 2.86
1526                2.86
1235                0.71
Among 2036 features, 2022 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 102.26s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              20
2025                                  sz_vol              14
895                                hs300_vol              12
342               (60k_high-60k_low)/60k_avg              12
452                   (close/p60min_close-1)              10
430                  (close/p120max_close-1)              10
454                           (high-low)/avg              10
567                                      amt               8
1449   p600mv_(120k_low-120k_close)/120k_avg               7
1009  p1250mv_(250k_low-250k_close)/250k_avg               5
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               5
1705                                  sh_vol               4
1526         p80mv_(20k_low/p20mv_20k_low-1)               3
1467       p60mv_(20k_open/p20mv_20k_open-1)               2
1471      p60mv_(60k_close-60k_open)/60k_avg               2
468                   (high/p250mean_high-1)               2
1536            p9mv_(3k_high-3k_low)/3k_avg               2
234                  (3k_high-3k_low)/3k_avg               2
511                       (low/p60min_low-1)               2
1525        p80mv_(20k_low-20k_open)/20k_avg               2

      importance_percent
1865               14.29
2025               10.00
895                 8.57
342                 8.57
452                 7.14
430                 7.14
454                 7.14
567                 5.71
1449                5.00
1009                3.57
1061                3.57
1705                2.86
1526                2.14
1467                1.43
1471                1.43
468                 1.43
1536                1.43
234                 1.43
511                 1.43
1525                1.43
Among 2036 features, 2011 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 103.09s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 97.08s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              30               21.43
2025                              sz_vol              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
538                 (open/p60min_open-1)              10                7.14
432              (close/p120min_close-1)              10                7.14
134           (20k_high-20k_low)/20k_avg              10                7.14
345         (60k_high/p120mv_60k_high-1)               9                6.43
1138          p20mv_lower_shadow/20k_avg               9                6.43
399                   (avg/p10max_avg-1)               7                5.00
303               (5k_low/p5mv_5k_low-1)               3                2.14
347         (60k_high/p240mv_60k_high-1)               1                0.71
1473  p60mv_(60k_high-60k_close)/60k_avg               1                0.71
Among 2036 features, 2021 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 101.48s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
342       (60k_high-60k_low)/60k_avg              12                8.57
454                   (high-low)/avg              10                7.14
2025                          sz_vol               9                6.43
895                        hs300_vol               9                6.43
567                              amt               9                6.43
1865                        sz50_vol               8                5.71
134       (20k_high-20k_low)/20k_avg               6                4.29
26        (10k_high-10k_low)/10k_avg               6                4.29
234          (3k_high-3k_low)/3k_avg               5                3.57
80     (120k_high-120k_low)/120k_avg               4                2.86
1125    p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
1705                          sh_vol               3                2.14
142      (20k_low-20k_close)/20k_avg               3                2.14
1418   p5mv_(5k_low-5k_close)/5k_avg               3                2.14
1491    p6mv_(3k_high-3k_low)/3k_avg               2                1.43
337   (60k_close/p180mv_60k_close-1)               2                1.43
468           (high/p250mean_high-1)               2                1.43
2031             upper_shadow/5k_avg               2                1.43
908                           market               2                1.43
1097           p1mv_upper_shadow/avg               2                1.43
Among 2036 features, 1980 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 106.80s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              13
567                                      amt              11
342               (60k_high-60k_low)/60k_avg              10
430                  (close/p120max_close-1)               8
2025                                  sz_vol               8
234                  (3k_high-3k_low)/3k_avg               7
1705                                  sh_vol               6
895                                hs300_vol               5
1009  p1250mv_(250k_low-250k_close)/250k_avg               4
1283           p3mv_(3k_low-3k_close)/3k_avg               4
511                       (low/p60min_low-1)               4
454                           (high-low)/avg               3
1235        p300mv_(60k_low/p60mv_60k_low-1)               2
1063     p180mv_(60k_close-60k_open)/60k_avg               2
80             (120k_high-120k_low)/120k_avg               2
26                (10k_high-10k_low)/10k_avg               2
142              (20k_low-20k_close)/20k_avg               2
1449   p600mv_(120k_low-120k_close)/120k_avg               2
1375     p500mv_(250k_avg/p250mv_250k_avg-1)               2
1471      p60mv_(60k_close-60k_open)/60k_avg               2

      importance_percent
1865                9.29
567                 7.86
342                 7.14
430                 5.71
2025                5.71
234                 5.00
1705                4.29
895                 3.57
1009                2.86
1283                2.86
511                 2.86
454                 2.14
1235                1.43
1063                1.43
80                  1.43
26                  1.43
142                 1.43
1449                1.43
1375                1.43
1471                1.43
Among 2036 features, 1976 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 107.76s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              14
2025                                  sz_vol              13
430                  (close/p120max_close-1)               8
567                                      amt               6
1705                                  sh_vol               6
986      p120mv_(60k_close-60k_open)/60k_avg               4
895                                hs300_vol               4
1009  p1250mv_(250k_low-250k_close)/250k_avg               4
1449   p600mv_(120k_low-120k_close)/120k_avg               3
454                           (high-low)/avg               3
1268            p360mv_lower_shadow/120k_avg               3
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               3
1347     p480mv_(120k_low/p120mv_120k_low-1)               3
2034                                     vol               3
132            (20k_close/p80mv_20k_close-1)               2
1350            p480mv_lower_shadow/120k_avg               2
1452   p600mv_(120k_open/p120mv_120k_open-1)               2
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               2
83            (120k_high/p120mv_120k_high-1)               2
1000             p120mv_upper_shadow/60k_avg               2

      importance_percent
1865               10.00
2025                9.29
430                 5.71
567                 4.29
1705                4.29
986                 2.86
895                 2.86
1009                2.86
1449                2.14
454                 2.14
1268                2.14
1061                2.14
1347                2.14
2034                2.14
132                 1.43
1350                1.43
1452                1.43
1457                1.43
83                  1.43
1000                1.43
Among 2036 features, 1971 features are not used in the model
2013-01-01 2018-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 169181
<class 'pandas.core.frame.DataFrame'>
Int64Index: 169181 entries, 20130807 to 20180403
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 701.4 MB
None
(70, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170110       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170706       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170303       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170425       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171116       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170227       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170822       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
20171214    NaN
20171115    NaN
20170110    NaN
20170706    NaN
20170303    NaN
20170425    NaN
20170125    NaN
20171116    NaN
20170227    NaN
20170822    NaN
Train dates:20130104-20180530
(166030, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_decline
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 127.57s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
1705                                     sh_vol              44
895                                   hs300_vol              44
1865                                   sz50_vol              39
2025                                     sz_vol              37
2048        layer0_custom_revenue2_y_s_avg_pred              28
732                                     cyb_vol              28
2112     layer0_custom_revenue_y_s_decline_pred              16
2148        layer0_custom_revenue2_y_l_avg_pred              11
2086     layer0_custom_revenue_y_l_decline_pred               9
2036         layer0_custom_revenue_y_l_avg_pred               8
1526            p80mv_(20k_low/p20mv_20k_low-1)               7
2124       layer0_custom_revenue2_y_s_rise_pred               7
433                        (close/p1mv_close-1)               6
2159  layer0_custom_revenue2_y_l_avg_tree9_leaf               6
937          p100mv_(20k_open/p20mv_20k_open-1)               6
938            p100mv_(20k_vol/p20mv_20k_vol-1)               5
1088                        p1mv_(high-low)/avg               5
2072         layer0_custom_revenue_y_s_avg_pred               5
1202                  p25mv_upper_shadow/5k_avg               4
1465           p60mv_(20k_low-20k_open)/20k_avg               4

      importance_percent
1705                5.87
895                 5.87
1865                5.20
2025                4.93
2048                3.73
732                 3.73
2112                2.13
2148                1.47
2086                1.20
2036                1.07
1526                0.93
2124                0.93
433                 0.80
2159                0.80
937                 0.80
938                 0.67
1088                0.67
2072                0.67
1202                0.53
1465                0.53
Among 2160 features, 1851 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 129.06s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              41
1865                                    sz50_vol              29
1705                                      sh_vol              28
2025                                      sz_vol              26
2112      layer0_custom_revenue_y_s_decline_pred              15
567                                          amt              14
2048         layer0_custom_revenue2_y_s_avg_pred              14
2136        layer0_custom_revenue2_y_l_rise_pred              13
2086      layer0_custom_revenue_y_l_decline_pred              12
732                                      cyb_vol              12
895                                    hs300_vol              10
1526             p80mv_(20k_low/p20mv_20k_low-1)               9
2145  layer0_custom_revenue2_y_l_rise_tree7_leaf               9
1463           p60mv_(20k_high/p20mv_20k_high-1)               7
2036          layer0_custom_revenue_y_l_avg_pred               7
568                                         area               7
1466             p60mv_(20k_low/p20mv_20k_low-1)               6
1346        p480mv_(120k_low-120k_open)/120k_avg               5
453                             (high-close)/avg               5
81                (120k_high-120k_open)/120k_avg               5

      importance_percent
2148                5.47
1865                3.87
1705                3.73
2025                3.47
2112                2.00
567                 1.87
2048                1.87
2136                1.73
2086                1.60
732                 1.60
895                 1.33
1526                1.20
2145                1.20
1463                0.93
2036                0.93
568                 0.93
1466                0.80
1346                0.67
453                 0.67
81                  0.67
Among 2160 features, 1840 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 127.88s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              39
2136        layer0_custom_revenue2_y_l_rise_pred              20
568                                         area              19
1865                                    sz50_vol              17
2048         layer0_custom_revenue2_y_s_avg_pred              15
1705                                      sh_vol              14
567                                          amt              13
2025                                      sz_vol              11
2137      layer0_custom_revenue2_y_l_rise_output              10
2035                                        vol0               9
2098         layer0_custom_revenue_y_l_rise_pred               7
454                               (high-low)/avg               7
1511         p750mv_(250k_low/p250mv_250k_low-1)               6
895                                    hs300_vol               6
2112      layer0_custom_revenue_y_s_decline_pred               5
2124        layer0_custom_revenue2_y_s_rise_pred               5
2036          layer0_custom_revenue_y_l_avg_pred               5
2145  layer0_custom_revenue2_y_l_rise_tree7_leaf               5
1456             p60mv_(20k_amt/p20mv_20k_amt-1)               5
1111             p20mv_(20k_amt/p20mv_20k_amt-1)               4

      importance_percent
2148                5.20
2136                2.67
568                 2.53
1865                2.27
2048                2.00
1705                1.87
567                 1.73
2025                1.47
2137                1.33
2035                1.20
2098                0.93
454                 0.93
1511                0.80
895                 0.80
2112                0.67
2124                0.67
2036                0.67
2145                0.67
1456                0.67
1111                0.53
Among 2160 features, 1744 features are not used in the model

Train l2_y_s_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 133.04s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              45
2025                                  sz_vol              36
1705                                  sh_vol              36
895                                hs300_vol              33
732                                  cyb_vol              27
2048     layer0_custom_revenue2_y_s_avg_pred              26
2148     layer0_custom_revenue2_y_l_avg_pred              16
2112  layer0_custom_revenue_y_s_decline_pred              16
2073    layer0_custom_revenue_y_s_avg_output               7
1094                 p1mv_(open/p1mv_open-1)               7
453                         (high-close)/avg               7
2136    layer0_custom_revenue2_y_l_rise_pred               6
2086  layer0_custom_revenue_y_l_decline_pred               5
937       p100mv_(20k_open/p20mv_20k_open-1)               5
1493           p6mv_(3k_high/p3mv_3k_high-1)               5
46               (10k_open/p40mv_10k_open-1)               4
958          p10mv_(5k_high-5k_close)/5k_avg               4
2036      layer0_custom_revenue_y_l_avg_pred               4
1535          p9mv_(3k_high-3k_close)/3k_avg               4
1198            p25mv_(5k_low/p5mv_5k_low-1)               4

      importance_percent
1865                6.00
2025                4.80
1705                4.80
895                 4.40
732                 3.60
2048                3.47
2148                2.13
2112                2.13
2073                0.93
1094                0.93
453                 0.93
2136                0.80
2086                0.67
937                 0.67
1493                0.67
46                  0.53
958                 0.53
2036                0.53
1535                0.53
1198                0.53
Among 2160 features, 1825 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 133.38s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2124    layer0_custom_revenue2_y_s_rise_pred              32
2048     layer0_custom_revenue2_y_s_avg_pred              24
1705                                  sh_vol              23
2025                                  sz_vol              22
1865                                sz50_vol              18
2148     layer0_custom_revenue2_y_l_avg_pred              16
2136    layer0_custom_revenue2_y_l_rise_pred              15
895                                hs300_vol              13
454                           (high-low)/avg               9
2137  layer0_custom_revenue2_y_l_rise_output               7
2033                        upper_shadow/avg               6
453                         (high-close)/avg               6
2112  layer0_custom_revenue_y_s_decline_pred               6
732                                  cyb_vol               6
1195          p25mv_(5k_high/p5mv_5k_high-1)               6
568                                     area               5
1087                   p1mv_(high-close)/avg               4
1516         p80mv_(20k_amt/p20mv_20k_amt-1)               4
2034                                     vol               4
1094                 p1mv_(open/p1mv_open-1)               4

      importance_percent
2124                4.27
2048                3.20
1705                3.07
2025                2.93
1865                2.40
2148                2.13
2136                2.00
895                 1.73
454                 1.20
2137                0.93
2033                0.80
453                 0.80
2112                0.80
732                 0.80
1195                0.80
568                 0.67
1087                0.53
1516                0.53
2034                0.53
1094                0.53
Among 2160 features, 1754 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 130.09s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2148        layer0_custom_revenue2_y_l_avg_pred              49
1865                                   sz50_vol              37
2025                                     sz_vol              35
1705                                     sh_vol              30
732                                     cyb_vol              23
2159  layer0_custom_revenue2_y_l_avg_tree9_leaf              16
1526            p80mv_(20k_low/p20mv_20k_low-1)              14
895                                   hs300_vol              13
2086     layer0_custom_revenue_y_l_decline_pred              13
2112     layer0_custom_revenue_y_s_decline_pred              12
1466            p60mv_(20k_low/p20mv_20k_low-1)              11
567                                         amt               9
2154  layer0_custom_revenue2_y_l_avg_tree4_leaf               8
1341     p480mv_(120k_high-120k_close)/120k_avg               8
2155  layer0_custom_revenue2_y_l_avg_tree5_leaf               7
1350               p480mv_lower_shadow/120k_avg               7
1450       p600mv_(120k_low-120k_open)/120k_avg               7
1108            p20mv_(10k_low/p10mv_10k_low-1)               6
939                 p100mv_lower_shadow/20k_avg               6
1457            p60mv_(20k_avg/p20mv_20k_avg-1)               6

      importance_percent
2148                6.53
1865                4.93
2025                4.67
1705                4.00
732                 3.07
2159                2.13
1526                1.87
895                 1.73
2086                1.73
2112                1.60
1466                1.47
567                 1.20
2154                1.07
1341                1.07
2155                0.93
1350                0.93
1450                0.93
1108                0.80
939                 0.80
1457                0.80
Among 2160 features, 1880 features are not used in the model
2013-01-01 2018-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 169009
<class 'pandas.core.frame.DataFrame'>
Int64Index: 169009 entries, 20130819 to 20180110
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 700.6 MB
None
(67, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170223       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170522       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170515       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170526       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170920       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171120       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170908       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
20170223    NaN
20171222    NaN
20170124    NaN
20170522    NaN
20170515    NaN
20170526    NaN
20170920    NaN
20171120    NaN
20171027    NaN
20171027    NaN
20170908    NaN
Train dates:20130104-20180530
(165884, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 165.09s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=50,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2186                  layer1_l2_y_l_avg_pred             117
2290              layer1_l2_y_l_decline_pred             116
2212                 layer1_l2_y_l_rise_pred              96
2148     layer0_custom_revenue2_y_l_avg_pred              49
2238                  layer1_l2_y_s_avg_pred              43
568                                     area              25
2160              layer1_l2_y_s_decline_pred              21
1705                                  sh_vol              19
2025                                  sz_vol              17
2264                 layer1_l2_y_s_rise_pred              16
1865                                sz50_vol              13
895                                hs300_vol              13
2098     layer0_custom_revenue_y_l_rise_pred              12
732                                  cyb_vol              10
1241         p30mv_(10k_avg/p10mv_10k_avg-1)               9
2112  layer0_custom_revenue_y_s_decline_pred               9
2086  layer0_custom_revenue_y_l_decline_pred               9
2208           layer1_l2_y_l_avg_tree21_leaf               9
2239            layer1_l2_y_s_avg_tree0_leaf               8
2193            layer1_l2_y_l_avg_tree6_leaf               7

      importance_percent
2186                7.80
2290                7.73
2212                6.40
2148                3.27
2238                2.87
568                 1.67
2160                1.40
1705                1.27
2025                1.13
2264                1.07
1865                0.87
895                 0.87
2098                0.80
732                 0.67
1241                0.60
2112                0.60
2086                0.60
2208                0.60
2239                0.53
2193                0.47
Among 2316 features, 1750 features are not used in the model
test range: 20180701 20190101
2018-07-01 2018-12-31
Time slice keys in hdf5: 2018/0101-0101

Current key: 2018/0101-0101
Current slice size(length): 347712

Total concatenating size: 347712
Result dataset size: 178346

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2     -0.121       -0.121     0.006    -0.117    -0.126
(-0.20,-0.15]     15      0.007       -0.056     0.142     0.258    -0.139
(-0.15,-0.10]    100     -0.069       -0.096     0.108     0.318    -0.220
(-0.10,-0.05]   1968     -0.044       -0.070     0.115     0.647    -0.422
(-0.05,0.00]   27282     -0.033       -0.054     0.105     0.736    -0.612
(0.00,0.05]    93200     -0.013       -0.033     0.101     0.856    -0.617
(0.05,0.10]    41019      0.004       -0.022     0.118     0.989    -0.459
(0.10,0.15]    10520      0.026        0.052     0.142     1.029    -0.387
(0.15,0.20]     3045      0.039        0.064     0.154     0.843    -0.341
(0.20,0.25]      786      0.044        0.078     0.159     0.709    -0.314
(0.25,0.30]      122      0.043        0.079     0.162     0.432    -0.327
(0.30,0.35]       19      0.126        0.169     0.172     0.323    -0.205
(0.35,0.40]        4      0.136        0.142     0.028     0.161     0.097
(0.40,0.45]        3      0.258        0.260     0.034     0.290     0.222
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2     -0.177       -0.177     0.004    -0.174    -0.180
(-0.20,-0.15]     15      0.006       -0.080     0.203     0.335    -0.177
(-0.15,-0.10]    100     -0.096       -0.144     0.153     0.439    -0.298
(-0.10,-0.05]   1968     -0.068       -0.111     0.157     0.852    -0.534
(-0.05,0.00]   27282     -0.052       -0.090     0.145     0.920    -0.677
(0.00,0.05]    93200     -0.025       -0.062     0.141     1.077    -0.674
(0.05,0.10]    41019     -0.001       -0.059     0.166     1.304    -0.544
(0.10,0.15]    10520      0.030        0.082     0.200     1.328    -0.452
(0.15,0.20]     3045      0.046        0.104     0.219     1.044    -0.429
(0.20,0.25]      786      0.054        0.121     0.228     0.890    -0.402
(0.25,0.30]      122      0.063        0.147     0.233     0.613    -0.381
(0.30,0.35]       19      0.194        0.252     0.229     0.420    -0.269
(0.35,0.40]        4      0.203        0.213     0.041     0.239     0.146
(0.40,0.45]        3      0.363        0.372     0.037     0.395     0.323
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2     -0.066       -0.066     0.009    -0.060    -0.072
(-0.20,-0.15]     15      0.008       -0.032     0.084     0.189    -0.100
(-0.15,-0.10]    100     -0.042       -0.053     0.067     0.196    -0.167
(-0.10,-0.05]   1968     -0.027       -0.029     0.075     0.443    -0.310
(-0.05,0.00]   27282     -0.020       -0.017     0.067     0.563    -0.564
(0.00,0.05]    93200     -0.007       -0.006     0.064     0.670    -0.570
(0.05,0.10]    41019      0.004       -0.001     0.074     0.721    -0.406
(0.10,0.15]    10520      0.018        0.012     0.089     0.733    -0.346
(0.15,0.20]     3045      0.025        0.019     0.095     0.659    -0.291
(0.20,0.25]      786      0.025        0.026     0.098     0.554    -0.273
(0.25,0.30]      122      0.028        0.030     0.095     0.251    -0.284
(0.30,0.35]       19      0.074        0.088     0.108     0.226    -0.151
(0.35,0.40]        4      0.068        0.071     0.015     0.082     0.048
(0.40,0.45]        3      0.152        0.149     0.033     0.186     0.121
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2      0.024        0.024     0.005     0.027     0.020
(-0.20,-0.15]     15      0.116        0.044     0.127     0.335     0.002
(-0.15,-0.10]    100      0.057        0.032     0.071     0.439     0.000
(-0.10,-0.05]   1968      0.066        0.043     0.078     0.852     0.000
(-0.05,0.00]   27282      0.063        0.042     0.069     0.920     0.000
(0.00,0.05]    93200      0.068        0.049     0.070     1.077     0.000
(0.05,0.10]    41019      0.093        0.069     0.089     1.304     0.000
(0.10,0.15]    10520      0.126        0.100     0.111     1.328     0.000
(0.15,0.20]     3045      0.145        0.114     0.125     1.044     0.000
(0.20,0.25]      786      0.158        0.134     0.122     0.890     0.000
(0.25,0.30]      122      0.166        0.151     0.125     0.613     0.000
(0.30,0.35]       19      0.245        0.252     0.139     0.420     0.000
(0.35,0.40]        4      0.203        0.213     0.041     0.239     0.146
(0.40,0.45]        3      0.363        0.372     0.037     0.395     0.323
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

test period:20190101-20190701
dataset range: 20130101 20190101
2013-01-01 2018-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 186915
<class 'pandas.core.frame.DataFrame'>
Int64Index: 186915 entries, 20130607 to 20180925
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 774.9 MB
None
(95, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170324       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170707       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170405       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
20170324    NaN
20170707    NaN
20170926    NaN
20170926    NaN
20170405    NaN
Train dates:20130104-20181129
(183796, 2036)

----------Train layer 0----------

Train custom_revenue_y_l_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 121.50s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
2025                                 sz_vol              20
382                    (amt/p250mean_amt-1)              10
1536           p9mv_(3k_high-3k_low)/3k_avg              10
1347    p480mv_(120k_low/p120mv_120k_low-1)              10
430                 (close/p120max_close-1)              10
1865                               sz50_vol              10
1000            p120mv_upper_shadow/60k_avg              10
1509  p750mv_(250k_low-250k_close)/250k_avg              10
1266  p360mv_(120k_open/p120mv_120k_open-1)              10
1457        p60mv_(20k_avg/p20mv_20k_avg-1)              10
1268           p360mv_lower_shadow/120k_avg              10
1396      p50mv_(10k_high/p10mv_10k_high-1)               7
81           (120k_high-120k_open)/120k_avg               5
110                      (20MA/p2mv_20MA-1)               4
1138             p20mv_lower_shadow/20k_avg               2
1321        p40mv_(20k_avg/p20mv_20k_avg-1)               2

      importance_percent
2025               14.29
382                 7.14
1536                7.14
1347                7.14
430                 7.14
1865                7.14
1000                7.14
1509                7.14
1266                7.14
1457                7.14
1268                7.14
1396                5.00
81                  3.57
110                 2.86
1138                1.43
1321                1.43
Among 2036 features, 2020 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 125.99s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
895                               hs300_vol              20
342              (60k_high-60k_low)/60k_avg              15
1865                               sz50_vol              12
2025                                 sz_vol              12
454                          (high-low)/avg               9
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               5
567                                     amt               5
1705                                 sh_vol               4
22            (10k_close/p30mv_10k_close-1)               4
430                 (close/p120max_close-1)               4
1449  p600mv_(120k_low-120k_close)/120k_avg               4
2034                                    vol               3
462                   (high/p120min_high-1)               2
453                        (high-close)/avg               2
1102     p20mv_(10k_high-10k_close)/10k_avg               2
732                                 cyb_vol               2
1491           p6mv_(3k_high-3k_low)/3k_avg               2
1309     p40mv_(10k_close-10k_open)/10k_avg               2
2035                                   vol0               2
1490         p6mv_(3k_high-3k_close)/3k_avg               2

      importance_percent
895                14.29
342                10.71
1865                8.57
2025                8.57
454                 6.43
1125                3.57
567                 3.57
1705                2.86
22                  2.86
430                 2.86
1449                2.86
2034                2.14
462                 1.43
453                 1.43
1102                1.43
732                 1.43
1491                1.43
1309                1.43
2035                1.43
1490                1.43
Among 2036 features, 1991 features are not used in the model

Train custom_revenue_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 120.19s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                               feature  importance_raw  importance_percent
134         (20k_high-20k_low)/20k_avg              20               14.29
342         (60k_high-60k_low)/60k_avg              20               14.29
1865                          sz50_vol              14               10.00
2035                              vol0              13                9.29
454                     (high-low)/avg              10                7.14
234            (3k_high-3k_low)/3k_avg              10                7.14
895                          hs300_vol              10                7.14
1107  p20mv_(10k_low-10k_open)/10k_avg               9                6.43
404                (avg/p120min_avg-1)               8                5.71
1495      p6mv_(3k_low-3k_open)/3k_avg               6                4.29
405                   (avg/p1mv_avg-1)               5                3.57
1490    p6mv_(3k_high-3k_close)/3k_avg               4                2.86
1219               p2mv_(low-open)/avg               3                2.14
1210             p2mv_(amt/p1mv_amt-1)               3                2.14
424                 (avg/p60min_avg-1)               2                1.43
243            (3k_low-3k_open)/3k_avg               1                0.71
492                   (low/p1mv_low-1)               1                0.71
567                                amt               1                0.71
Among 2036 features, 2018 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 123.32s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
895                               hs300_vol              20
2025                                 sz_vol              20
1865                               sz50_vol              16
22            (10k_close/p30mv_10k_close-1)              10
1309     p40mv_(10k_close-10k_open)/10k_avg              10
303                  (5k_low/p5mv_5k_low-1)              10
1138             p20mv_lower_shadow/20k_avg              10
1449  p600mv_(120k_low-120k_close)/120k_avg              10
1490         p6mv_(3k_high-3k_close)/3k_avg               6
372                     (amt/p10mean_amt-1)               6
1361                  p4mv_(close-open)/avg               6
248                  (3k_low/p6mv_3k_low-1)               4
541                     (vol/p10mean_vol-1)               4
1705                                 sh_vol               4
1491           p6mv_(3k_high-3k_low)/3k_avg               4

      importance_percent
895                14.29
2025               14.29
1865               11.43
22                  7.14
1309                7.14
303                 7.14
1138                7.14
1449                7.14
1490                4.29
372                 4.29
1361                4.29
248                 2.86
541                 2.86
1705                2.86
1491                2.86
Among 2036 features, 2021 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 114.29s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 120.13s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
80            (120k_high-120k_low)/120k_avg              20
567                                     amt              20
1446   p600mv_(120k_high-120k_low)/120k_avg              18
1000            p120mv_upper_shadow/60k_avg              16
134              (20k_high-20k_low)/20k_avg              11
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1507  p750mv_(250k_high-250k_open)/250k_avg              10
1509  p750mv_(250k_low-250k_close)/250k_avg               8
1465       p60mv_(20k_low-20k_open)/20k_avg               8
1466        p60mv_(20k_low/p20mv_20k_low-1)               6
538                    (open/p60min_open-1)               6
325              (60k_amt/p240mv_60k_amt-1)               4
895                               hs300_vol               2
26               (10k_high-10k_low)/10k_avg               1

      importance_percent
80                 14.29
567                14.29
1446               12.86
1000               11.43
134                 7.86
1321                7.14
1507                7.14
1509                5.71
1465                5.71
1466                4.29
538                 4.29
325                 2.86
895                 1.43
26                  0.71
Among 2036 features, 2022 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 120.43s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
1705                              sh_vol              16               11.43
342           (60k_high-60k_low)/60k_avg              16               11.43
2025                              sz_vol              12                8.57
454                       (high-low)/avg              10                7.14
1865                            sz50_vol               8                5.71
430              (close/p120max_close-1)               8                5.71
567                                  amt               7                5.00
2027               upper_shadow/120k_avg               7                5.00
234              (3k_high-3k_low)/3k_avg               6                4.29
468               (high/p250mean_high-1)               6                4.29
496                  (low/p250max_low-1)               5                3.57
1112     p20mv_(20k_avg/p20mv_20k_avg-1)               4                2.86
1350        p480mv_lower_shadow/120k_avg               4                2.86
133         (20k_high-20k_close)/20k_avg               2                1.43
1268        p360mv_lower_shadow/120k_avg               2                1.43
895                            hs300_vol               2                1.43
2035                                vol0               2                1.43
132        (20k_close/p80mv_20k_close-1)               2                1.43
452               (close/p60min_close-1)               2                1.43
1518  p80mv_(20k_close-20k_open)/20k_avg               2                1.43
Among 2036 features, 2002 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 116.12s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 126.10s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023F80A6D730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              41               29.29
432              (close/p120min_close-1)              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)               9                6.43
2025                              sz_vol               7                5.00
399                   (avg/p10max_avg-1)               7                5.00
134           (20k_high-20k_low)/20k_avg               7                5.00
441                (close/p3max_close-1)               5                3.57
185       (250k_high-250k_open)/250k_avg               3                2.14
288              (5k_high-5k_low)/5k_avg               3                2.14
1705                              sh_vol               3                2.14
1865                            sz50_vol               3                2.14
550                  (vol/p250max_vol-1)               3                2.14
303               (5k_low/p5mv_5k_low-1)               3                2.14
1120    p20mv_(20k_low-20k_open)/20k_avg               2                1.43
968            p10mv_lower_shadow/5k_avg               1                0.71
1021       p12mv_(3k_high-3k_low)/3k_avg               1                0.71
732                              cyb_vol               1                0.71
Among 2036 features, 2015 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 113.26s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
895                               hs300_vol              13
342              (60k_high-60k_low)/60k_avg              12
454                          (high-low)/avg              10
134              (20k_high-20k_low)/20k_avg               9
2025                                 sz_vol               9
567                                     amt               8
80            (120k_high-120k_low)/120k_avg               7
234                 (3k_high-3k_low)/3k_avg               6
1865                               sz50_vol               6
1418          p5mv_(5k_low-5k_close)/5k_avg               4
26               (10k_high-10k_low)/10k_avg               4
142             (20k_low-20k_close)/20k_avg               4
1705                                 sh_vol               3
1449  p600mv_(120k_low-120k_close)/120k_avg               3
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               3
468                  (high/p250mean_high-1)               3
482                    (high/p60min_high-1)               2
430                 (close/p120max_close-1)               2
453                        (high-close)/avg               2
1415           p5mv_(5k_high-5k_low)/5k_avg               2

      importance_percent
895                 9.29
342                 8.57
454                 7.14
134                 6.43
2025                6.43
567                 5.71
80                  5.00
234                 4.29
1865                4.29
1418                2.86
26                  2.86
142                 2.86
1705                2.14
1449                2.14
1125                2.14
468                 2.14
482                 1.43
430                 1.43
453                 1.43
1415                1.43
Among 2036 features, 1990 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 120.96s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
567                                      amt              12
342               (60k_high-60k_low)/60k_avg               9
1705                                  sh_vol               9
2025                                  sz_vol               9
430                  (close/p120max_close-1)               8
1865                                sz50_vol               7
234                  (3k_high-3k_low)/3k_avg               6
2027                   upper_shadow/120k_avg               6
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               5
134               (20k_high-20k_low)/20k_avg               4
1503  p750mv_(250k_close-250k_open)/250k_avg               3
1283           p3mv_(3k_low-3k_close)/3k_avg               3
142              (20k_low-20k_close)/20k_avg               3
438                 (close/p250mean_close-1)               3
1526         p80mv_(20k_low/p20mv_20k_low-1)               3
454                           (high-low)/avg               3
496                      (low/p250max_low-1)               2
895                                hs300_vol               2
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
1341  p480mv_(120k_high-120k_close)/120k_avg               2

      importance_percent
567                 8.57
342                 6.43
1705                6.43
2025                6.43
430                 5.71
1865                5.00
234                 4.29
2027                4.29
1112                3.57
134                 2.86
1503                2.14
1283                2.14
142                 2.14
438                 2.14
1526                2.14
454                 2.14
496                 1.43
895                 1.43
1009                1.43
1341                1.43
Among 2036 features, 1980 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 116.43s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023F80A6D8C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2025                                  sz_vol              13
1865                                sz50_vol              11
1705                                  sh_vol               7
454                           (high-low)/avg               5
986      p120mv_(60k_close-60k_open)/60k_avg               4
1347     p480mv_(120k_low/p120mv_120k_low-1)               4
1503  p750mv_(250k_close-250k_open)/250k_avg               4
895                                hs300_vol               3
567                                      amt               3
1138              p20mv_lower_shadow/20k_avg               3
1268            p360mv_lower_shadow/120k_avg               3
2034                                     vol               3
1509   p750mv_(250k_low-250k_close)/250k_avg               3
430                  (close/p120max_close-1)               3
1064    p180mv_(60k_close/p60mv_60k_close-1)               2
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
79           (120k_high-120k_close)/120k_avg               2
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               2
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               2
489                      (low/p120max_low-1)               2

      importance_percent
2025                9.29
1865                7.86
1705                5.00
454                 3.57
986                 2.86
1347                2.86
1503                2.86
895                 2.14
567                 2.14
1138                2.14
1268                2.14
2034                2.14
1509                2.14
430                 2.14
1064                1.43
1009                1.43
79                  1.43
1457                1.43
1061                1.43
489                 1.43
Among 2036 features, 1961 features are not used in the model
2013-01-01 2018-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 186915
<class 'pandas.core.frame.DataFrame'>
Int64Index: 186915 entries, 20130807 to 20180716
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 774.9 MB
None
(90, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170110       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170706       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170303       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170425       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171116       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170227       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170822       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
20171214    NaN
20171115    NaN
20170110    NaN
20170706    NaN
20170303    NaN
20170425    NaN
20170125    NaN
20171116    NaN
20170227    NaN
20170822    NaN
Train dates:20130104-20181129
(183769, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_decline
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 153.43s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2025                                     sz_vol              51
1705                                     sh_vol              39
1865                                   sz50_vol              38
895                                   hs300_vol              38
732                                     cyb_vol              27
2048        layer0_custom_revenue2_y_s_avg_pred              26
2112     layer0_custom_revenue_y_s_decline_pred              21
2086     layer0_custom_revenue_y_l_decline_pred              12
937          p100mv_(20k_open/p20mv_20k_open-1)              12
2148        layer0_custom_revenue2_y_l_avg_pred              11
2072         layer0_custom_revenue_y_s_avg_pred               7
2058  layer0_custom_revenue2_y_s_avg_tree8_leaf               7
1189               p25mv_(5k_avg/p5mv_5k_avg-1)               6
2073       layer0_custom_revenue_y_s_avg_output               6
1457            p60mv_(20k_avg/p20mv_20k_avg-1)               6
88               (120k_low-120k_close)/120k_avg               5
1454               p600mv_lower_shadow/120k_avg               5
2124       layer0_custom_revenue2_y_s_rise_pred               5
2158  layer0_custom_revenue2_y_l_avg_tree8_leaf               5
1463          p60mv_(20k_high/p20mv_20k_high-1)               4

      importance_percent
2025                6.80
1705                5.20
1865                5.07
895                 5.07
732                 3.60
2048                3.47
2112                2.80
2086                1.60
937                 1.60
2148                1.47
2072                0.93
2058                0.93
1189                0.80
2073                0.80
1457                0.80
88                  0.67
1454                0.67
2124                0.67
2158                0.67
1463                0.53
Among 2160 features, 1864 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 159.36s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2148        layer0_custom_revenue2_y_l_avg_pred              32
1705                                     sh_vol              26
2025                                     sz_vol              25
1865                                   sz50_vol              24
567                                         amt              16
2112     layer0_custom_revenue_y_s_decline_pred              14
1466            p60mv_(20k_low/p20mv_20k_low-1)              14
2136       layer0_custom_revenue2_y_l_rise_pred              13
732                                     cyb_vol              13
2048        layer0_custom_revenue2_y_s_avg_pred              13
2086     layer0_custom_revenue_y_l_decline_pred              11
2098        layer0_custom_revenue_y_l_rise_pred              11
895                                   hs300_vol              11
937          p100mv_(20k_open/p20mv_20k_open-1)              10
1526            p80mv_(20k_low/p20mv_20k_low-1)              10
1463          p60mv_(20k_high/p20mv_20k_high-1)               9
2137     layer0_custom_revenue2_y_l_rise_output               7
2158  layer0_custom_revenue2_y_l_avg_tree8_leaf               7
1454               p600mv_lower_shadow/120k_avg               7
2124       layer0_custom_revenue2_y_s_rise_pred               7

      importance_percent
2148                4.27
1705                3.47
2025                3.33
1865                3.20
567                 2.13
2112                1.87
1466                1.87
2136                1.73
732                 1.73
2048                1.73
2086                1.47
2098                1.47
895                 1.47
937                 1.33
1526                1.33
1463                1.20
2137                0.93
2158                0.93
1454                0.93
2124                0.93
Among 2160 features, 1842 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 149.50s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2148         layer0_custom_revenue2_y_l_avg_pred              35
568                                         area              21
2137      layer0_custom_revenue2_y_l_rise_output              18
2025                                      sz_vol              17
1865                                    sz50_vol              16
567                                          amt              15
2136        layer0_custom_revenue2_y_l_rise_pred              15
2098         layer0_custom_revenue_y_l_rise_pred              13
2048         layer0_custom_revenue2_y_s_avg_pred              11
2035                                        vol0               9
895                                    hs300_vol               7
2112      layer0_custom_revenue_y_s_decline_pred               6
2143  layer0_custom_revenue2_y_l_rise_tree5_leaf               5
1466             p60mv_(20k_low/p20mv_20k_low-1)               5
931            p100mv_(20k_high-20k_low)/20k_avg               5
2107   layer0_custom_revenue_y_l_rise_tree7_leaf               5
995           p120mv_(60k_open/p60mv_60k_open-1)               5
1517             p80mv_(20k_avg/p20mv_20k_avg-1)               5
1456             p60mv_(20k_amt/p20mv_20k_amt-1)               5
1235            p300mv_(60k_low/p60mv_60k_low-1)               5

      importance_percent
2148                4.67
568                 2.80
2137                2.40
2025                2.27
1865                2.13
567                 2.00
2136                2.00
2098                1.73
2048                1.47
2035                1.20
895                 0.93
2112                0.80
2143                0.67
1466                0.67
931                 0.67
2107                0.67
995                 0.67
1517                0.67
1456                0.67
1235                0.67
Among 2160 features, 1767 features are not used in the model

Train l2_y_s_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 155.41s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
1865                                   sz50_vol              49
895                                   hs300_vol              46
2025                                     sz_vol              42
1705                                     sh_vol              31
2048        layer0_custom_revenue2_y_s_avg_pred              23
2112     layer0_custom_revenue_y_s_decline_pred              21
732                                     cyb_vol              20
2148        layer0_custom_revenue2_y_l_avg_pred              17
937          p100mv_(20k_open/p20mv_20k_open-1)              12
2124       layer0_custom_revenue2_y_s_rise_pred               8
1094                    p1mv_(open/p1mv_open-1)               8
2058  layer0_custom_revenue2_y_s_avg_tree8_leaf               7
47                  (10k_open/p50mv_10k_open-1)               6
1437                   p5mv_lower_shadow/5k_avg               6
453                            (high-close)/avg               6
2086     layer0_custom_revenue_y_l_decline_pred               6
2073       layer0_custom_revenue_y_s_avg_output               5
1088                        p1mv_(high-low)/avg               5
1219                        p2mv_(low-open)/avg               5
549                          (vol/p20min_vol-1)               5

      importance_percent
1865                6.53
895                 6.13
2025                5.60
1705                4.13
2048                3.07
2112                2.80
732                 2.67
2148                2.27
937                 1.60
2124                1.07
1094                1.07
2058                0.93
47                  0.80
1437                0.80
453                 0.80
2086                0.80
2073                0.67
1088                0.67
1219                0.67
549                 0.67
Among 2160 features, 1844 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 158.25s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2124        layer0_custom_revenue2_y_s_rise_pred              29
2048         layer0_custom_revenue2_y_s_avg_pred              24
2025                                      sz_vol              19
1865                                    sz50_vol              19
1705                                      sh_vol              18
2148         layer0_custom_revenue2_y_l_avg_pred              16
2112      layer0_custom_revenue_y_s_decline_pred              13
895                                    hs300_vol              12
2137      layer0_custom_revenue2_y_l_rise_output              11
2033                            upper_shadow/avg               9
453                             (high-close)/avg               8
732                                      cyb_vol               7
2136        layer0_custom_revenue2_y_l_rise_pred               6
568                                         area               5
1545                    p9mv_upper_shadow/3k_avg               5
454                               (high-low)/avg               5
2142  layer0_custom_revenue2_y_l_rise_tree4_leaf               5
1516             p80mv_(20k_amt/p20mv_20k_amt-1)               4
2150   layer0_custom_revenue2_y_l_avg_tree0_leaf               4
1087                       p1mv_(high-close)/avg               4

      importance_percent
2124                3.87
2048                3.20
2025                2.53
1865                2.53
1705                2.40
2148                2.13
2112                1.73
895                 1.60
2137                1.47
2033                1.20
453                 1.07
732                 0.93
2136                0.80
568                 0.67
1545                0.67
454                 0.67
2142                0.67
1516                0.53
2150                0.53
1087                0.53
Among 2160 features, 1760 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 155.62s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2025                                     sz_vol              38
2148        layer0_custom_revenue2_y_l_avg_pred              35
1705                                     sh_vol              31
1865                                   sz50_vol              29
732                                     cyb_vol              21
1526            p80mv_(20k_low/p20mv_20k_low-1)              19
2112     layer0_custom_revenue_y_s_decline_pred              15
895                                   hs300_vol              15
927            p100mv_(20k_avg/p20mv_20k_avg-1)              13
2158  layer0_custom_revenue2_y_l_avg_tree8_leaf              12
2086     layer0_custom_revenue_y_l_decline_pred              12
1466            p60mv_(20k_low/p20mv_20k_low-1)              11
1321            p40mv_(20k_avg/p20mv_20k_avg-1)               9
1346       p480mv_(120k_low-120k_open)/120k_avg               7
1454               p600mv_lower_shadow/120k_avg               7
1238                p300mv_lower_shadow/60k_avg               7
2036         layer0_custom_revenue_y_l_avg_pred               7
937          p100mv_(20k_open/p20mv_20k_open-1)               6
939                 p100mv_lower_shadow/20k_avg               5
1465           p60mv_(20k_low-20k_open)/20k_avg               5

      importance_percent
2025                5.07
2148                4.67
1705                4.13
1865                3.87
732                 2.80
1526                2.53
2112                2.00
895                 2.00
927                 1.73
2158                1.60
2086                1.60
1466                1.47
1321                1.20
1346                0.93
1454                0.93
1238                0.93
2036                0.93
937                 0.80
939                 0.67
1465                0.67
Among 2160 features, 1862 features are not used in the model
2013-01-01 2018-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 186915
<class 'pandas.core.frame.DataFrame'>
Int64Index: 186915 entries, 20130819 to 20180110
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 774.9 MB
None
(95, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170223       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170522       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170515       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170526       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170920       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171120       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170908       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
20170223    NaN
20171222    NaN
20170124    NaN
20170522    NaN
20170515    NaN
20170526    NaN
20170920    NaN
20171120    NaN
20171027    NaN
20171027    NaN
20170908    NaN
Train dates:20130104-20181129
(183806, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 201.09s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=50,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2290              layer1_l2_y_l_decline_pred             119
2186                  layer1_l2_y_l_avg_pred             115
2212                 layer1_l2_y_l_rise_pred              91
2238                  layer1_l2_y_s_avg_pred              62
2148     layer0_custom_revenue2_y_l_avg_pred              47
568                                     area              29
2086  layer0_custom_revenue_y_l_decline_pred              27
2025                                  sz_vol              19
1865                                sz50_vol              18
2160              layer1_l2_y_s_decline_pred              15
2264                 layer1_l2_y_s_rise_pred              14
895                                hs300_vol              14
2234          layer1_l2_y_l_rise_tree21_leaf              11
1705                                  sh_vol              11
2237          layer1_l2_y_l_rise_tree24_leaf              10
2304       layer1_l2_y_l_decline_tree13_leaf               9
2112  layer0_custom_revenue_y_s_decline_pred               9
2222           layer1_l2_y_l_rise_tree9_leaf               8
1235        p300mv_(60k_low/p60mv_60k_low-1)               7
2243            layer1_l2_y_s_avg_tree4_leaf               7

      importance_percent
2290                7.93
2186                7.67
2212                6.07
2238                4.13
2148                3.13
568                 1.93
2086                1.80
2025                1.27
1865                1.20
2160                1.00
2264                0.93
895                 0.93
2234                0.73
1705                0.73
2237                0.67
2304                0.60
2112                0.60
2222                0.53
1235                0.47
2243                0.47
Among 2316 features, 1759 features are not used in the model
test range: 20190101 20190701
2019-01-01 2019-06-30
Time slice keys in hdf5: 2019/0101-0101

Current key: 2019/0101-0101
Current slice size(length): 69168

Total concatenating size: 69168
Result dataset size: 69168

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      3      0.097        0.095     0.009     0.108     0.089
(-0.20,-0.15]     39      0.266        0.230     0.235     1.254    -0.104
(-0.15,-0.10]   2339      0.148        0.146     0.162     1.304    -0.267
(-0.10,-0.05]   9410      0.123        0.116     0.160     1.491    -0.294
(-0.05,0.00]   17714      0.114        0.100     0.143     1.586    -0.402
(0.00,0.05]     9296      0.136        0.116     0.151     1.402    -0.372
(0.05,0.10]     1106      0.192        0.183     0.219     1.162    -0.278
(0.10,0.15]      281      0.232        0.271     0.242     0.808    -0.283
(0.15,0.20]       65      0.343        0.395     0.255     0.823    -0.211
(0.20,0.25]       11      0.439        0.431     0.164     0.684     0.173
(0.25,0.30]        0        NaN          NaN       NaN       NaN       NaN
(0.30,0.35]        0        NaN          NaN       NaN       NaN       NaN
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\matplotlib\pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      3      0.152        0.153     0.019     0.170     0.132
(-0.20,-0.15]     39      0.371        0.340     0.316     1.672    -0.205
(-0.15,-0.10]   2339      0.198        0.210     0.230     1.720    -0.366
(-0.10,-0.05]   9410      0.161        0.169     0.226     2.121    -0.392
(-0.05,0.00]   17714      0.151        0.145     0.199     2.227    -0.501
(0.00,0.05]     9296      0.181        0.161     0.206     1.802    -0.490
(0.05,0.10]     1106      0.247        0.246     0.305     1.406    -0.391
(0.10,0.15]      281      0.295        0.379     0.346     1.194    -0.388
(0.15,0.20]       65      0.462        0.525     0.363     1.097    -0.301
(0.20,0.25]       11      0.626        0.565     0.226     0.933     0.301
(0.25,0.30]        0        NaN          NaN       NaN       NaN       NaN
(0.30,0.35]        0        NaN          NaN       NaN       NaN       NaN
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      3      0.043        0.045     0.017     0.059     0.025
(-0.20,-0.15]     39      0.161        0.137     0.159     0.836    -0.002
(-0.15,-0.10]   2339      0.092        0.081     0.104     0.895    -0.197
(-0.10,-0.05]   9410      0.078        0.062     0.104     0.976    -0.203
(-0.05,0.00]   17714      0.073        0.055     0.093     0.965    -0.303
(0.00,0.05]     9296      0.088        0.070     0.101     1.002    -0.255
(0.05,0.10]     1106      0.130        0.112     0.145     0.918    -0.165
(0.10,0.15]      281      0.156        0.168     0.155     0.504    -0.179
(0.15,0.20]       65      0.220        0.258     0.160     0.574    -0.121
(0.20,0.25]       11      0.253        0.277     0.117     0.438     0.045
(0.25,0.30]        0        NaN          NaN       NaN       NaN       NaN
(0.30,0.35]        0        NaN          NaN       NaN       NaN       NaN
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      3      0.152        0.153     0.019     0.170     0.132
(-0.20,-0.15]     39      0.381        0.340     0.303     1.672     0.135
(-0.15,-0.10]   2339      0.239        0.210     0.177     1.720     0.000
(-0.10,-0.05]   9410      0.206        0.170     0.174     2.121     0.000
(-0.05,0.00]   17714      0.185        0.146     0.160     2.227     0.000
(0.00,0.05]     9296      0.206        0.162     0.174     1.802     0.000
(0.05,0.10]     1106      0.301        0.247     0.237     1.406     0.000
(0.10,0.15]      281      0.366        0.379     0.249     1.194     0.000
(0.15,0.20]       65      0.508        0.525     0.284     1.097     0.044
(0.20,0.25]       11      0.626        0.565     0.226     0.933     0.301
(0.25,0.30]        0        NaN          NaN       NaN       NaN       NaN
(0.30,0.35]        0        NaN          NaN       NaN       NaN       NaN
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

Process finished with exit code 0















// output lgbm n=25
C:\Users\dell-pc\Anaconda3.6\python.exe C:/Users/dell-pc/Quant/Quant/scribble_script.py

test period:20160701-20170101
dataset range: 20130101 20160701
2013-01-01 2016-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 102536
<class 'pandas.core.frame.DataFrame'>
Int64Index: 102536 entries, 20130607 to 20160309
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 425.1 MB
None
(0, 8)
Empty DataFrame
Columns: [y_l_rise, y_l_decline, y_l_avg, y_s_rise, y_s_decline, y_s_avg, y_l, y_l_r]
Index: []
Train dates:20130104-20160527
(99886, 2036)

----------Train layer 0----------

Train custom_revenue_y_s_rise
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 39.25s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
1865                        sz50_vol              26               18.57
342       (60k_high-60k_low)/60k_avg              20               14.29
895                        hs300_vol              14               10.00
454                   (high-low)/avg              10                7.14
468           (high/p250mean_high-1)              10                7.14
1490  p6mv_(3k_high-3k_close)/3k_avg              10                7.14
2035                            vol0               7                5.00
243          (3k_low-3k_open)/3k_avg               7                5.00
1415    p5mv_(5k_high-5k_low)/5k_avg               7                5.00
234          (3k_high-3k_low)/3k_avg               7                5.00
22     (10k_close/p30mv_10k_close-1)               7                5.00
134       (20k_high-20k_low)/20k_avg               6                4.29
1125    p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
1222           p2mv_(vol/p1mv_vol-1)               3                2.14
567                              amt               3                2.14
Among 2036 features, 2021 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 44.65s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
80             (120k_high-120k_low)/120k_avg              20
1062        p180mv_(60k_avg/p60mv_60k_avg-1)              12
1321         p40mv_(20k_avg/p20mv_20k_avg-1)              10
125                (20k_avg/p80mv_20k_avg-1)              10
567                                      amt              10
1865                                sz50_vol              10
2025                                  sz_vol              10
88            (120k_low-120k_close)/120k_avg              10
1526         p80mv_(20k_low/p20mv_20k_low-1)               9
1235        p300mv_(60k_low/p60mv_60k_low-1)               9
1511     p750mv_(250k_low/p250mv_250k_low-1)               8
1705                                  sh_vol               6
123                (20k_avg/p40mv_20k_avg-1)               4
988      p120mv_(60k_high-60k_close)/60k_avg               4
184            (250k_high-250k_low)/250k_avg               3
1466         p60mv_(20k_low/p20mv_20k_low-1)               2
343              (60k_high-60k_open)/60k_avg               1
975   p120mv_(120k_high-120k_close)/120k_avg               1
901                    lower_shadow/120k_avg               1

      importance_percent
80                 14.29
1062                8.57
1321                7.14
125                 7.14
567                 7.14
1865                7.14
2025                7.14
88                  7.14
1526                6.43
1235                6.43
1511                5.71
1705                4.29
123                 2.86
988                 2.86
184                 2.14
1466                1.43
343                 0.71
975                 0.71
901                 0.71
Among 2036 features, 2017 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 40.12s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
2025                              sz_vol              16               11.43
895                            hs300_vol              14               10.00
1865                            sz50_vol              11                7.86
342           (60k_high-60k_low)/60k_avg               8                5.71
1705                              sh_vol               5                3.57
2035                                vol0               5                3.57
22         (10k_close/p30mv_10k_close-1)               4                2.86
567                                  amt               4                2.86
1125        p20mv_(5k_avg/p5mv_5k_avg-1)               4                2.86
453                     (high-close)/avg               3                2.14
430              (close/p120max_close-1)               3                2.14
1489     p6mv_(3k_close/p3mv_3k_close-1)               3                2.14
454                       (high-low)/avg               3                2.14
1309  p40mv_(10k_close-10k_open)/10k_avg               3                2.14
468               (high/p250mean_high-1)               2                1.43
1495        p6mv_(3k_low-3k_open)/3k_avg               2                1.43
1107    p20mv_(10k_low-10k_open)/10k_avg               2                1.43
132        (20k_close/p80mv_20k_close-1)               2                1.43
303               (5k_low/p5mv_5k_low-1)               2                1.43
1490      p6mv_(3k_high-3k_close)/3k_avg               2                1.43
Among 2036 features, 1980 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 36.85s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 41.05s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                feature  importance_raw  importance_percent
567                                 amt              10                7.14
1865                           sz50_vol               9                6.43
1705                             sh_vol               7                5.00
1112    p20mv_(20k_avg/p20mv_20k_avg-1)               5                3.57
2025                             sz_vol               5                3.57
496                 (low/p250max_low-1)               4                2.86
124           (20k_avg/p60mv_20k_avg-1)               4                2.86
2027              upper_shadow/120k_avg               4                2.86
1268       p360mv_lower_shadow/120k_avg               4                2.86
133        (20k_high-20k_close)/20k_avg               4                2.86
430             (close/p120max_close-1)               3                2.14
1283      p3mv_(3k_low-3k_close)/3k_avg               3                2.14
1480  p60mv_(60k_open/p60mv_60k_open-1)               3                2.14
342          (60k_high-60k_low)/60k_avg               3                2.14
351          (60k_low-60k_open)/60k_avg               2                1.43
1423            p5mv_(60MA/p1mv_60MA-1)               2                1.43
908                              market               2                1.43
191      (250k_low-250k_close)/250k_avg               2                1.43
454                      (high-low)/avg               2                1.43
1466    p60mv_(20k_low/p20mv_20k_low-1)               2                1.43
Among 2036 features, 1962 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 46.25s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
1865                            sz50_vol              15               10.71
2025                              sz_vol              11                7.86
454                       (high-low)/avg               9                6.43
342           (60k_high-60k_low)/60k_avg               8                5.71
567                                  amt               6                4.29
895                            hs300_vol               5                3.57
1418       p5mv_(5k_low-5k_close)/5k_avg               4                2.86
234              (3k_high-3k_low)/3k_avg               4                2.86
1705                              sh_vol               4                2.86
2035                                vol0               4                2.86
908                               market               3                2.14
134           (20k_high-20k_low)/20k_avg               3                2.14
1539       p9mv_(3k_low-3k_close)/3k_avg               3                2.14
1415        p5mv_(5k_high-5k_low)/5k_avg               3                2.14
468               (high/p250mean_high-1)               3                2.14
1490      p6mv_(3k_high-3k_close)/3k_avg               2                1.43
1125        p20mv_(5k_avg/p5mv_5k_avg-1)               2                1.43
1107    p20mv_(10k_low-10k_open)/10k_avg               2                1.43
26            (10k_high-10k_low)/10k_avg               2                1.43
1311  p40mv_(10k_high-10k_close)/10k_avg               2                1.43
Among 2036 features, 1974 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 45.66s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                  feature  importance_raw  importance_percent
2025                               sz_vol              28               20.00
895                             hs300_vol              12                8.57
1138           p20mv_lower_shadow/20k_avg              10                7.14
22          (10k_close/p30mv_10k_close-1)              10                7.14
1251    p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
991    p120mv_(60k_high/p60mv_60k_high-1)              10                7.14
988   p120mv_(60k_high-60k_close)/60k_avg              10                7.14
248                (3k_low/p6mv_3k_low-1)               8                5.71
1107     p20mv_(10k_low-10k_open)/10k_avg               8                5.71
548                   (vol/p20mean_vol-1)               6                4.29
1490       p6mv_(3k_high-3k_close)/3k_avg               6                4.29
1865                             sz50_vol               6                4.29
243               (3k_low-3k_open)/3k_avg               6                4.29
1495         p6mv_(3k_low-3k_open)/3k_avg               2                1.43
303                (5k_low/p5mv_5k_low-1)               2                1.43
1219                  p2mv_(low-open)/avg               2                1.43
2035                                 vol0               2                1.43
547                    (vol/p20max_vol-1)               2                1.43
Among 2036 features, 2018 features are not used in the model

Train custom_revenue_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 42.47s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
2035                                   vol0              10
1536           p9mv_(3k_high-3k_low)/3k_avg              10
1457        p60mv_(20k_avg/p20mv_20k_avg-1)              10
132           (20k_close/p80mv_20k_close-1)              10
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1000            p120mv_upper_shadow/60k_avg              10
1138             p20mv_lower_shadow/20k_avg              10
1061       p180mv_(60k_amt/p60mv_60k_amt-1)              10
927        p100mv_(20k_avg/p20mv_20k_avg-1)              10
1705                                 sh_vol              10
1865                               sz50_vol              10
2025                                 sz_vol              10
1449  p600mv_(120k_low-120k_close)/120k_avg              10
1247      p30mv_(10k_high/p10mv_10k_high-1)               8
509                      (low/p60max_low-1)               2

      importance_percent
2035                7.14
1536                7.14
1457                7.14
132                 7.14
1321                7.14
1000                7.14
1138                7.14
1061                7.14
927                 7.14
1705                7.14
1865                7.14
2025                7.14
1449                7.14
1247                5.71
509                 1.43
Among 2036 features, 2021 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 44.25s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2025                                  sz_vol              14
1865                                sz50_vol               9
1705                                  sh_vol               8
567                                      amt               4
1268            p360mv_lower_shadow/120k_avg               4
2035                                    vol0               4
988      p120mv_(60k_high-60k_close)/60k_avg               3
986      p120mv_(60k_close-60k_open)/60k_avg               3
1449   p600mv_(120k_low-120k_close)/120k_avg               3
1138              p20mv_lower_shadow/20k_avg               3
1241         p30mv_(10k_avg/p10mv_10k_avg-1)               3
124                (20k_avg/p60mv_20k_avg-1)               3
430                  (close/p120max_close-1)               2
133             (20k_high-20k_close)/20k_avg               2
921     p1000mv_(250k_low/p250mv_250k_low-1)               2
1525        p80mv_(20k_low-20k_open)/20k_avg               2
1321         p40mv_(20k_avg/p20mv_20k_avg-1)               2
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
125                (20k_avg/p80mv_20k_avg-1)               2
1350            p480mv_lower_shadow/120k_avg               2

      importance_percent
2025               10.00
1865                6.43
1705                5.71
567                 2.86
1268                2.86
2035                2.86
988                 2.14
986                 2.14
1449                2.14
1138                2.14
1241                2.14
124                 2.14
430                 1.43
133                 1.43
921                 1.43
1525                1.43
1321                1.43
1009                1.43
125                 1.43
1350                1.43
Among 2036 features, 1960 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 49.88s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              20               14.29
2035                                vol0              10                7.14
2025                              sz_vol              10                7.14
432              (close/p120min_close-1)              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
933   p100mv_(20k_high/p20mv_20k_high-1)              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
491                  (low/p120min_low-1)               8                5.71
345         (60k_high/p120mv_60k_high-1)               8                5.71
486                   (low/p10max_low-1)               7                5.00
1865                            sz50_vol               7                5.00
1705                              sh_vol               6                4.29
946     p10mv_(10k_high-10k_low)/10k_avg               5                3.57
1138          p20mv_lower_shadow/20k_avg               4                2.86
399                   (avg/p10max_avg-1)               2                1.43
303               (5k_low/p5mv_5k_low-1)               1                0.71
1250     p30mv_(10k_low/p10mv_10k_low-1)               1                0.71
347         (60k_high/p240mv_60k_high-1)               1                0.71
Among 2036 features, 2017 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 41.56s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 47.05s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                      feature  importance_raw  \
1865                                 sz50_vol              13
2035                                     vol0              10
342                (60k_high-60k_low)/60k_avg              10
2027                    upper_shadow/120k_avg               9
133              (20k_high-20k_close)/20k_avg               9
82                    (120k_high/250k_high-1)               8
1112          p20mv_(20k_avg/p20mv_20k_avg-1)               7
1480        p60mv_(60k_open/p60mv_60k_open-1)               6
124                 (20k_avg/p60mv_20k_avg-1)               6
2025                                   sz_vol               5
1061         p180mv_(60k_amt/p60mv_60k_amt-1)               5
496                       (low/p250max_low-1)               5
1519      p80mv_(20k_close/p20mv_20k_close-1)               5
1350             p480mv_lower_shadow/120k_avg               5
1525         p80mv_(20k_low-20k_open)/20k_avg               5
1268             p360mv_lower_shadow/120k_avg               5
1705                                   sh_vol               3
430                   (close/p120max_close-1)               3
438                  (close/p250mean_close-1)               3
1005  p1250mv_(250k_high-250k_close)/250k_avg               2

      importance_percent
1865                9.29
2035                7.14
342                 7.14
2027                6.43
133                 6.43
82                  5.71
1112                5.00
1480                4.29
124                 4.29
2025                3.57
1061                3.57
496                 3.57
1519                3.57
1350                3.57
1525                3.57
1268                3.57
1705                2.14
430                 2.14
438                 2.14
1005                1.43
Among 2036 features, 2006 features are not used in the model
2013-01-01 2016-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 102669
<class 'pandas.core.frame.DataFrame'>
Int64Index: 102669 entries, 20130807 to 20160325
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 425.6 MB
None
(0, 8)
Empty DataFrame
Columns: [y_l_rise, y_l_decline, y_l_avg, y_s_rise, y_s_decline, y_s_avg, y_l, y_l_r]
Index: []
Train dates:20130104-20160527
(99931, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 54.22s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              49
2025                                  sz_vol              38
895                                hs300_vol              38
1705                                  sh_vol              35
2060     layer0_custom_revenue2_y_s_avg_pred              29
2134  layer0_custom_revenue_y_s_decline_pred              22
2122     layer0_custom_revenue2_y_l_avg_pred              18
732                                  cyb_vol              13
2086    layer0_custom_revenue2_y_s_rise_pred              10
453                         (high-close)/avg               8
2048  layer0_custom_revenue_y_l_decline_pred               7
2098      layer0_custom_revenue_y_s_avg_pred               6
1198            p25mv_(5k_low/p5mv_5k_low-1)               6
47               (10k_open/p50mv_10k_open-1)               6
519                       (open/p1mv_open-1)               5
1215                     p2mv_(high-low)/avg               5
2075  layer0_custom_revenue2_y_l_rise_output               4
1526         p80mv_(20k_low/p20mv_20k_low-1)               4
1400       p50mv_(10k_open/p10mv_10k_open-1)               4
1101     p20mv_(10k_close/p10mv_10k_close-1)               4

      importance_percent
1865                6.53
2025                5.07
895                 5.07
1705                4.67
2060                3.87
2134                2.93
2122                2.40
732                 1.73
2086                1.33
453                 1.07
2048                0.93
2098                0.80
1198                0.80
47                  0.80
519                 0.67
1215                0.67
2075                0.53
1526                0.53
1400                0.53
1101                0.53
Among 2160 features, 1830 features are not used in the model

Train l2_y_s_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 56.12s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1705                                  sh_vol              47
895                                hs300_vol              47
2025                                  sz_vol              47
1865                                sz50_vol              43
2060     layer0_custom_revenue2_y_s_avg_pred              32
2122     layer0_custom_revenue2_y_l_avg_pred              18
2134  layer0_custom_revenue_y_s_decline_pred              16
2048  layer0_custom_revenue_y_l_decline_pred              15
732                                  cyb_vol              10
937       p100mv_(20k_open/p20mv_20k_open-1)               7
936         p100mv_(20k_low/p20mv_20k_low-1)               5
938         p100mv_(20k_vol/p20mv_20k_vol-1)               5
1026            p12mv_(3k_low/p3mv_3k_low-1)               5
433                     (close/p1mv_close-1)               5
955             p10mv_(5k_avg/p5mv_5k_avg-1)               4
1088                     p1mv_(high-low)/avg               4
1487             p6mv_(3k_avg/p3mv_3k_avg-1)               4
34               (10k_low-10k_close)/10k_avg               4
2098      layer0_custom_revenue_y_s_avg_pred               4
1250         p30mv_(10k_low/p10mv_10k_low-1)               4

      importance_percent
1705                6.27
895                 6.27
2025                6.27
1865                5.73
2060                4.27
2122                2.40
2134                2.13
2048                2.00
732                 1.33
937                 0.93
936                 0.67
938                 0.67
1026                0.67
433                 0.67
955                 0.53
1088                0.53
1487                0.53
34                  0.53
2098                0.53
1250                0.53
Among 2160 features, 1846 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 59.87s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2122     layer0_custom_revenue2_y_l_avg_pred              60
1865                                sz50_vol              31
1705                                  sh_vol              24
732                                  cyb_vol              20
2025                                  sz_vol              18
2134  layer0_custom_revenue_y_s_decline_pred              17
2048  layer0_custom_revenue_y_l_decline_pred              17
1526         p80mv_(20k_low/p20mv_20k_low-1)              17
895                                hs300_vol              15
1238             p300mv_lower_shadow/60k_avg               9
567                                      amt               9
1108         p20mv_(10k_low/p10mv_10k_low-1)               8
2060     layer0_custom_revenue2_y_s_avg_pred               7
2110      layer0_custom_revenue_y_l_avg_pred               6
1116        p20mv_(20k_high-20k_low)/20k_avg               6
1341  p480mv_(120k_high-120k_close)/120k_avg               6
1350            p480mv_lower_shadow/120k_avg               5
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               5
81            (120k_high-120k_open)/120k_avg               5
1466         p60mv_(20k_low/p20mv_20k_low-1)               5

      importance_percent
2122                8.00
1865                4.13
1705                3.20
732                 2.67
2025                2.40
2134                2.27
2048                2.27
1526                2.27
895                 2.00
1238                1.20
567                 1.20
1108                1.07
2060                0.93
2110                0.80
1116                0.80
1341                0.80
1350                0.67
1457                0.67
81                  0.67
1466                0.67
Among 2160 features, 1834 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 57.97s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2122         layer0_custom_revenue2_y_l_avg_pred              42
2025                                      sz_vol              20
2060         layer0_custom_revenue2_y_s_avg_pred              18
1865                                    sz50_vol              17
2134      layer0_custom_revenue_y_s_decline_pred              17
2075      layer0_custom_revenue2_y_l_rise_output              16
895                                    hs300_vol              15
1705                                      sh_vol              15
2048      layer0_custom_revenue_y_l_decline_pred              15
567                                          amt              14
568                                         area              11
732                                      cyb_vol              10
2074        layer0_custom_revenue2_y_l_rise_pred              10
2148         layer0_custom_revenue_y_l_rise_pred               9
1526             p80mv_(20k_low/p20mv_20k_low-1)               8
2084  layer0_custom_revenue2_y_l_rise_tree8_leaf               8
1457             p60mv_(20k_avg/p20mv_20k_avg-1)               6
1065         p180mv_(60k_high-60k_close)/60k_avg               6
1238                 p300mv_lower_shadow/60k_avg               6
1466             p60mv_(20k_low/p20mv_20k_low-1)               6

      importance_percent
2122                5.60
2025                2.67
2060                2.40
1865                2.27
2134                2.27
2075                2.13
895                 2.00
1705                2.00
2048                2.00
567                 1.87
568                 1.47
732                 1.33
2074                1.33
2148                1.20
1526                1.07
2084                1.07
1457                0.80
1065                0.80
1238                0.80
1466                0.80
Among 2160 features, 1800 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 56.41s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2086    layer0_custom_revenue2_y_s_rise_pred              30
2060     layer0_custom_revenue2_y_s_avg_pred              25
1705                                  sh_vol              21
2122     layer0_custom_revenue2_y_l_avg_pred              20
1865                                sz50_vol              17
2025                                  sz_vol              14
2075  layer0_custom_revenue2_y_l_rise_output              13
895                                hs300_vol              13
2134  layer0_custom_revenue_y_s_decline_pred              10
454                           (high-low)/avg               9
2074    layer0_custom_revenue2_y_l_rise_pred               8
732                                  cyb_vol               8
453                         (high-close)/avg               7
568                                     area               7
1254              p30mv_upper_shadow/10k_avg               6
1088                     p1mv_(high-low)/avg               5
2033                        upper_shadow/avg               5
2036     layer0_custom_revenue_y_s_rise_pred               5
965           p10mv_(5k_open/p5mv_5k_open-1)               4
72           (120k_close-120k_open)/120k_avg               4

      importance_percent
2086                4.00
2060                3.33
1705                2.80
2122                2.67
1865                2.27
2025                1.87
2075                1.73
895                 1.73
2134                1.33
454                 1.20
2074                1.07
732                 1.07
453                 0.93
568                 0.93
1254                0.80
1088                0.67
2033                0.67
2036                0.67
965                 0.53
72                  0.53
Among 2160 features, 1756 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 55.62s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2122        layer0_custom_revenue2_y_l_avg_pred              43
568                                        area              20
2086       layer0_custom_revenue2_y_s_rise_pred              19
2060        layer0_custom_revenue2_y_s_avg_pred              15
567                                         amt              14
2075     layer0_custom_revenue2_y_l_rise_output              13
2074       layer0_custom_revenue2_y_l_rise_pred              11
2148        layer0_custom_revenue_y_l_rise_pred              10
1865                                   sz50_vol              10
2035                                       vol0               9
1235           p300mv_(60k_low/p60mv_60k_low-1)               8
2025                                     sz_vol               7
2134     layer0_custom_revenue_y_s_decline_pred               6
1705                                     sh_vol               6
2130  layer0_custom_revenue2_y_l_avg_tree6_leaf               5
453                            (high-close)/avg               5
984            p120mv_(60k_amt/p60mv_60k_amt-1)               4
2034                                        vol               4
732                                     cyb_vol               4
1307            p40mv_(10k_amt/p10mv_10k_amt-1)               4

      importance_percent
2122                5.73
568                 2.67
2086                2.53
2060                2.00
567                 1.87
2075                1.73
2074                1.47
2148                1.33
1865                1.33
2035                1.20
1235                1.07
2025                0.93
2134                0.80
1705                0.80
2130                0.67
453                 0.67
984                 0.53
2034                0.53
732                 0.53
1307                0.53
Among 2160 features, 1766 features are not used in the model
2013-01-01 2016-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 102608
<class 'pandas.core.frame.DataFrame'>
Int64Index: 102608 entries, 20130819 to 20160217
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 425.4 MB
None
(0, 8)
Empty DataFrame
Columns: [y_l_rise, y_l_decline, y_l_avg, y_s_rise, y_s_decline, y_s_avg, y_l, y_l_r]
Index: []
Train dates:20130104-20160527
(99933, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 66.05s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=30,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2238                  layer1_l2_y_l_avg_pred             111
2290                 layer1_l2_y_l_rise_pred              82
2212              layer1_l2_y_l_decline_pred              64
2122     layer0_custom_revenue2_y_l_avg_pred              37
2160                  layer1_l2_y_s_avg_pred              29
2186              layer1_l2_y_s_decline_pred              23
568                                     area              18
2025                                  sz_vol              15
1865                                sz50_vol              10
2134  layer0_custom_revenue_y_s_decline_pred               9
895                                hs300_vol               8
1705                                  sh_vol               7
2074    layer0_custom_revenue2_y_l_rise_pred               7
2148     layer0_custom_revenue_y_l_rise_pred               7
2075  layer0_custom_revenue2_y_l_rise_output               6
2217        layer1_l2_y_l_decline_tree4_leaf               5
2222        layer1_l2_y_l_decline_tree9_leaf               5
2303          layer1_l2_y_l_rise_tree12_leaf               5
1347     p480mv_(120k_low/p120mv_120k_low-1)               5
2264                 layer1_l2_y_s_rise_pred               5

      importance_percent
2238               12.33
2290                9.11
2212                7.11
2122                4.11
2160                3.22
2186                2.56
568                 2.00
2025                1.67
1865                1.11
2134                1.00
895                 0.89
1705                0.78
2074                0.78
2148                0.78
2075                0.67
2217                0.56
2222                0.56
2303                0.56
1347                0.56
2264                0.56
Among 2316 features, 1965 features are not used in the model
test range: 20160701 20170101
2016-07-01 2016-12-31
Time slice keys in hdf5: 2016/0101-0101

Current key: 2016/0101-0101
Current slice size(length): 313767

Total concatenating size: 313767
Result dataset size: 160893

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      6     -0.118       -0.148     0.108     0.090    -0.215
(-0.25,-0.20]    132     -0.064       -0.091     0.098     0.255    -0.235
(-0.20,-0.15]    822     -0.031       -0.072     0.111     0.449    -0.299
(-0.15,-0.10]   2894     -0.024       -0.061     0.106     0.889    -0.253
(-0.10,-0.05]   8642     -0.005       -0.040     0.101     0.878    -0.266
(-0.05,0.00]   38666      0.013        0.011     0.087     0.902    -0.335
(0.00,0.05]    75034      0.016        0.016     0.079     1.064    -0.323
(0.05,0.10]    25447      0.012        0.023     0.090     0.899    -0.291
(0.10,0.15]     6712     -0.013       -0.044     0.094     0.803    -0.259
(0.15,0.20]     1727     -0.029       -0.059     0.100     0.912    -0.297
(0.20,0.25]      457     -0.021       -0.059     0.108     0.364    -0.231
(0.25,0.30]      143     -0.032       -0.070     0.118     0.288    -0.232
(0.30,0.35]       49     -0.033       -0.068     0.105     0.293    -0.214
(0.35,0.40]       16     -0.035       -0.057     0.103     0.198    -0.153
(0.40,0.45]        1     -0.109       -0.109       NaN    -0.109    -0.109
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      6     -0.158       -0.197     0.144     0.123    -0.284
(-0.25,-0.20]    132     -0.091       -0.131     0.141     0.362    -0.312
(-0.20,-0.15]    822     -0.046       -0.109     0.157     0.609    -0.385
(-0.15,-0.10]   2894     -0.038       -0.097     0.153     1.642    -0.337
(-0.10,-0.05]   8642     -0.010       -0.068     0.147     1.619    -0.373
(-0.05,0.00]   38666      0.017        0.000     0.125     1.274    -0.391
(0.00,0.05]    75034      0.020        0.000     0.113     1.336    -0.393
(0.05,0.10]    25447      0.012        0.034     0.128     1.215    -0.372
(0.10,0.15]     6712     -0.020       -0.068     0.131     1.047    -0.360
(0.15,0.20]     1727     -0.042       -0.089     0.140     1.213    -0.364
(0.20,0.25]      457     -0.030       -0.089     0.154     0.544    -0.299
(0.25,0.30]      143     -0.045       -0.109     0.165     0.367    -0.326
(0.30,0.35]       49     -0.041       -0.100     0.154     0.371    -0.293
(0.35,0.40]       16     -0.054       -0.082     0.143     0.286    -0.190
(0.40,0.45]        1     -0.147       -0.147       NaN    -0.147    -0.147
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      6     -0.078       -0.099     0.073     0.057    -0.147
(-0.25,-0.20]    132     -0.037       -0.040     0.060     0.147    -0.159
(-0.20,-0.15]    822     -0.017       -0.031     0.067     0.294    -0.213
(-0.15,-0.10]   2894     -0.013       -0.022     0.063     0.617    -0.177
(-0.10,-0.05]   8642     -0.003       -0.010     0.058     0.584    -0.182
(-0.05,0.00]   38666      0.009        0.000     0.052     0.650    -0.283
(0.00,0.05]    75034      0.010        0.001     0.048     0.792    -0.254
(0.05,0.10]    25447      0.008        0.004     0.055     0.613    -0.211
(0.10,0.15]     6712     -0.008       -0.016     0.059     0.588    -0.179
(0.15,0.20]     1727     -0.019       -0.027     0.062     0.612    -0.230
(0.20,0.25]      457     -0.012       -0.024     0.066     0.222    -0.182
(0.25,0.30]      143     -0.020       -0.032     0.073     0.210    -0.169
(0.30,0.35]       49     -0.017       -0.032     0.062     0.216    -0.135
(0.35,0.40]       16     -0.026       -0.033     0.061     0.109    -0.116
(0.40,0.45]        1     -0.071       -0.071       NaN    -0.071    -0.071
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      6      0.042        0.027     0.044     0.123     0.000
(-0.25,-0.20]    132      0.063        0.049     0.063     0.362     0.000
(-0.20,-0.15]    822      0.074        0.046     0.085     0.609     0.000
(-0.15,-0.10]   2894      0.074        0.048     0.090     1.642     0.000
(-0.10,-0.05]   8642      0.076        0.051     0.089     1.619     0.000
(-0.05,0.00]   38666      0.072        0.051     0.078     1.274     0.000
(0.00,0.05]    75034      0.065        0.045     0.073     1.336     0.000
(0.05,0.10]    25447      0.074        0.053     0.077     1.215     0.000
(0.10,0.15]     6712      0.064        0.039     0.076     1.047     0.000
(0.15,0.20]     1727      0.063        0.037     0.079     1.213     0.000
(0.20,0.25]      457      0.078        0.048     0.082     0.544     0.000
(0.25,0.30]      143      0.080        0.046     0.088     0.367     0.000
(0.30,0.35]       49      0.082        0.061     0.080     0.371     0.000
(0.35,0.40]       16      0.062        0.018     0.088     0.286     0.005
(0.40,0.45]        1      0.009        0.009       NaN     0.009     0.009
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

test period:20170101-20170701
dataset range: 20130101 20170101
2013-01-01 2016-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 118807
<class 'pandas.core.frame.DataFrame'>
Int64Index: 118807 entries, 20130607 to 20160715
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 492.5 MB
None
(15, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
Train dates:20130104-20161201
(115939, 2036)

----------Train layer 0----------

Train custom_revenue_y_s_rise
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 52.49s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
1865                        sz50_vol              26               18.57
342       (60k_high-60k_low)/60k_avg              20               14.29
468           (high/p250mean_high-1)              10                7.14
1125    p20mv_(5k_avg/p5mv_5k_avg-1)              10                7.14
454                   (high-low)/avg              10                7.14
2035                            vol0               8                5.71
134       (20k_high-20k_low)/20k_avg               8                5.71
895                        hs300_vol               7                5.00
234          (3k_high-3k_low)/3k_avg               6                4.29
567                              amt               6                4.29
110               (20MA/p2mv_20MA-1)               4                2.86
243          (3k_low-3k_open)/3k_avg               4                2.86
1490  p6mv_(3k_high-3k_close)/3k_avg               4                2.86
1415    p5mv_(5k_high-5k_low)/5k_avg               4                2.86
2025                          sz_vol               4                2.86
1222           p2mv_(vol/p1mv_vol-1)               3                2.14
109               (20MA/p1mv_20MA-1)               2                1.43
453                 (high-close)/avg               2                1.43
1495    p6mv_(3k_low-3k_open)/3k_avg               2                1.43
Among 2036 features, 2017 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 56.65s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
88           (120k_low-120k_close)/120k_avg              20
80            (120k_high-120k_low)/120k_avg              20
2025                                 sz_vol              10
927        p100mv_(20k_avg/p20mv_20k_avg-1)              10
1226       p300mv_(60k_avg/p60mv_60k_avg-1)              10
132           (20k_close/p80mv_20k_close-1)              10
567                                     amt              10
26               (10k_high-10k_low)/10k_avg              10
1466        p60mv_(20k_low/p20mv_20k_low-1)              10
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1263  p360mv_(120k_low-120k_close)/120k_avg               7
972     p120mv_(120k_avg/p120mv_120k_avg-1)               6
1000            p120mv_upper_shadow/60k_avg               6
934      p100mv_(20k_low-20k_close)/20k_avg               1

      importance_percent
88                 14.29
80                 14.29
2025                7.14
927                 7.14
1226                7.14
132                 7.14
567                 7.14
26                  7.14
1466                7.14
1321                7.14
1263                5.00
972                 4.29
1000                4.29
934                 0.71
Among 2036 features, 2022 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 59.92s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
2025                                 sz_vol              18
895                               hs300_vol              13
1865                               sz50_vol              12
342              (60k_high-60k_low)/60k_avg               8
454                          (high-low)/avg               6
567                                     amt               5
1705                                 sh_vol               4
1309     p40mv_(10k_close-10k_open)/10k_avg               4
430                 (close/p120max_close-1)               3
22            (10k_close/p30mv_10k_close-1)               3
2035                                   vol0               3
1449  p600mv_(120k_low-120k_close)/120k_avg               3
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               3
21            (10k_close/p20mv_10k_close-1)               2
1138             p20mv_lower_shadow/20k_avg               2
132           (20k_close/p80mv_20k_close-1)               2
453                        (high-close)/avg               2
1102     p20mv_(10k_high-10k_close)/10k_avg               2
1491           p6mv_(3k_high-3k_low)/3k_avg               2
1490         p6mv_(3k_high-3k_close)/3k_avg               2

      importance_percent
2025               12.86
895                 9.29
1865                8.57
342                 5.71
454                 4.29
567                 3.57
1705                2.86
1309                2.86
430                 2.14
22                  2.14
2035                2.14
1449                2.14
1125                2.14
21                  1.43
1138                1.43
132                 1.43
453                 1.43
1102                1.43
1491                1.43
1490                1.43
Among 2036 features, 1977 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 50.88s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 52.80s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
567                                      amt              12
2025                                  sz_vol               6
1865                                sz50_vol               6
1705                                  sh_vol               5
342               (60k_high-60k_low)/60k_avg               5
83            (120k_high/p120mv_120k_high-1)               5
133             (20k_high-20k_close)/20k_avg               4
2027                   upper_shadow/120k_avg               4
1283           p3mv_(3k_low-3k_close)/3k_avg               4
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               4
430                  (close/p120max_close-1)               4
1009  p1250mv_(250k_low-250k_close)/250k_avg               3
124                (20k_avg/p60mv_20k_avg-1)               3
895                                hs300_vol               3
1263   p360mv_(120k_low-120k_close)/120k_avg               3
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               3
454                           (high-low)/avg               3
351               (60k_low-60k_open)/60k_avg               2
921     p1000mv_(250k_low/p250mv_250k_low-1)               2
1000             p120mv_upper_shadow/60k_avg               2

      importance_percent
567                 8.57
2025                4.29
1865                4.29
1705                3.57
342                 3.57
83                  3.57
133                 2.86
2027                2.86
1283                2.86
1112                2.86
430                 2.86
1009                2.14
124                 2.14
895                 2.14
1263                2.14
1061                2.14
454                 2.14
351                 1.43
921                 1.43
1000                1.43
Among 2036 features, 1965 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 51.91s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              12
342               (60k_high-60k_low)/60k_avg               9
454                           (high-low)/avg               9
2025                                  sz_vol               9
567                                      amt               8
234                  (3k_high-3k_low)/3k_avg               6
895                                hs300_vol               6
134               (20k_high-20k_low)/20k_avg               5
26                (10k_high-10k_low)/10k_avg               5
2035                                    vol0               5
1418           p5mv_(5k_low-5k_close)/5k_avg               3
468                   (high/p250mean_high-1)               3
1705                                  sh_vol               3
1125            p20mv_(5k_avg/p5mv_5k_avg-1)               3
1449   p600mv_(120k_low-120k_close)/120k_avg               2
1415            p5mv_(5k_high-5k_low)/5k_avg               2
975   p120mv_(120k_high-120k_close)/120k_avg               2
908                                   market               2
1311      p40mv_(10k_high-10k_close)/10k_avg               2
164                     (250MA/p3mv_250MA-1)               1

      importance_percent
1865                8.57
342                 6.43
454                 6.43
2025                6.43
567                 5.71
234                 4.29
895                 4.29
134                 3.57
26                  3.57
2035                3.57
1418                2.14
468                 2.14
1705                2.14
1125                2.14
1449                1.43
1415                1.43
975                 1.43
908                 1.43
1311                1.43
164                 0.71
Among 2036 features, 1973 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 53.26s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                  feature  importance_raw  importance_percent
2025                               sz_vol              30               21.43
1107     p20mv_(10k_low-10k_open)/10k_avg              10                7.14
303                (5k_low/p5mv_5k_low-1)              10                7.14
988   p120mv_(60k_high-60k_close)/60k_avg              10                7.14
1251    p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
895                             hs300_vol              10                7.14
1138           p20mv_lower_shadow/20k_avg              10                7.14
22          (10k_close/p30mv_10k_close-1)              10                7.14
1865                             sz50_vol               9                6.43
1495         p6mv_(3k_low-3k_open)/3k_avg               6                4.29
547                    (vol/p20max_vol-1)               5                3.57
503                      (low/p3mv_low-1)               4                2.86
1219                  p2mv_(low-open)/avg               4                2.86
247                (3k_low/p3mv_3k_low-1)               3                2.14
2035                                 vol0               3                2.14
991    p120mv_(60k_high/p60mv_60k_high-1)               2                1.43
548                   (vol/p20mean_vol-1)               1                0.71
243               (3k_low-3k_open)/3k_avg               1                0.71
1                      (10MA/p1mv_10MA-1)               1                0.71
1490       p6mv_(3k_high-3k_close)/3k_avg               1                0.71
Among 2036 features, 2016 features are not used in the model

Train custom_revenue_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 53.41s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
1705                                 sh_vol              10
1536           p9mv_(3k_high-3k_low)/3k_avg              10
1138             p20mv_lower_shadow/20k_avg              10
1449  p600mv_(120k_low-120k_close)/120k_avg              10
1865                               sz50_vol              10
1061       p180mv_(60k_amt/p60mv_60k_amt-1)              10
24            (10k_close/p50mv_10k_close-1)              10
2025                                 sz_vol              10
454                          (high-low)/avg              10
988     p120mv_(60k_high-60k_close)/60k_avg              10
132           (20k_close/p80mv_20k_close-1)              10
1457        p60mv_(20k_avg/p20mv_20k_avg-1)              10
927        p100mv_(20k_avg/p20mv_20k_avg-1)              10
1347    p480mv_(120k_low/p120mv_120k_low-1)               9
1152   p240mv_(120k_low-120k_open)/120k_avg               1

      importance_percent
1705                7.14
1536                7.14
1138                7.14
1449                7.14
1865                7.14
1061                7.14
24                  7.14
2025                7.14
454                 7.14
988                 7.14
132                 7.14
1457                7.14
927                 7.14
1347                6.43
1152                0.71
Among 2036 features, 2021 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 53.32s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
2025                                 sz_vol              13
1865                               sz50_vol              12
567                                     amt               7
988     p120mv_(60k_high-60k_close)/60k_avg               6
1705                                 sh_vol               5
430                 (close/p120max_close-1)               4
1449  p600mv_(120k_low-120k_close)/120k_avg               4
1321        p40mv_(20k_avg/p20mv_20k_avg-1)               4
1457        p60mv_(20k_avg/p20mv_20k_avg-1)               4
454                          (high-low)/avg               3
986     p120mv_(60k_close-60k_open)/60k_avg               3
1061       p180mv_(60k_amt/p60mv_60k_amt-1)               3
1014          p1250mv_lower_shadow/250k_avg               3
1268           p360mv_lower_shadow/120k_avg               3
1241        p30mv_(10k_avg/p10mv_10k_avg-1)               2
125               (20k_avg/p80mv_20k_avg-1)               2
124               (20k_avg/p60mv_20k_avg-1)               2
732                                 cyb_vol               2
1138             p20mv_lower_shadow/20k_avg               2
341            (60k_high-60k_close)/60k_avg               2

      importance_percent
2025                9.29
1865                8.57
567                 5.00
988                 4.29
1705                3.57
430                 2.86
1449                2.86
1321                2.86
1457                2.86
454                 2.14
986                 2.14
1061                2.14
1014                2.14
1268                2.14
1241                1.43
125                 1.43
124                 1.43
732                 1.43
1138                1.43
341                 1.43
Among 2036 features, 1964 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 58.43s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              30               21.43
486                   (low/p10max_low-1)              10                7.14
538                 (open/p60min_open-1)              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
345         (60k_high/p120mv_60k_high-1)              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
2025                              sz_vol              10                7.14
134           (20k_high-20k_low)/20k_avg              10                7.14
432              (close/p120min_close-1)              10                7.14
1138          p20mv_lower_shadow/20k_avg               6                4.29
946     p10mv_(10k_high-10k_low)/10k_avg               4                2.86
Among 2036 features, 2023 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 54.97s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 59.59s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                   feature  importance_raw  importance_percent
1865                              sz50_vol              16               11.43
133           (20k_high-20k_close)/20k_avg              10                7.14
342             (60k_high-60k_low)/60k_avg              10                7.14
454                         (high-low)/avg               9                6.43
124              (20k_avg/p60mv_20k_avg-1)               8                5.71
567                                    amt               8                5.71
2027                 upper_shadow/120k_avg               7                5.00
1525      p80mv_(20k_low-20k_open)/20k_avg               6                4.29
430                (close/p120max_close-1)               5                3.57
496                    (low/p250max_low-1)               4                2.86
1268          p360mv_lower_shadow/120k_avg               4                2.86
346           (60k_high/p180mv_60k_high-1)               4                2.86
1152  p240mv_(120k_low-120k_open)/120k_avg               4                2.86
1112       p20mv_(20k_avg/p20mv_20k_avg-1)               4                2.86
1061      p180mv_(60k_amt/p60mv_60k_amt-1)               4                2.86
2035                                  vol0               4                2.86
1519   p80mv_(20k_close/p20mv_20k_close-1)               4                2.86
895                              hs300_vol               3                2.14
1705                                sh_vol               3                2.14
83          (120k_high/p120mv_120k_high-1)               3                2.14
Among 2036 features, 2006 features are not used in the model
2013-01-01 2016-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 118807
<class 'pandas.core.frame.DataFrame'>
Int64Index: 118807 entries, 20130807 to 20160325
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 492.5 MB
None
(10, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
Train dates:20130104-20161201
(116070, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 68.89s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
895                                    hs300_vol              41
1865                                    sz50_vol              38
1705                                      sh_vol              36
2025                                      sz_vol              32
2060         layer0_custom_revenue2_y_s_avg_pred              21
2122         layer0_custom_revenue2_y_l_avg_pred              21
2134      layer0_custom_revenue_y_s_decline_pred              13
732                                      cyb_vol              13
2098          layer0_custom_revenue_y_s_avg_pred              10
2086        layer0_custom_revenue2_y_s_rise_pred               8
453                             (high-close)/avg               8
1198                p25mv_(5k_low/p5mv_5k_low-1)               6
2110          layer0_custom_revenue_y_l_avg_pred               5
1317             p40mv_(10k_low/p10mv_10k_low-1)               5
1088                         p1mv_(high-low)/avg               5
47                   (10k_open/p50mv_10k_open-1)               5
1545                    p9mv_upper_shadow/3k_avg               5
2074        layer0_custom_revenue2_y_l_rise_pred               5
2084  layer0_custom_revenue2_y_l_rise_tree8_leaf               4
1042              p15mv_(3k_open/p3mv_3k_open-1)               4

      importance_percent
895                 5.47
1865                5.07
1705                4.80
2025                4.27
2060                2.80
2122                2.80
2134                1.73
732                 1.73
2098                1.33
2086                1.07
453                 1.07
1198                0.80
2110                0.67
1317                0.67
1088                0.67
47                  0.67
1545                0.67
2074                0.67
2084                0.53
1042                0.53
Among 2160 features, 1805 features are not used in the model

Train l2_y_s_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 70.55s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
1705                                     sh_vol              52
1865                                   sz50_vol              40
2025                                     sz_vol              37
895                                   hs300_vol              36
2060        layer0_custom_revenue2_y_s_avg_pred              26
2122        layer0_custom_revenue2_y_l_avg_pred              21
732                                     cyb_vol              20
937          p100mv_(20k_open/p20mv_20k_open-1)              13
2134     layer0_custom_revenue_y_s_decline_pred              12
2098         layer0_custom_revenue_y_s_avg_pred              11
2070  layer0_custom_revenue2_y_s_avg_tree8_leaf              10
46                  (10k_open/p40mv_10k_open-1)               8
2048     layer0_custom_revenue_y_l_decline_pred               7
2086       layer0_custom_revenue2_y_s_rise_pred               7
2110         layer0_custom_revenue_y_l_avg_pred               6
938            p100mv_(20k_vol/p20mv_20k_vol-1)               5
1400          p50mv_(10k_open/p10mv_10k_open-1)               5
47                  (10k_open/p50mv_10k_open-1)               5
968                   p10mv_lower_shadow/5k_avg               5
955                p10mv_(5k_avg/p5mv_5k_avg-1)               5

      importance_percent
1705                6.93
1865                5.33
2025                4.93
895                 4.80
2060                3.47
2122                2.80
732                 2.67
937                 1.73
2134                1.60
2098                1.47
2070                1.33
46                  1.07
2048                0.93
2086                0.93
2110                0.80
938                 0.67
1400                0.67
47                  0.67
968                 0.67
955                 0.67
Among 2160 features, 1860 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 77.82s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2122        layer0_custom_revenue2_y_l_avg_pred              57
1865                                   sz50_vol              34
2025                                     sz_vol              25
1705                                     sh_vol              24
732                                     cyb_vol              19
2048     layer0_custom_revenue_y_l_decline_pred              17
1526            p80mv_(20k_low/p20mv_20k_low-1)              16
895                                   hs300_vol              10
2134     layer0_custom_revenue_y_s_decline_pred              10
567                                         amt               9
1350               p480mv_lower_shadow/120k_avg               9
1238                p300mv_lower_shadow/60k_avg               8
1466            p60mv_(20k_low/p20mv_20k_low-1)               8
2110         layer0_custom_revenue_y_l_avg_pred               7
1341     p480mv_(120k_high-120k_close)/120k_avg               7
2128  layer0_custom_revenue2_y_l_avg_tree4_leaf               7
1065        p180mv_(60k_high-60k_close)/60k_avg               6
1108            p20mv_(10k_low/p10mv_10k_low-1)               6
937          p100mv_(20k_open/p20mv_20k_open-1)               6
916       p1000mv_(250k_high-250k_low)/250k_avg               5

      importance_percent
2122                7.60
1865                4.53
2025                3.33
1705                3.20
732                 2.53
2048                2.27
1526                2.13
895                 1.33
2134                1.33
567                 1.20
1350                1.20
1238                1.07
1466                1.07
2110                0.93
1341                0.93
2128                0.93
1065                0.80
1108                0.80
937                 0.80
916                 0.67
Among 2160 features, 1846 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 71.08s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2122         layer0_custom_revenue2_y_l_avg_pred              42
1705                                      sh_vol              21
1865                                    sz50_vol              20
567                                          amt              19
2025                                      sz_vol              16
732                                      cyb_vol              14
2060         layer0_custom_revenue2_y_s_avg_pred              13
2074        layer0_custom_revenue2_y_l_rise_pred              12
895                                    hs300_vol              11
1526             p80mv_(20k_low/p20mv_20k_low-1)              11
2134      layer0_custom_revenue_y_s_decline_pred              11
453                             (high-close)/avg              10
568                                         area              10
2075      layer0_custom_revenue2_y_l_rise_output               9
2148         layer0_custom_revenue_y_l_rise_pred               9
2084  layer0_custom_revenue2_y_l_rise_tree8_leaf               9
2076  layer0_custom_revenue2_y_l_rise_tree0_leaf               8
2048      layer0_custom_revenue_y_l_decline_pred               7
1234           p300mv_(60k_low-60k_open)/60k_avg               7
2110          layer0_custom_revenue_y_l_avg_pred               6

      importance_percent
2122                5.60
1705                2.80
1865                2.67
567                 2.53
2025                2.13
732                 1.87
2060                1.73
2074                1.60
895                 1.47
1526                1.47
2134                1.47
453                 1.33
568                 1.33
2075                1.20
2148                1.20
2084                1.20
2076                1.07
2048                0.93
1234                0.93
2110                0.80
Among 2160 features, 1818 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 68.40s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2086    layer0_custom_revenue2_y_s_rise_pred              29
2060     layer0_custom_revenue2_y_s_avg_pred              27
1705                                  sh_vol              24
1865                                sz50_vol              21
2025                                  sz_vol              19
2122     layer0_custom_revenue2_y_l_avg_pred              18
895                                hs300_vol              13
2074    layer0_custom_revenue2_y_l_rise_pred              11
732                                  cyb_vol               7
454                           (high-low)/avg               7
1545                p9mv_upper_shadow/3k_avg               6
453                         (high-close)/avg               6
1159    p240mv_(60k_close/p60mv_60k_close-1)               6
2075  layer0_custom_revenue2_y_l_rise_output               6
1198            p25mv_(5k_low/p5mv_5k_low-1)               5
568                                     area               5
2036     layer0_custom_revenue_y_s_rise_pred               5
2034                                     vol               5
2033                        upper_shadow/avg               5
1119       p20mv_(20k_low-20k_close)/20k_avg               4

      importance_percent
2086                3.87
2060                3.60
1705                3.20
1865                2.80
2025                2.53
2122                2.40
895                 1.73
2074                1.47
732                 0.93
454                 0.93
1545                0.80
453                 0.80
1159                0.80
2075                0.80
1198                0.67
568                 0.67
2036                0.67
2034                0.67
2033                0.67
1119                0.53
Among 2160 features, 1751 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 70.09s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2122         layer0_custom_revenue2_y_l_avg_pred              42
568                                         area              23
2074        layer0_custom_revenue2_y_l_rise_pred              19
2086        layer0_custom_revenue2_y_s_rise_pred              13
567                                          amt              13
2075      layer0_custom_revenue2_y_l_rise_output              13
1705                                      sh_vol              11
2060         layer0_custom_revenue2_y_s_avg_pred              11
1865                                    sz50_vol              10
2035                                        vol0               9
2148         layer0_custom_revenue_y_l_rise_pred               9
1235            p300mv_(60k_low/p60mv_60k_low-1)               7
2025                                      sz_vol               7
2084  layer0_custom_revenue2_y_l_rise_tree8_leaf               6
1456             p60mv_(20k_amt/p20mv_20k_amt-1)               5
1517             p80mv_(20k_avg/p20mv_20k_avg-1)               5
566                                   adj_factor               5
1238                 p300mv_lower_shadow/60k_avg               5
2079  layer0_custom_revenue2_y_l_rise_tree3_leaf               4
937           p100mv_(20k_open/p20mv_20k_open-1)               4

      importance_percent
2122                5.60
568                 3.07
2074                2.53
2086                1.73
567                 1.73
2075                1.73
1705                1.47
2060                1.47
1865                1.33
2035                1.20
2148                1.20
1235                0.93
2025                0.93
2084                0.80
1456                0.67
1517                0.67
566                 0.67
1238                0.67
2079                0.53
937                 0.53
Among 2160 features, 1766 features are not used in the model
2013-01-01 2016-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Total concatenating size: 118807
Result dataset size: 118807
<class 'pandas.core.frame.DataFrame'>
Int64Index: 118807 entries, 20130819 to 20161109
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 492.5 MB
None
(9, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
Train dates:20130104-20161201
(116043, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 81.43s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=30,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2238                  layer1_l2_y_l_avg_pred             110
2290                 layer1_l2_y_l_rise_pred              87
2212              layer1_l2_y_l_decline_pred              71
2122     layer0_custom_revenue2_y_l_avg_pred              33
2160                  layer1_l2_y_s_avg_pred              27
2186              layer1_l2_y_s_decline_pred              16
895                                hs300_vol              14
568                                     area              12
2025                                  sz_vol              11
2148     layer0_custom_revenue_y_l_rise_pred              11
1705                                  sh_vol              10
2245            layer1_l2_y_l_avg_tree6_leaf              10
2048  layer0_custom_revenue_y_l_decline_pred               8
1865                                sz50_vol               8
2237       layer1_l2_y_l_decline_tree24_leaf               7
2230       layer1_l2_y_l_decline_tree17_leaf               6
2229       layer1_l2_y_l_decline_tree16_leaf               6
2302          layer1_l2_y_l_rise_tree11_leaf               6
1021           p12mv_(3k_high-3k_low)/3k_avg               5
134               (20k_high-20k_low)/20k_avg               4

      importance_percent
2238               12.22
2290                9.67
2212                7.89
2122                3.67
2160                3.00
2186                1.78
895                 1.56
568                 1.33
2025                1.22
2148                1.22
1705                1.11
2245                1.11
2048                0.89
1865                0.89
2237                0.78
2230                0.67
2229                0.67
2302                0.67
1021                0.56
134                 0.44
Among 2316 features, 1971 features are not used in the model
test range: 20170101 20170701
2017-01-01 2017-06-30
Time slice keys in hdf5: 2017/0101-0101

Current key: 2017/0101-0101
Current slice size(length): 333371

Total concatenating size: 333371
Result dataset size: 158718

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      0        NaN          NaN       NaN       NaN       NaN
(-0.20,-0.15]      0        NaN          NaN       NaN       NaN       NaN
(-0.15,-0.10]      9     -0.087       -0.102     0.129     0.090    -0.287
(-0.10,-0.05]    150     -0.020       -0.060     0.142     0.442    -0.342
(-0.05,0.00]    6389     -0.011        0.000     0.115     0.965    -0.374
(0.00,0.05]    82707      0.003        0.000     0.093     1.172    -0.450
(0.05,0.10]    63668      0.017        0.030     0.097     1.149    -0.449
(0.10,0.15]     5128      0.041        0.053     0.107     1.105    -0.367
(0.15,0.20]      486      0.076        0.086     0.141     0.914    -0.219
(0.20,0.25]       44      0.076        0.089     0.130     0.322    -0.144
(0.25,0.30]       18      0.015        0.062     0.109     0.164    -0.148
(0.30,0.35]        1      0.111        0.111       NaN     0.111     0.111
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      0        NaN          NaN       NaN       NaN       NaN
(-0.20,-0.15]      0        NaN          NaN       NaN       NaN       NaN
(-0.15,-0.10]      9     -0.107       -0.133     0.169     0.133    -0.356
(-0.10,-0.05]    150     -0.039       -0.097     0.198     0.563    -0.431
(-0.05,0.00]    6389     -0.022       -0.048     0.163     1.240    -0.466
(0.00,0.05]    82707      0.000        0.000     0.130     1.561    -0.495
(0.05,0.10]    63668      0.020        0.046     0.135     1.514    -0.494
(0.10,0.15]     5128      0.050        0.078     0.150     1.432    -0.425
(0.15,0.20]      486      0.095        0.122     0.193     1.130    -0.314
(0.20,0.25]       44      0.089        0.128     0.185     0.399    -0.229
(0.25,0.30]       18      0.019        0.102     0.157     0.211    -0.218
(0.30,0.35]        1      0.161        0.161       NaN     0.161     0.161
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      0        NaN          NaN       NaN       NaN       NaN
(-0.20,-0.15]      0        NaN          NaN       NaN       NaN       NaN
(-0.15,-0.10]      9     -0.067       -0.070     0.089     0.048    -0.218
(-0.10,-0.05]    150     -0.012       -0.017     0.090     0.321    -0.252
(-0.05,0.00]    6389     -0.005        0.000     0.070     0.690    -0.296
(0.00,0.05]    82707      0.003        0.000     0.058     1.010    -0.405
(0.05,0.10]    63668      0.012        0.009     0.061     0.845    -0.403
(0.10,0.15]     5128      0.027        0.023     0.069     0.778    -0.309
(0.15,0.20]      486      0.054        0.045     0.096     0.698    -0.139
(0.20,0.25]       44      0.058        0.052     0.082     0.245    -0.059
(0.25,0.30]       18      0.010        0.023     0.065     0.117    -0.112
(0.30,0.35]        1      0.061        0.061       NaN     0.061     0.061
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      0        NaN          NaN       NaN       NaN       NaN
(-0.20,-0.15]      0        NaN          NaN       NaN       NaN       NaN
(-0.15,-0.10]      9      0.044        0.023     0.051     0.133     0.000
(-0.10,-0.05]    150      0.098        0.065     0.097     0.563     0.000
(-0.05,0.00]    6389      0.075        0.047     0.093     1.240     0.000
(0.00,0.05]    82707      0.064        0.043     0.079     1.561     0.000
(0.05,0.10]    63668      0.079        0.058     0.083     1.514     0.000
(0.10,0.15]     5128      0.105        0.084     0.093     1.432     0.000
(0.15,0.20]      486      0.152        0.123     0.129     1.130     0.000
(0.20,0.25]       44      0.155        0.131     0.104     0.399     0.003
(0.25,0.30]       18      0.103        0.113     0.060     0.211     0.008
(0.30,0.35]        1      0.161        0.161       NaN     0.161     0.161
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

test period:20170701-20180101
dataset range: 20130101 20170701
2013-01-01 2017-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 134597
<class 'pandas.core.frame.DataFrame'>
Int64Index: 134597 entries, 20130607 to 20170609
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 558.0 MB
None
(28, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170324       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170405       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170302       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170517       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170406       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
20170324    NaN
20170405    NaN
20170302    NaN
20170517    NaN
20170406    NaN
Train dates:20130104-20170531
(131732, 2036)

----------Train layer 0----------

Train custom_revenue_y_s_rise
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 70.94s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                               feature  importance_raw  importance_percent
342         (60k_high-60k_low)/60k_avg              20               14.29
1865                          sz50_vol              19               13.57
895                          hs300_vol              16               11.43
2035                              vol0              13                9.29
234            (3k_high-3k_low)/3k_avg              10                7.14
454                     (high-low)/avg              10                7.14
424                 (avg/p60min_avg-1)              10                7.14
1125      p20mv_(5k_avg/p5mv_5k_avg-1)               7                5.00
1495      p6mv_(3k_low-3k_open)/3k_avg               6                4.29
243            (3k_low-3k_open)/3k_avg               4                2.86
1415      p5mv_(5k_high-5k_low)/5k_avg               3                2.14
110                 (20MA/p2mv_20MA-1)               3                2.14
109                 (20MA/p1mv_20MA-1)               3                2.14
453                   (high-close)/avg               3                2.14
370                     (amt/3k_amt-1)               2                1.43
1107  p20mv_(10k_low-10k_open)/10k_avg               2                1.43
133       (20k_high-20k_close)/20k_avg               2                1.43
1219               p2mv_(low-open)/avg               2                1.43
2                   (10MA/p2mv_10MA-1)               1                0.71
539                     (vol/3k_vol-1)               1                0.71
Among 2036 features, 2013 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 71.23s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                   feature  importance_raw  importance_percent
80           (120k_high-120k_low)/120k_avg              20               14.29
2025                                sz_vol              10                7.14
1466       p60mv_(20k_low/p20mv_20k_low-1)              10                7.14
26              (10k_high-10k_low)/10k_avg              10                7.14
1260  p360mv_(120k_high-120k_low)/120k_avg              10                7.14
1511   p750mv_(250k_low/p250mv_250k_low-1)              10                7.14
114                    (20k_amt/60k_amt-1)              10                7.14
1321       p40mv_(20k_avg/p20mv_20k_avg-1)              10                7.14
567                                    amt              10                7.14
1865                              sz50_vol              10                7.14
927       p100mv_(20k_avg/p20mv_20k_avg-1)              10                7.14
132          (20k_close/p80mv_20k_close-1)               8                5.71
491                    (low/p120min_low-1)               7                5.00
1000           p120mv_upper_shadow/60k_avg               2                1.43
350            (60k_low-60k_close)/60k_avg               2                1.43
88          (120k_low-120k_close)/120k_avg               1                0.71
Among 2036 features, 2020 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 72.76s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                               feature  importance_raw  importance_percent
1865                          sz50_vol              19               13.57
2025                            sz_vol              12                8.57
895                          hs300_vol              12                8.57
342         (60k_high-60k_low)/60k_avg              11                7.86
454                     (high-low)/avg               7                5.00
1705                            sh_vol               6                4.29
1125      p20mv_(5k_avg/p5mv_5k_avg-1)               4                2.86
567                                amt               4                2.86
2035                              vol0               4                2.86
430            (close/p120max_close-1)               3                2.14
1490    p6mv_(3k_high-3k_close)/3k_avg               3                2.14
22       (10k_close/p30mv_10k_close-1)               3                2.14
1138        p20mv_lower_shadow/20k_avg               2                1.43
2034                               vol               2                1.43
732                            cyb_vol               2                1.43
132      (20k_close/p80mv_20k_close-1)               2                1.43
1303          p3mv_lower_shadow/3k_avg               1                0.71
1107  p20mv_(10k_low-10k_open)/10k_avg               1                0.71
487                (low/p10mean_low-1)               1                0.71
1212             p2mv_(close-open)/avg               1                0.71
Among 2036 features, 1976 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 66.28s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 68.12s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
567                                      amt              10
1865                                sz50_vol              10
1705                                  sh_vol               6
454                           (high-low)/avg               6
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               5
342               (60k_high-60k_low)/60k_avg               5
2025                                  sz_vol               5
468                   (high/p250mean_high-1)               4
1187            p250mv_upper_shadow/250k_avg               3
1517         p80mv_(20k_avg/p20mv_20k_avg-1)               3
2027                   upper_shadow/120k_avg               3
496                      (low/p250max_low-1)               3
430                  (close/p120max_close-1)               3
124                (20k_avg/p60mv_20k_avg-1)               2
133             (20k_high-20k_close)/20k_avg               2
22             (10k_close/p30mv_10k_close-1)               2
1229     p300mv_(60k_high-60k_close)/60k_avg               2
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               2
1283           p3mv_(3k_low-3k_close)/3k_avg               2

      importance_percent
567                 7.14
1865                7.14
1705                4.29
454                 4.29
1112                3.57
342                 3.57
2025                3.57
468                 2.86
1187                2.14
1517                2.14
2027                2.14
496                 2.14
430                 2.14
124                 1.43
133                 1.43
22                  1.43
1229                1.43
1009                1.43
1457                1.43
1283                1.43
Among 2036 features, 1963 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 65.53s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
342           (60k_high-60k_low)/60k_avg              12                8.57
1865                            sz50_vol              11                7.86
895                            hs300_vol               9                6.43
454                       (high-low)/avg               8                5.71
567                                  amt               7                5.00
234              (3k_high-3k_low)/3k_avg               7                5.00
2025                              sz_vol               7                5.00
134           (20k_high-20k_low)/20k_avg               6                4.29
1418       p5mv_(5k_low-5k_close)/5k_avg               5                3.57
468               (high/p250mean_high-1)               4                2.86
1705                              sh_vol               3                2.14
80         (120k_high-120k_low)/120k_avg               3                2.14
2035                                vol0               3                2.14
1125        p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
88        (120k_low-120k_close)/120k_avg               2                1.43
288              (5k_high-5k_low)/5k_avg               2                1.43
1311  p40mv_(10k_high-10k_close)/10k_avg               2                1.43
1415        p5mv_(5k_high-5k_low)/5k_avg               2                1.43
142          (20k_low-20k_close)/20k_avg               2                1.43
1107    p20mv_(10k_low-10k_open)/10k_avg               2                1.43
Among 2036 features, 1980 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 71.21s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
2025                              sz_vol              17               12.14
1865                            sz50_vol              16               11.43
895                            hs300_vol              14               10.00
22         (10k_close/p30mv_10k_close-1)              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
1102  p20mv_(10k_high-10k_close)/10k_avg              10                7.14
1251   p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
1138          p20mv_lower_shadow/20k_avg              10                7.14
248               (3k_low/p6mv_3k_low-1)              10                7.14
547                   (vol/p20max_vol-1)               7                5.00
1495        p6mv_(3k_low-3k_open)/3k_avg               7                5.00
1298                p3mv_(low-close)/avg               4                2.86
1490      p6mv_(3k_high-3k_close)/3k_avg               3                2.14
548                  (vol/p20mean_vol-1)               3                2.14
243              (3k_low-3k_open)/3k_avg               3                2.14
531                   (open/p4mv_open-1)               3                2.14
732                              cyb_vol               3                2.14
Among 2036 features, 2019 features are not used in the model

Train custom_revenue_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 69.46s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                      feature  importance_raw  \
1865                                 sz50_vol              27
2025                                   sz_vol              18
1705                                   sh_vol              18
1457          p60mv_(20k_avg/p20mv_20k_avg-1)              10
1000              p120mv_upper_shadow/60k_avg              10
1061         p180mv_(60k_amt/p60mv_60k_amt-1)               9
1536             p9mv_(3k_high-3k_low)/3k_avg               9
2035                                     vol0               9
132             (20k_close/p80mv_20k_close-1)               9
1138               p20mv_lower_shadow/20k_avg               8
1321          p40mv_(20k_avg/p20mv_20k_avg-1)               2
1241          p30mv_(10k_avg/p10mv_10k_avg-1)               1
124                 (20k_avg/p60mv_20k_avg-1)               1
307                 (5k_open/p20mv_5k_open-1)               1
550                       (vol/p250max_vol-1)               1
993         p120mv_(60k_low-60k_open)/60k_avg               1
454                            (high-low)/avg               1
1347      p480mv_(120k_low/p120mv_120k_low-1)               1
1009   p1250mv_(250k_low-250k_close)/250k_avg               1
1340  p480mv_(120k_close/p120mv_120k_close-1)               1

      importance_percent
1865               19.29
2025               12.86
1705               12.86
1457                7.14
1000                7.14
1061                6.43
1536                6.43
2035                6.43
132                 6.43
1138                5.71
1321                1.43
1241                0.71
124                 0.71
307                 0.71
550                 0.71
993                 0.71
454                 0.71
1347                0.71
1009                0.71
1340                0.71
Among 2036 features, 2014 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 66.52s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              14
2025                                  sz_vol              11
1009  p1250mv_(250k_low-250k_close)/250k_avg               5
1705                                  sh_vol               5
895                                hs300_vol               4
567                                      amt               4
430                  (close/p120max_close-1)               4
2035                                    vol0               3
1229     p300mv_(60k_high-60k_close)/60k_avg               3
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               3
454                           (high-low)/avg               3
986      p120mv_(60k_close-60k_open)/60k_avg               3
1138              p20mv_lower_shadow/20k_avg               3
124                (20k_avg/p60mv_20k_avg-1)               3
1268            p360mv_lower_shadow/120k_avg               2
1347     p480mv_(120k_low/p120mv_120k_low-1)               2
1010   p1250mv_(250k_low-250k_open)/250k_avg               2
1321         p40mv_(20k_avg/p20mv_20k_avg-1)               2
898                                      low               2
1449   p600mv_(120k_low-120k_close)/120k_avg               2

      importance_percent
1865               10.00
2025                7.86
1009                3.57
1705                3.57
895                 2.86
567                 2.86
430                 2.86
2035                2.14
1229                2.14
1112                2.14
454                 2.14
986                 2.14
1138                2.14
124                 2.14
1268                1.43
1347                1.43
1010                1.43
1321                1.43
898                 1.43
1449                1.43
Among 2036 features, 1959 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 68.19s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              30               21.43
26            (10k_high-10k_low)/10k_avg              20               14.29
80         (120k_high-120k_low)/120k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
432              (close/p120min_close-1)              10                7.14
345         (60k_high/p120mv_60k_high-1)              10                7.14
2025                              sz_vol              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
1241     p30mv_(10k_avg/p10mv_10k_avg-1)               7                5.00
1138          p20mv_lower_shadow/20k_avg               7                5.00
486                   (low/p10max_low-1)               5                3.57
303               (5k_low/p5mv_5k_low-1)               5                3.57
946     p10mv_(10k_high-10k_low)/10k_avg               3                2.14
538                 (open/p60min_open-1)               3                2.14
Among 2036 features, 2022 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 63.77s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 69.21s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                   feature  importance_raw  importance_percent
1865                              sz50_vol              20               14.29
430                (close/p120max_close-1)              12                8.57
454                         (high-low)/avg              10                7.14
342             (60k_high-60k_low)/60k_avg              10                7.14
567                                    amt              10                7.14
1705                                sh_vol               7                5.00
2027                 upper_shadow/120k_avg               6                4.29
496                    (low/p250max_low-1)               6                4.29
133           (20k_high-20k_close)/20k_avg               6                4.29
1519   p80mv_(20k_close/p20mv_20k_close-1)               5                3.57
895                              hs300_vol               5                3.57
124              (20k_avg/p60mv_20k_avg-1)               5                3.57
1112       p20mv_(20k_avg/p20mv_20k_avg-1)               5                3.57
1350          p480mv_lower_shadow/120k_avg               5                3.57
1241       p30mv_(10k_avg/p10mv_10k_avg-1)               5                3.57
324             (60k_amt/p180mv_60k_amt-1)               4                2.86
1510  p750mv_(250k_low-250k_open)/250k_avg               3                2.14
437                (close/p250max_close-1)               3                2.14
450                 (close/p60max_close-1)               3                2.14
326             (60k_amt/p300mv_60k_amt-1)               2                1.43
Among 2036 features, 2008 features are not used in the model
2013-01-01 2017-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 134697
<class 'pandas.core.frame.DataFrame'>
Int64Index: 134697 entries, 20130807 to 20170306
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 558.4 MB
None
(21, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170110       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170303       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170425       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170227       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170109       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170607       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170518       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170222       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
20170110    NaN
20170303    NaN
20170425    NaN
20170125    NaN
20170227    NaN
20170117    NaN
20170109    NaN
20170607    NaN
20170518    NaN
20170222    NaN
Train dates:20130104-20170531
(131823, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 87.26s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
895                                   hs300_vol              40
2025                                     sz_vol              37
1865                                   sz50_vol              36
1705                                     sh_vol              33
2060        layer0_custom_revenue2_y_s_avg_pred              29
2122        layer0_custom_revenue2_y_l_avg_pred              20
2134     layer0_custom_revenue_y_s_decline_pred              14
732                                     cyb_vol              12
2098         layer0_custom_revenue_y_s_avg_pred               9
1437                   p5mv_lower_shadow/5k_avg               7
453                            (high-close)/avg               7
1094                    p1mv_(open/p1mv_open-1)               6
1198               p25mv_(5k_low/p5mv_5k_low-1)               5
1215                        p2mv_(high-low)/avg               5
937          p100mv_(20k_open/p20mv_20k_open-1)               4
1400          p50mv_(10k_open/p10mv_10k_open-1)               4
2074       layer0_custom_revenue2_y_l_rise_pred               4
47                  (10k_open/p50mv_10k_open-1)               4
2065  layer0_custom_revenue2_y_s_avg_tree3_leaf               4
2110         layer0_custom_revenue_y_l_avg_pred               4

      importance_percent
895                 5.33
2025                4.93
1865                4.80
1705                4.40
2060                3.87
2122                2.67
2134                1.87
732                 1.60
2098                1.20
1437                0.93
453                 0.93
1094                0.80
1198                0.67
1215                0.67
937                 0.53
1400                0.53
2074                0.53
47                  0.53
2065                0.53
2110                0.53
Among 2160 features, 1813 features are not used in the model

Train l2_y_s_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 82.62s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                      feature  importance_raw  \
895                                 hs300_vol              46
1865                                 sz50_vol              43
1705                                   sh_vol              42
2025                                   sz_vol              40
2060      layer0_custom_revenue2_y_s_avg_pred              20
2110       layer0_custom_revenue_y_l_avg_pred              14
2134   layer0_custom_revenue_y_s_decline_pred              14
2122      layer0_custom_revenue2_y_l_avg_pred              13
732                                   cyb_vol              12
2098       layer0_custom_revenue_y_s_avg_pred              11
2048   layer0_custom_revenue_y_l_decline_pred               9
2111     layer0_custom_revenue_y_l_avg_output               7
1268             p360mv_lower_shadow/120k_avg               7
1005  p1250mv_(250k_high-250k_close)/250k_avg               4
1020          p12mv_(3k_high-3k_close)/3k_avg               4
1215                      p2mv_(high-low)/avg               4
2086     layer0_custom_revenue2_y_s_rise_pred               4
1088                      p1mv_(high-low)/avg               4
938          p100mv_(20k_vol/p20mv_20k_vol-1)               4
567                                       amt               4

      importance_percent
895                 6.13
1865                5.73
1705                5.60
2025                5.33
2060                2.67
2110                1.87
2134                1.87
2122                1.73
732                 1.60
2098                1.47
2048                1.20
2111                0.93
1268                0.93
1005                0.53
1020                0.53
1215                0.53
2086                0.53
1088                0.53
938                 0.53
567                 0.53
Among 2160 features, 1825 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 88.05s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                       feature  importance_raw  \
2122       layer0_custom_revenue2_y_l_avg_pred              45
2025                                    sz_vol              31
1705                                    sh_vol              24
1865                                  sz50_vol              22
2048    layer0_custom_revenue_y_l_decline_pred              20
732                                    cyb_vol              18
1526           p80mv_(20k_low/p20mv_20k_low-1)              15
2110        layer0_custom_revenue_y_l_avg_pred              13
895                                  hs300_vol              11
2111      layer0_custom_revenue_y_l_avg_output              11
567                                        amt               9
2134    layer0_custom_revenue_y_s_decline_pred               9
2060       layer0_custom_revenue2_y_s_avg_pred               8
1238               p300mv_lower_shadow/60k_avg               6
1341    p480mv_(120k_high-120k_close)/120k_avg               6
1466           p60mv_(20k_low/p20mv_20k_low-1)               6
1350              p480mv_lower_shadow/120k_avg               6
1454              p600mv_lower_shadow/120k_avg               6
1269              p360mv_upper_shadow/120k_avg               6
2121  layer0_custom_revenue_y_l_avg_tree9_leaf               5

      importance_percent
2122                6.00
2025                4.13
1705                3.20
1865                2.93
2048                2.67
732                 2.40
1526                2.00
2110                1.73
895                 1.47
2111                1.47
567                 1.20
2134                1.20
2060                1.07
1238                0.80
1341                0.80
1466                0.80
1350                0.80
1454                0.80
1269                0.80
2121                0.67
Among 2160 features, 1847 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 83.65s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2122         layer0_custom_revenue2_y_l_avg_pred              34
567                                          amt              22
1705                                      sh_vol              15
2025                                      sz_vol              14
1865                                    sz50_vol              13
2060         layer0_custom_revenue2_y_s_avg_pred              13
1526             p80mv_(20k_low/p20mv_20k_low-1)              12
732                                      cyb_vol              12
2111        layer0_custom_revenue_y_l_avg_output              11
2134      layer0_custom_revenue_y_s_decline_pred              11
2110          layer0_custom_revenue_y_l_avg_pred              10
2074        layer0_custom_revenue2_y_l_rise_pred              10
2048      layer0_custom_revenue_y_l_decline_pred               9
1466             p60mv_(20k_low/p20mv_20k_low-1)               7
2080  layer0_custom_revenue2_y_l_rise_tree4_leaf               7
453                             (high-close)/avg               7
895                                    hs300_vol               7
2083  layer0_custom_revenue2_y_l_rise_tree7_leaf               6
937           p100mv_(20k_open/p20mv_20k_open-1)               6
2098          layer0_custom_revenue_y_s_avg_pred               6

      importance_percent
2122                4.53
567                 2.93
1705                2.00
2025                1.87
1865                1.73
2060                1.73
1526                1.60
732                 1.60
2111                1.47
2134                1.47
2110                1.33
2074                1.33
2048                1.20
1466                0.93
2080                0.93
453                 0.93
895                 0.93
2083                0.80
937                 0.80
2098                0.80
Among 2160 features, 1800 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 82.48s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2086       layer0_custom_revenue2_y_s_rise_pred              28
1705                                     sh_vol              26
2060        layer0_custom_revenue2_y_s_avg_pred              24
895                                   hs300_vol              21
2025                                     sz_vol              19
2122        layer0_custom_revenue2_y_l_avg_pred              19
1865                                   sz50_vol              16
2074       layer0_custom_revenue2_y_l_rise_pred              10
568                                        area               9
2075     layer0_custom_revenue2_y_l_rise_output               8
454                              (high-low)/avg               8
453                            (high-close)/avg               6
2098         layer0_custom_revenue_y_s_avg_pred               5
2070  layer0_custom_revenue2_y_s_avg_tree8_leaf               5
2033                           upper_shadow/avg               5
2134     layer0_custom_revenue_y_s_decline_pred               5
1198               p25mv_(5k_low/p5mv_5k_low-1)               4
1544                   p9mv_lower_shadow/3k_avg               4
2125  layer0_custom_revenue2_y_l_avg_tree1_leaf               4
943          p10mv_(10k_close-10k_open)/10k_avg               4

      importance_percent
2086                3.73
1705                3.47
2060                3.20
895                 2.80
2025                2.53
2122                2.53
1865                2.13
2074                1.33
568                 1.20
2075                1.07
454                 1.07
453                 0.80
2098                0.67
2070                0.67
2033                0.67
2134                0.67
1198                0.53
1544                0.53
2125                0.53
943                 0.53
Among 2160 features, 1766 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 84.03s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2122     layer0_custom_revenue2_y_l_avg_pred              41
568                                     area              21
2075  layer0_custom_revenue2_y_l_rise_output              17
567                                      amt              14
2025                                  sz_vol              13
2086    layer0_custom_revenue2_y_s_rise_pred              13
2074    layer0_custom_revenue2_y_l_rise_pred              11
2060     layer0_custom_revenue2_y_s_avg_pred              10
2110      layer0_custom_revenue_y_l_avg_pred               8
2034                                     vol               8
2148     layer0_custom_revenue_y_l_rise_pred               7
2035                                    vol0               7
1526         p80mv_(20k_low/p20mv_20k_low-1)               5
986      p120mv_(60k_close-60k_open)/60k_avg               5
1865                                sz50_vol               5
1521        p80mv_(20k_high-20k_low)/20k_avg               5
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               4
47               (10k_open/p50mv_10k_open-1)               4
1235        p300mv_(60k_low/p60mv_60k_low-1)               4
1186            p250mv_lower_shadow/250k_avg               4

      importance_percent
2122                5.47
568                 2.80
2075                2.27
567                 1.87
2025                1.73
2086                1.73
2074                1.47
2060                1.33
2110                1.07
2034                1.07
2148                0.93
2035                0.93
1526                0.67
986                 0.67
1865                0.67
1521                0.67
1457                0.53
47                  0.53
1235                0.53
1186                0.53
Among 2160 features, 1765 features are not used in the model
2013-01-01 2017-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 134730
<class 'pandas.core.frame.DataFrame'>
Int64Index: 134730 entries, 20130819 to 20170418
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 558.5 MB
None
(27, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170223       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170522       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170515       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170526       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170215       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170523       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170220       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170502       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170308       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170126       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
20170223    NaN
20170124    NaN
20170522    NaN
20170515    NaN
20170526    NaN
20170215    NaN
20170523    NaN
20170220    NaN
20170502    NaN
20170308    NaN
20170126    NaN
Train dates:20130104-20170531
(131992, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 100.89s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=30,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2238                      layer1_l2_y_l_avg_pred              99
2212                  layer1_l2_y_l_decline_pred              96
2290                     layer1_l2_y_l_rise_pred              87
2160                      layer1_l2_y_s_avg_pred              42
2186                  layer1_l2_y_s_decline_pred              23
2025                                      sz_vol              21
2122         layer0_custom_revenue2_y_l_avg_pred              21
1865                                    sz50_vol              21
895                                    hs300_vol              20
568                                         area              12
2228           layer1_l2_y_l_decline_tree15_leaf              10
2134      layer0_custom_revenue_y_s_decline_pred               8
2148         layer0_custom_revenue_y_l_rise_pred               8
2190            layer1_l2_y_s_decline_tree3_leaf               8
2048      layer0_custom_revenue_y_l_decline_pred               6
905                          lower_shadow/5k_avg               6
2127   layer0_custom_revenue2_y_l_avg_tree3_leaf               5
26                    (10k_high-10k_low)/10k_avg               4
2095  layer0_custom_revenue2_y_s_rise_tree7_leaf               4
343                  (60k_high-60k_open)/60k_avg               4

      importance_percent
2238               11.00
2212               10.67
2290                9.67
2160                4.67
2186                2.56
2025                2.33
2122                2.33
1865                2.33
895                 2.22
568                 1.33
2228                1.11
2134                0.89
2148                0.89
2190                0.89
2048                0.67
905                 0.67
2127                0.56
26                  0.44
2095                0.44
343                 0.44
Among 2316 features, 2008 features are not used in the model
test range: 20170701 20180101
2017-07-01 2017-12-31
Time slice keys in hdf5: 2017/0101-0101

Current key: 2017/0101-0101
Current slice size(length): 333371

Total concatenating size: 333371
Result dataset size: 173294

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      3     -0.106       -0.094     0.035    -0.079    -0.145
(-0.25,-0.20]      3     -0.094       -0.084     0.039    -0.061    -0.136
(-0.20,-0.15]     67     -0.067       -0.110     0.118     0.353    -0.213
(-0.15,-0.10]    409     -0.002       -0.025     0.114     0.395    -0.254
(-0.10,-0.05]  10278      0.037        0.044     0.114     0.780    -0.374
(-0.05,0.00]   52179      0.020        0.024     0.100     0.837    -0.365
(0.00,0.05]    96413      0.013        0.000     0.091     1.134    -0.339
(0.05,0.10]    12095      0.021        0.031     0.108     1.448    -0.315
(0.10,0.15]     1303      0.023        0.038     0.118     1.132    -0.322
(0.15,0.20]      269      0.020       -0.038     0.134     0.510    -0.229
(0.20,0.25]       52      0.035        0.030     0.152     0.412    -0.191
(0.25,0.30]        6      0.078       -0.035     0.254     0.403    -0.138
(0.30,0.35]        1     -0.067       -0.067       NaN    -0.067    -0.067
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      3     -0.165       -0.157     0.029    -0.141    -0.197
(-0.25,-0.20]      3     -0.146       -0.150     0.043    -0.101    -0.187
(-0.20,-0.15]     67     -0.088       -0.160     0.171     0.465    -0.279
(-0.15,-0.10]    409     -0.003       -0.046     0.165     0.557    -0.325
(-0.10,-0.05]  10278      0.053        0.071     0.160     1.127    -0.458
(-0.05,0.00]   52179      0.027        0.041     0.140     1.206    -0.454
(0.00,0.05]    96413      0.016        0.000     0.128     1.507    -0.429
(0.05,0.10]    12095      0.027        0.052     0.151     2.058    -0.406
(0.10,0.15]     1303      0.030        0.060     0.163     1.530    -0.422
(0.15,0.20]      269      0.025       -0.059     0.182     0.665    -0.273
(0.20,0.25]       52      0.050        0.065     0.207     0.525    -0.241
(0.25,0.30]        6      0.099       -0.012     0.330     0.508    -0.208
(0.30,0.35]        1     -0.100       -0.100       NaN    -0.100    -0.100
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      3     -0.047       -0.030     0.041    -0.017    -0.093
(-0.25,-0.20]      3     -0.041       -0.020     0.038    -0.018    -0.085
(-0.20,-0.15]     67     -0.039       -0.063     0.073     0.240    -0.148
(-0.15,-0.10]    409     -0.002       -0.002     0.067     0.233    -0.184
(-0.10,-0.05]  10278      0.022        0.014     0.072     0.470    -0.290
(-0.05,0.00]   52179      0.012        0.004     0.063     0.549    -0.281
(0.00,0.05]    96413      0.008        0.000     0.057     0.786    -0.262
(0.05,0.10]    12095      0.014        0.006     0.069     0.839    -0.241
(0.10,0.15]     1303      0.014        0.008     0.076     0.734    -0.222
(0.15,0.20]      269      0.016       -0.003     0.088     0.355    -0.187
(0.20,0.25]       52      0.025        0.007     0.099     0.298    -0.142
(0.25,0.30]        6      0.073       -0.012     0.173     0.300    -0.069
(0.30,0.35]        1     -0.034       -0.034       NaN    -0.034    -0.034
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      3      0.104        0.114     0.038     0.136     0.061
(-0.25,-0.20]      3      0.077        0.093     0.056     0.124     0.015
(-0.20,-0.15]     67      0.081        0.063     0.080     0.465     0.000
(-0.15,-0.10]    409      0.095        0.079     0.081     0.557     0.000
(-0.10,-0.05]  10278      0.109        0.078     0.106     1.127     0.000
(-0.05,0.00]   52179      0.084        0.059     0.091     1.206     0.000
(0.00,0.05]    96413      0.072        0.047     0.082     1.507     0.000
(0.05,0.10]    12095      0.090        0.065     0.101     2.058     0.000
(0.10,0.15]     1303      0.100        0.076     0.102     1.530     0.000
(0.15,0.20]      269      0.112        0.074     0.115     0.665     0.000
(0.20,0.25]       52      0.129        0.083     0.135     0.525     0.000
(0.25,0.30]        6      0.186        0.048     0.250     0.508     0.000
(0.30,0.35]        1      0.027        0.027       NaN     0.027     0.027
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

test period:20180101-20180701
dataset range: 20130101 20180101
2013-01-01 2017-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 152144
<class 'pandas.core.frame.DataFrame'>
Int64Index: 152144 entries, 20130607 to 20170609
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 630.7 MB
None
(51, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170324       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170707       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170405       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
20170324    NaN
20170707    NaN
20170926    NaN
20170926    NaN
20170405    NaN
Train dates:20130104-20171130
(149126, 2036)

----------Train layer 0----------

Train custom_revenue_y_s_rise
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 80.83s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
1865                        sz50_vol              25               17.86
342       (60k_high-60k_low)/60k_avg              20               14.29
895                        hs300_vol              14               10.00
134       (20k_high-20k_low)/20k_avg              11                7.86
454                   (high-low)/avg              10                7.14
1125    p20mv_(5k_avg/p5mv_5k_avg-1)              10                7.14
1490  p6mv_(3k_high-3k_close)/3k_avg               7                5.00
2035                            vol0               6                4.29
88    (120k_low-120k_close)/120k_avg               6                4.29
567                              amt               6                4.29
234          (3k_high-3k_low)/3k_avg               4                2.86
482             (high/p60min_high-1)               4                2.86
1499        p6mv_lower_shadow/3k_avg               4                2.86
453                 (high-close)/avg               2                1.43
243          (3k_low-3k_open)/3k_avg               2                1.43
1705                          sh_vol               2                1.43
1495    p6mv_(3k_low-3k_open)/3k_avg               2                1.43
1210           p2mv_(amt/p1mv_amt-1)               2                1.43
450           (close/p60max_close-1)               1                0.71
1415    p5mv_(5k_high-5k_low)/5k_avg               1                0.71
Among 2036 features, 2015 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 90.02s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
80            (120k_high-120k_low)/120k_avg              20
88           (120k_low-120k_close)/120k_avg              19
567                                     amt              13
26               (10k_high-10k_low)/10k_avg              10
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1447  p600mv_(120k_high-120k_open)/120k_avg               8
1070      p180mv_(60k_low-60k_open)/60k_avg               8
2025                                 sz_vol               7
132           (20k_close/p80mv_20k_close-1)               7
1466        p60mv_(20k_low/p20mv_20k_low-1)               7
1865                               sz50_vol               7
927        p100mv_(20k_avg/p20mv_20k_avg-1)               7
1000            p120mv_upper_shadow/60k_avg               6
1511    p750mv_(250k_low/p250mv_250k_low-1)               2
1479        p60mv_(60k_low/p60mv_60k_low-1)               2
1260   p360mv_(120k_high-120k_low)/120k_avg               2
895                               hs300_vol               2
1526        p80mv_(20k_low/p20mv_20k_low-1)               1
134              (20k_high-20k_low)/20k_avg               1
972     p120mv_(120k_avg/p120mv_120k_avg-1)               1

      importance_percent
80                 14.29
88                 13.57
567                 9.29
26                  7.14
1321                7.14
1447                5.71
1070                5.71
2025                5.00
132                 5.00
1466                5.00
1865                5.00
927                 5.00
1000                4.29
1511                1.43
1479                1.43
1260                1.43
895                 1.43
1526                0.71
134                 0.71
972                 0.71
Among 2036 features, 2016 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 82.45s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
1865                               sz50_vol              17
2025                                 sz_vol              17
895                               hs300_vol              13
342              (60k_high-60k_low)/60k_avg              12
454                          (high-low)/avg               8
567                                     amt               6
430                 (close/p120max_close-1)               4
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               4
132           (20k_close/p80mv_20k_close-1)               3
1449  p600mv_(120k_low-120k_close)/120k_avg               3
22            (10k_close/p30mv_10k_close-1)               3
248                  (3k_low/p6mv_3k_low-1)               2
1705                                 sh_vol               2
2035                                   vol0               2
453                        (high-close)/avg               2
1489        p6mv_(3k_close/p3mv_3k_close-1)               2
1418          p5mv_(5k_low-5k_close)/5k_avg               2
1251      p30mv_(10k_open/p10mv_10k_open-1)               2
1219                    p2mv_(low-open)/avg               2
1490         p6mv_(3k_high-3k_close)/3k_avg               2

      importance_percent
1865               12.14
2025               12.14
895                 9.29
342                 8.57
454                 5.71
567                 4.29
430                 2.86
1125                2.86
132                 2.14
1449                2.14
22                  2.14
248                 1.43
1705                1.43
2035                1.43
453                 1.43
1489                1.43
1418                1.43
1251                1.43
1219                1.43
1490                1.43
Among 2036 features, 1986 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 81.89s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 81.21s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
567                                      amt              12
1865                                sz50_vol              11
342               (60k_high-60k_low)/60k_avg              10
454                           (high-low)/avg               8
2025                                  sz_vol               7
430                  (close/p120max_close-1)               5
1705                                  sh_vol               5
133             (20k_high-20k_close)/20k_avg               4
1283           p3mv_(3k_low-3k_close)/3k_avg               3
468                   (high/p250mean_high-1)               3
142              (20k_low-20k_close)/20k_avg               3
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               3
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
895                                hs300_vol               2
1366                 p4mv_(high/p1mv_high-1)               2
288                  (5k_high-5k_low)/5k_avg               2
1375     p500mv_(250k_avg/p250mv_250k_avg-1)               2
22             (10k_close/p30mv_10k_close-1)               2
1268            p360mv_lower_shadow/120k_avg               2
1235        p300mv_(60k_low/p60mv_60k_low-1)               2

      importance_percent
567                 8.57
1865                7.86
342                 7.14
454                 5.71
2025                5.00
430                 3.57
1705                3.57
133                 2.86
1283                2.14
468                 2.14
142                 2.14
1112                2.14
1009                1.43
895                 1.43
1366                1.43
288                 1.43
1375                1.43
22                  1.43
1268                1.43
1235                1.43
Among 2036 features, 1972 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 80.79s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
342       (60k_high-60k_low)/60k_avg              14               10.00
1865                        sz50_vol              11                7.86
454                   (high-low)/avg              10                7.14
2025                          sz_vol               9                6.43
567                              amt               9                6.43
1418   p5mv_(5k_low-5k_close)/5k_avg               6                4.29
234          (3k_high-3k_low)/3k_avg               6                4.29
134       (20k_high-20k_low)/20k_avg               5                3.57
2035                            vol0               5                3.57
895                        hs300_vol               5                3.57
1705                          sh_vol               4                2.86
80     (120k_high-120k_low)/120k_avg               4                2.86
468           (high/p250mean_high-1)               4                2.86
142      (20k_low-20k_close)/20k_avg               3                2.14
1125    p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
1490  p6mv_(3k_high-3k_close)/3k_avg               3                2.14
908                           market               2                1.43
26        (10k_high-10k_low)/10k_avg               2                1.43
88    (120k_low-120k_close)/120k_avg               2                1.43
288          (5k_high-5k_low)/5k_avg               2                1.43
Among 2036 features, 1989 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 80.29s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
1865                            sz50_vol              15               10.71
895                            hs300_vol              13                9.29
2025                              sz_vol              10                7.14
1251   p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
732                              cyb_vol              10                7.14
454                       (high-low)/avg              10                7.14
1138          p20mv_lower_shadow/20k_avg              10                7.14
1219                 p2mv_(low-open)/avg               7                5.00
1490      p6mv_(3k_high-3k_close)/3k_avg               5                3.57
531                   (open/p4mv_open-1)               5                3.57
937   p100mv_(20k_open/p20mv_20k_open-1)               5                3.57
132        (20k_close/p80mv_20k_close-1)               5                3.57
1536        p9mv_(3k_high-3k_low)/3k_avg               5                3.57
22         (10k_close/p30mv_10k_close-1)               5                3.57
303               (5k_low/p5mv_5k_low-1)               5                3.57
248               (3k_low/p6mv_3k_low-1)               5                3.57
1495        p6mv_(3k_low-3k_open)/3k_avg               5                3.57
519                   (open/p1mv_open-1)               4                2.86
563                   (vol/p60max_vol-1)               3                2.14
2035                                vol0               2                1.43
Among 2036 features, 2015 features are not used in the model

Train custom_revenue_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 87.09s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              18
2025                                  sz_vol              16
1705                                  sh_vol              14
1000             p120mv_upper_shadow/60k_avg              10
1457         p60mv_(20k_avg/p20mv_20k_avg-1)              10
132            (20k_close/p80mv_20k_close-1)              10
430                  (close/p120max_close-1)              10
1536            p9mv_(3k_high-3k_low)/3k_avg              10
1138              p20mv_lower_shadow/20k_avg               8
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               6
2035                                    vol0               6
1009  p1250mv_(250k_low-250k_close)/250k_avg               4
1321         p40mv_(20k_avg/p20mv_20k_avg-1)               2
496                      (low/p250max_low-1)               2
454                           (high-low)/avg               2
1347     p480mv_(120k_low/p120mv_120k_low-1)               2
550                      (vol/p250max_vol-1)               2
946         p10mv_(10k_high-10k_low)/10k_avg               2
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               2
947        p10mv_(10k_high-10k_open)/10k_avg               2

      importance_percent
1865               12.86
2025               11.43
1705               10.00
1000                7.14
1457                7.14
132                 7.14
430                 7.14
1536                7.14
1138                5.71
1061                4.29
2035                4.29
1009                2.86
1321                1.43
496                 1.43
454                 1.43
1347                1.43
550                 1.43
946                 1.43
1112                1.43
947                 1.43
Among 2036 features, 2015 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 82.27s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              12
2025                                  sz_vol               7
1705                                  sh_vol               7
1009  p1250mv_(250k_low-250k_close)/250k_avg               5
430                  (close/p120max_close-1)               4
895                                hs300_vol               4
2035                                    vol0               4
1449   p600mv_(120k_low-120k_close)/120k_avg               3
1321         p40mv_(20k_avg/p20mv_20k_avg-1)               3
1229     p300mv_(60k_high-60k_close)/60k_avg               3
1241         p30mv_(10k_avg/p10mv_10k_avg-1)               3
2034                                     vol               3
1347     p480mv_(120k_low/p120mv_120k_low-1)               3
454                           (high-low)/avg               3
1268            p360mv_lower_shadow/120k_avg               3
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               2
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               2
567                                      amt               2
453                         (high-close)/avg               2
154              (20k_open/p60mv_20k_open-1)               2

      importance_percent
1865                8.57
2025                5.00
1705                5.00
1009                3.57
430                 2.86
895                 2.86
2035                2.86
1449                2.14
1321                2.14
1229                2.14
1241                2.14
2034                2.14
1347                2.14
454                 2.14
1268                2.14
1457                1.43
1061                1.43
567                 1.43
453                 1.43
154                 1.43
Among 2036 features, 1961 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 85.29s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              30               21.43
486                   (low/p10max_low-1)              10                7.14
2025                              sz_vol              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
134           (20k_high-20k_low)/20k_avg              10                7.14
432              (close/p120min_close-1)              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
491                  (low/p120min_low-1)               6                4.29
345         (60k_high/p120mv_60k_high-1)               6                4.29
1138          p20mv_lower_shadow/20k_avg               6                4.29
946     p10mv_(10k_high-10k_low)/10k_avg               4                2.86
1705                              sh_vol               4                2.86
538                 (open/p60min_open-1)               4                2.86
Among 2036 features, 2021 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 80.68s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 84.46s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                   feature  importance_raw  importance_percent
1865                              sz50_vol              21               15.00
342             (60k_high-60k_low)/60k_avg              18               12.86
430                (close/p120max_close-1)              12                8.57
567                                    amt              10                7.14
454                         (high-low)/avg              10                7.14
895                              hs300_vol               7                5.00
133           (20k_high-20k_close)/20k_avg               6                4.29
1705                                sh_vol               6                4.29
1112       p20mv_(20k_avg/p20mv_20k_avg-1)               5                3.57
1519   p80mv_(20k_close/p20mv_20k_close-1)               5                3.57
1241       p30mv_(10k_avg/p10mv_10k_avg-1)               4                2.86
2027                 upper_shadow/120k_avg               4                2.86
124              (20k_avg/p60mv_20k_avg-1)               4                2.86
496                    (low/p250max_low-1)               4                2.86
1350          p480mv_lower_shadow/120k_avg               3                2.14
1268          p360mv_lower_shadow/120k_avg               2                1.43
1061      p180mv_(60k_amt/p60mv_60k_amt-1)               2                1.43
346           (60k_high/p180mv_60k_high-1)               2                1.43
987   p120mv_(60k_close/p60mv_60k_close-1)               2                1.43
437                (close/p250max_close-1)               2                1.43
Among 2036 features, 2008 features are not used in the model
2013-01-01 2017-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 152144
<class 'pandas.core.frame.DataFrame'>
Int64Index: 152144 entries, 20130807 to 20170306
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 630.7 MB
None
(45, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170110       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170706       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170303       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170425       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171116       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170227       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170822       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
20171214    NaN
20171115    NaN
20170110    NaN
20170706    NaN
20170303    NaN
20170425    NaN
20170125    NaN
20171116    NaN
20170227    NaN
20170822    NaN
Train dates:20130104-20171130
(149201, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 100.87s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2025                                     sz_vol              41
1865                                   sz50_vol              36
1705                                     sh_vol              32
895                                   hs300_vol              31
2060        layer0_custom_revenue2_y_s_avg_pred              26
2134     layer0_custom_revenue_y_s_decline_pred              17
2122        layer0_custom_revenue2_y_l_avg_pred              15
732                                     cyb_vol              13
2098         layer0_custom_revenue_y_s_avg_pred              11
1219                        p2mv_(low-open)/avg               8
453                            (high-close)/avg               7
2110         layer0_custom_revenue_y_l_avg_pred               7
2099       layer0_custom_revenue_y_s_avg_output               5
935           p100mv_(20k_low-20k_open)/20k_avg               5
1088                        p1mv_(high-low)/avg               5
519                          (open/p1mv_open-1)               5
2148        layer0_custom_revenue_y_l_rise_pred               5
1215                        p2mv_(high-low)/avg               5
2070  layer0_custom_revenue2_y_s_avg_tree8_leaf               5
1198               p25mv_(5k_low/p5mv_5k_low-1)               5

      importance_percent
2025                5.47
1865                4.80
1705                4.27
895                 4.13
2060                3.47
2134                2.27
2122                2.00
732                 1.73
2098                1.47
1219                1.07
453                 0.93
2110                0.93
2099                0.67
935                 0.67
1088                0.67
519                 0.67
2148                0.67
1215                0.67
2070                0.67
1198                0.67
Among 2160 features, 1818 features are not used in the model

Train l2_y_s_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 102.07s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                      feature  importance_raw  \
2025                                   sz_vol              48
1865                                 sz50_vol              39
895                                 hs300_vol              36
1705                                   sh_vol              31
2060      layer0_custom_revenue2_y_s_avg_pred              24
732                                   cyb_vol              18
2134   layer0_custom_revenue_y_s_decline_pred              15
2110       layer0_custom_revenue_y_l_avg_pred              11
2098       layer0_custom_revenue_y_s_avg_pred               9
2122      layer0_custom_revenue2_y_l_avg_pred               9
2099     layer0_custom_revenue_y_s_avg_output               8
2048   layer0_custom_revenue_y_l_decline_pred               8
1088                      p1mv_(high-low)/avg               6
1526          p80mv_(20k_low/p20mv_20k_low-1)               6
2086     layer0_custom_revenue2_y_s_rise_pred               6
938          p100mv_(20k_vol/p20mv_20k_vol-1)               5
1390          p50mv_(10k_avg/p10mv_10k_avg-1)               5
937        p100mv_(20k_open/p20mv_20k_open-1)               5
350               (60k_low-60k_close)/60k_avg               4
1005  p1250mv_(250k_high-250k_close)/250k_avg               4

      importance_percent
2025                6.40
1865                5.20
895                 4.80
1705                4.13
2060                3.20
732                 2.40
2134                2.00
2110                1.47
2098                1.20
2122                1.20
2099                1.07
2048                1.07
1088                0.80
1526                0.80
2086                0.80
938                 0.67
1390                0.67
937                 0.67
350                 0.53
1005                0.53
Among 2160 features, 1841 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 109.96s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2122        layer0_custom_revenue2_y_l_avg_pred              38
1865                                   sz50_vol              33
2025                                     sz_vol              26
1526            p80mv_(20k_low/p20mv_20k_low-1)              21
2048     layer0_custom_revenue_y_l_decline_pred              21
2110         layer0_custom_revenue_y_l_avg_pred              20
1705                                     sh_vol              19
732                                     cyb_vol              19
2134     layer0_custom_revenue_y_s_decline_pred              10
1238                p300mv_lower_shadow/60k_avg               7
1350               p480mv_lower_shadow/120k_avg               7
1269               p360mv_upper_shadow/120k_avg               6
2129  layer0_custom_revenue2_y_l_avg_tree5_leaf               6
1074                p180mv_lower_shadow/60k_avg               6
1264       p360mv_(120k_low-120k_open)/120k_avg               6
567                                         amt               6
1160        p240mv_(60k_high-60k_close)/60k_avg               6
1062           p180mv_(60k_avg/p60mv_60k_avg-1)               6
1341     p480mv_(120k_high-120k_close)/120k_avg               6
1466            p60mv_(20k_low/p20mv_20k_low-1)               6

      importance_percent
2122                5.07
1865                4.40
2025                3.47
1526                2.80
2048                2.80
2110                2.67
1705                2.53
732                 2.53
2134                1.33
1238                0.93
1350                0.93
1269                0.80
2129                0.80
1074                0.80
1264                0.80
567                 0.80
1160                0.80
1062                0.80
1341                0.80
1466                0.80
Among 2160 features, 1854 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 112.75s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2122         layer0_custom_revenue2_y_l_avg_pred              35
1865                                    sz50_vol              25
2025                                      sz_vol              22
567                                          amt              16
2134      layer0_custom_revenue_y_s_decline_pred              16
2148         layer0_custom_revenue_y_l_rise_pred              15
1705                                      sh_vol              14
732                                      cyb_vol              13
1526             p80mv_(20k_low/p20mv_20k_low-1)              12
2074        layer0_custom_revenue2_y_l_rise_pred              12
1466             p60mv_(20k_low/p20mv_20k_low-1)              11
2110          layer0_custom_revenue_y_l_avg_pred              11
2048      layer0_custom_revenue_y_l_decline_pred              10
2060         layer0_custom_revenue2_y_s_avg_pred              10
895                                    hs300_vol               9
2098          layer0_custom_revenue_y_s_avg_pred               8
2081  layer0_custom_revenue2_y_l_rise_tree5_leaf               7
1463           p60mv_(20k_high/p20mv_20k_high-1)               7
986          p120mv_(60k_close-60k_open)/60k_avg               7
2082  layer0_custom_revenue2_y_l_rise_tree6_leaf               6

      importance_percent
2122                4.67
1865                3.33
2025                2.93
567                 2.13
2134                2.13
2148                2.00
1705                1.87
732                 1.73
1526                1.60
2074                1.60
1466                1.47
2110                1.47
2048                1.33
2060                1.33
895                 1.20
2098                1.07
2081                0.93
1463                0.93
986                 0.93
2082                0.80
Among 2160 features, 1833 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 101.79s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2086        layer0_custom_revenue2_y_s_rise_pred              31
2060         layer0_custom_revenue2_y_s_avg_pred              25
2025                                      sz_vol              20
2122         layer0_custom_revenue2_y_l_avg_pred              16
1705                                      sh_vol              14
1865                                    sz50_vol              12
895                                    hs300_vol              12
2074        layer0_custom_revenue2_y_l_rise_pred              12
454                               (high-low)/avg               9
2134      layer0_custom_revenue_y_s_decline_pred               8
1088                         p1mv_(high-low)/avg               7
2110          layer0_custom_revenue_y_l_avg_pred               7
2034                                         vol               7
2033                            upper_shadow/avg               7
2090  layer0_custom_revenue2_y_s_rise_tree2_leaf               6
2075      layer0_custom_revenue2_y_l_rise_output               6
1087                       p1mv_(high-close)/avg               5
2098          layer0_custom_revenue_y_s_avg_pred               5
2081  layer0_custom_revenue2_y_l_rise_tree5_leaf               5
2035                                        vol0               5

      importance_percent
2086                4.13
2060                3.33
2025                2.67
2122                2.13
1705                1.87
1865                1.60
895                 1.60
2074                1.60
454                 1.20
2134                1.07
1088                0.93
2110                0.93
2034                0.93
2033                0.93
2090                0.80
2075                0.80
1087                0.67
2098                0.67
2081                0.67
2035                0.67
Among 2160 features, 1761 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 103.19s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2122         layer0_custom_revenue2_y_l_avg_pred              34
568                                         area              20
2074        layer0_custom_revenue2_y_l_rise_pred              19
2110          layer0_custom_revenue_y_l_avg_pred              18
567                                          amt              16
1865                                    sz50_vol              13
2060         layer0_custom_revenue2_y_s_avg_pred              11
2148         layer0_custom_revenue_y_l_rise_pred              11
2025                                      sz_vol               9
1456             p60mv_(20k_amt/p20mv_20k_amt-1)               9
2075      layer0_custom_revenue2_y_l_rise_output               9
895                                    hs300_vol               8
2086        layer0_custom_revenue2_y_s_rise_pred               7
2034                                         vol               6
1235            p300mv_(60k_low/p60mv_60k_low-1)               6
1263       p360mv_(120k_low-120k_close)/120k_avg               6
1526             p80mv_(20k_low/p20mv_20k_low-1)               5
993            p120mv_(60k_low-60k_open)/60k_avg               5
453                             (high-close)/avg               5
2081  layer0_custom_revenue2_y_l_rise_tree5_leaf               5

      importance_percent
2122                4.53
568                 2.67
2074                2.53
2110                2.40
567                 2.13
1865                1.73
2060                1.47
2148                1.47
2025                1.20
1456                1.20
2075                1.20
895                 1.07
2086                0.93
2034                0.80
1235                0.80
1263                0.80
1526                0.67
993                 0.67
453                 0.67
2081                0.67
Among 2160 features, 1765 features are not used in the model
2013-01-01 2017-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Total concatenating size: 152144
Result dataset size: 152144
<class 'pandas.core.frame.DataFrame'>
Int64Index: 152144 entries, 20130819 to 20170418
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 630.7 MB
None
(45, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170223       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170522       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170515       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170526       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170920       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171120       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170908       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
20170223    NaN
20171222    NaN
20170124    NaN
20170522    NaN
20170515    NaN
20170526    NaN
20170920    NaN
20171120    NaN
20171027    NaN
20171027    NaN
20170908    NaN
Train dates:20130104-20171130
(149058, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 130.75s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=30,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2238                  layer1_l2_y_l_avg_pred             111
2212              layer1_l2_y_l_decline_pred              88
2290                 layer1_l2_y_l_rise_pred              80
2122     layer0_custom_revenue2_y_l_avg_pred              34
2160                  layer1_l2_y_s_avg_pred              27
2237       layer1_l2_y_l_decline_tree24_leaf              17
2186              layer1_l2_y_s_decline_pred              17
2025                                  sz_vol              16
895                                hs300_vol              15
2148     layer0_custom_revenue_y_l_rise_pred              12
568                                     area              12
2264                 layer1_l2_y_s_rise_pred              10
1705                                  sh_vol               9
1865                                sz50_vol               9
2218        layer1_l2_y_l_decline_tree5_leaf               9
2048  layer0_custom_revenue_y_l_decline_pred               6
2171           layer1_l2_y_s_avg_tree10_leaf               5
2239            layer1_l2_y_l_avg_tree0_leaf               5
2250           layer1_l2_y_l_avg_tree11_leaf               4
2217        layer1_l2_y_l_decline_tree4_leaf               4

      importance_percent
2238               12.33
2212                9.78
2290                8.89
2122                3.78
2160                3.00
2237                1.89
2186                1.89
2025                1.78
895                 1.67
2148                1.33
568                 1.33
2264                1.11
1705                1.00
1865                1.00
2218                1.00
2048                0.67
2171                0.56
2239                0.56
2250                0.44
2217                0.44
Among 2316 features, 1995 features are not used in the model
test range: 20180101 20180701
2018-01-01 2018-06-30
Time slice keys in hdf5: 2018/0101-0101

Current key: 2018/0101-0101
Current slice size(length): 347712

Total concatenating size: 347712
Result dataset size: 169366

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
                count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]       0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]       0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]       0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]       0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]       0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]       0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]       0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]       0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]       0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]       0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]       0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]       0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]       0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]       0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]       0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]       7     -0.013        0.079     0.123     0.092    -0.160
(-0.20,-0.15]      39     -0.123       -0.137     0.081     0.106    -0.230
(-0.15,-0.10]      95     -0.098       -0.124     0.109     0.296    -0.244
(-0.10,-0.05]     833     -0.069       -0.102     0.123     0.421    -0.392
(-0.05,0.00]    17589     -0.040       -0.065     0.113     0.727    -0.612
(0.00,0.05]    102112     -0.020       -0.039     0.102     0.762    -0.607
(0.05,0.10]     36723     -0.006       -0.039     0.125     1.003    -0.546
(0.10,0.15]      9128      0.019        0.038     0.146     0.976    -0.528
(0.15,0.20]      2092      0.044        0.066     0.171     0.654    -0.387
(0.20,0.25]       440      0.054        0.079     0.198     0.621    -0.369
(0.25,0.30]        78      0.066        0.084     0.204     0.565    -0.280
(0.30,0.35]        14      0.108        0.185     0.238     0.541    -0.261
(0.35,0.40]         2      0.288        0.288     0.287     0.490     0.085
(0.40,0.45]         0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]         0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]         0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]         0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]         0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]         0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]         0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]         0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]         0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]         0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]         0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]         0        NaN          NaN       NaN       NaN       NaN

y_l
                count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]       0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]       0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]       0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]       0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]       0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]       0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]       0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]       0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]       0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]       0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]       0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]       0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]       0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]       0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]       0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]       7     -0.072       -0.161     0.197     0.143    -0.278
(-0.20,-0.15]      39     -0.197       -0.203     0.102     0.159    -0.335
(-0.15,-0.10]      95     -0.154       -0.184     0.152     0.369    -0.352
(-0.10,-0.05]     833     -0.108       -0.160     0.174     0.572    -0.453
(-0.05,0.00]    17589     -0.063       -0.109     0.159     0.976    -0.639
(0.00,0.05]    102112     -0.036       -0.074     0.143     1.039    -0.631
(0.05,0.10]     36723     -0.018       -0.082     0.174     1.338    -0.602
(0.10,0.15]      9128      0.014       -0.056     0.202     1.326    -0.563
(0.15,0.20]      2092      0.051        0.103     0.233     0.817    -0.473
(0.20,0.25]       440      0.064        0.117     0.269     0.839    -0.429
(0.25,0.30]        78      0.076        0.131     0.283     0.702    -0.356
(0.30,0.35]        14      0.158        0.290     0.327     0.688    -0.339
(0.35,0.40]         2      0.406        0.406     0.341     0.648     0.165
(0.40,0.45]         0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]         0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]         0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]         0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]         0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]         0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]         0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]         0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]         0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]         0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]         0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]         0        NaN          NaN       NaN       NaN       NaN

y_l_avg
                count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]       0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]       0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]       0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]       0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]       0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]       0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]       0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]       0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]       0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]       0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]       0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]       0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]       0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]       0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]       0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]       7      0.003        0.030     0.041     0.042    -0.045
(-0.20,-0.15]      39     -0.062       -0.066     0.045     0.053    -0.161
(-0.15,-0.10]      95     -0.051       -0.062     0.063     0.223    -0.156
(-0.10,-0.05]     833     -0.037       -0.042     0.073     0.271    -0.337
(-0.05,0.00]    17589     -0.023       -0.022     0.070     0.503    -0.589
(0.00,0.05]    102112     -0.010       -0.008     0.064     0.609    -0.583
(0.05,0.10]     36723     -0.001       -0.006     0.079     0.668    -0.530
(0.10,0.15]      9128      0.015        0.006     0.094     0.688    -0.494
(0.15,0.20]      2092      0.034        0.023     0.114     0.517    -0.355
(0.20,0.25]       440      0.039        0.028     0.133     0.480    -0.314
(0.25,0.30]        78      0.046        0.020     0.131     0.428    -0.223
(0.30,0.35]        14      0.058        0.073     0.154     0.393    -0.183
(0.35,0.40]         2      0.169        0.169     0.233     0.333     0.004
(0.40,0.45]         0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]         0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]         0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]         0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]         0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]         0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]         0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]         0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]         0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]         0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]         0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]         0        NaN          NaN       NaN       NaN       NaN

y_l_rise
                count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]       0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]       0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]       0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]       0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]       0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]       0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]       0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]       0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]       0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]       0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]       0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]       0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]       0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]       0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]       0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]       7      0.110        0.127     0.033     0.143     0.063
(-0.20,-0.15]      39      0.048        0.034     0.040     0.159     0.000
(-0.15,-0.10]      95      0.068        0.050     0.068     0.369     0.000
(-0.10,-0.05]     833      0.075        0.054     0.078     0.572     0.000
(-0.05,0.00]    17589      0.065        0.040     0.076     0.976     0.000
(0.00,0.05]    102112      0.064        0.041     0.072     1.039     0.000
(0.05,0.10]     36723      0.091        0.066     0.089     1.338     0.000
(0.10,0.15]      9128      0.120        0.089     0.112     1.326     0.000
(0.15,0.20]      2092      0.153        0.118     0.135     0.817     0.000
(0.20,0.25]       440      0.179        0.141     0.160     0.839     0.000
(0.25,0.30]        78      0.199        0.161     0.165     0.702     0.005
(0.30,0.35]        14      0.251        0.290     0.202     0.688     0.000
(0.35,0.40]         2      0.406        0.406     0.341     0.648     0.165
(0.40,0.45]         0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]         0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]         0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]         0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]         0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]         0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]         0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]         0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]         0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]         0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]         0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]         0        NaN          NaN       NaN       NaN       NaN

test period:20180701-20190101
dataset range: 20130101 20180701
2013-01-01 2018-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 169080
<class 'pandas.core.frame.DataFrame'>
Int64Index: 169080 entries, 20130607 to 20180604
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 700.9 MB
None
(70, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170324       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170707       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170405       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
20170324    NaN
20170707    NaN
20170926    NaN
20170926    NaN
20170405    NaN
Train dates:20130104-20180530
(166068, 2036)

----------Train layer 0----------

Train custom_revenue_y_s_rise
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 102.03s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
342           (60k_high-60k_low)/60k_avg              20               14.29
895                            hs300_vol              20               14.29
2035                                vol0              12                8.57
1865                            sz50_vol              10                7.14
482                 (high/p60min_high-1)              10                7.14
234              (3k_high-3k_low)/3k_avg              10                7.14
454                       (high-low)/avg              10                7.14
1495        p6mv_(3k_low-3k_open)/3k_avg               7                5.00
453                     (high-close)/avg               4                2.86
110                   (20MA/p2mv_20MA-1)               4                2.86
243              (3k_low-3k_open)/3k_avg               4                2.86
1102  p20mv_(10k_high-10k_close)/10k_avg               4                2.86
133         (20k_high-20k_close)/20k_avg               3                2.14
134           (20k_high-20k_low)/20k_avg               3                2.14
1490      p6mv_(3k_high-3k_close)/3k_avg               3                2.14
1107    p20mv_(10k_low-10k_open)/10k_avg               3                2.14
1219                 p2mv_(low-open)/avg               3                2.14
1415        p5mv_(5k_high-5k_low)/5k_avg               3                2.14
370                       (amt/3k_amt-1)               3                2.14
1125        p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
Among 2036 features, 2015 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 105.24s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
80            (120k_high-120k_low)/120k_avg              20
567                                     amt              20
88           (120k_low-120k_close)/120k_avg              16
134              (20k_high-20k_low)/20k_avg              11
895                               hs300_vol              10
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1446   p600mv_(120k_high-120k_low)/120k_avg              10
1447  p600mv_(120k_high-120k_open)/120k_avg              10
26               (10k_high-10k_low)/10k_avg               9
1000            p120mv_upper_shadow/60k_avg               8
1509  p750mv_(250k_low-250k_close)/250k_avg               7
927        p100mv_(20k_avg/p20mv_20k_avg-1)               4
1526        p80mv_(20k_low/p20mv_20k_low-1)               4
1235       p300mv_(60k_low/p60mv_60k_low-1)               1

      importance_percent
80                 14.29
567                14.29
88                 11.43
134                 7.86
895                 7.14
1321                7.14
1446                7.14
1447                7.14
26                  6.43
1000                5.71
1509                5.00
927                 2.86
1526                2.86
1235                0.71
Among 2036 features, 2022 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 103.81s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
895                               hs300_vol              19
342              (60k_high-60k_low)/60k_avg              13
454                          (high-low)/avg               9
1865                               sz50_vol               9
2025                                 sz_vol               9
1705                                 sh_vol               6
430                 (close/p120max_close-1)               5
2034                                    vol               4
22            (10k_close/p30mv_10k_close-1)               3
1449  p600mv_(120k_low-120k_close)/120k_avg               3
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               3
567                                     amt               3
2035                                   vol0               3
1284           p3mv_(3k_low-3k_open)/3k_avg               2
732                                 cyb_vol               2
1243    p30mv_(10k_close/p10mv_10k_close-1)               2
1495           p6mv_(3k_low-3k_open)/3k_avg               2
1102     p20mv_(10k_high-10k_close)/10k_avg               2
297                 (5k_low-5k_open)/5k_avg               2
404                     (avg/p120min_avg-1)               2

      importance_percent
895                13.57
342                 9.29
454                 6.43
1865                6.43
2025                6.43
1705                4.29
430                 3.57
2034                2.86
22                  2.14
1449                2.14
1125                2.14
567                 2.14
2035                2.14
1284                1.43
732                 1.43
1243                1.43
1495                1.43
1102                1.43
297                 1.43
404                 1.43
Among 2036 features, 1983 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 101.64s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 102.27s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              13
567                                      amt              11
342               (60k_high-60k_low)/60k_avg              10
430                  (close/p120max_close-1)               8
2025                                  sz_vol               8
234                  (3k_high-3k_low)/3k_avg               7
1705                                  sh_vol               6
895                                hs300_vol               5
1009  p1250mv_(250k_low-250k_close)/250k_avg               4
1283           p3mv_(3k_low-3k_close)/3k_avg               4
511                       (low/p60min_low-1)               4
454                           (high-low)/avg               3
1235        p300mv_(60k_low/p60mv_60k_low-1)               2
1063     p180mv_(60k_close-60k_open)/60k_avg               2
80             (120k_high-120k_low)/120k_avg               2
26                (10k_high-10k_low)/10k_avg               2
142              (20k_low-20k_close)/20k_avg               2
1449   p600mv_(120k_low-120k_close)/120k_avg               2
1375     p500mv_(250k_avg/p250mv_250k_avg-1)               2
1471      p60mv_(60k_close-60k_open)/60k_avg               2

      importance_percent
1865                9.29
567                 7.86
342                 7.14
430                 5.71
2025                5.71
234                 5.00
1705                4.29
895                 3.57
1009                2.86
1283                2.86
511                 2.86
454                 2.14
1235                1.43
1063                1.43
80                  1.43
26                  1.43
142                 1.43
1449                1.43
1375                1.43
1471                1.43
Among 2036 features, 1976 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 107.76s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                             feature  importance_raw  importance_percent
342       (60k_high-60k_low)/60k_avg              12                8.57
454                   (high-low)/avg              10                7.14
2025                          sz_vol               9                6.43
895                        hs300_vol               9                6.43
567                              amt               9                6.43
1865                        sz50_vol               8                5.71
134       (20k_high-20k_low)/20k_avg               6                4.29
26        (10k_high-10k_low)/10k_avg               6                4.29
234          (3k_high-3k_low)/3k_avg               5                3.57
80     (120k_high-120k_low)/120k_avg               4                2.86
1125    p20mv_(5k_avg/p5mv_5k_avg-1)               3                2.14
1705                          sh_vol               3                2.14
142      (20k_low-20k_close)/20k_avg               3                2.14
1418   p5mv_(5k_low-5k_close)/5k_avg               3                2.14
1491    p6mv_(3k_high-3k_low)/3k_avg               2                1.43
337   (60k_close/p180mv_60k_close-1)               2                1.43
468           (high/p250mean_high-1)               2                1.43
2031             upper_shadow/5k_avg               2                1.43
908                           market               2                1.43
1097           p1mv_upper_shadow/avg               2                1.43
Among 2036 features, 1980 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 100.89s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
1865                            sz50_vol              20               14.29
2025                              sz_vol              20               14.29
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
22         (10k_close/p30mv_10k_close-1)              10                7.14
1                     (10MA/p1mv_10MA-1)              10                7.14
1251   p30mv_(10k_open/p10mv_10k_open-1)              10                7.14
895                            hs300_vol              10                7.14
1495        p6mv_(3k_low-3k_open)/3k_avg              10                7.14
1284        p3mv_(3k_low-3k_open)/3k_avg              10                7.14
454                       (high-low)/avg              10                7.14
1138          p20mv_lower_shadow/20k_avg              10                7.14
248               (3k_low/p6mv_3k_low-1)              10                7.14
Among 2036 features, 2024 features are not used in the model

Train custom_revenue_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 111.67s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                  feature  importance_raw  importance_percent
1865                             sz50_vol              29               20.71
1705                               sh_vol              11                7.86
2025                               sz_vol              10                7.14
1268         p360mv_lower_shadow/120k_avg              10                7.14
430               (close/p120max_close-1)              10                7.14
489                   (low/p120max_low-1)              10                7.14
1375  p500mv_(250k_avg/p250mv_250k_avg-1)              10                7.14
124             (20k_avg/p60mv_20k_avg-1)              10                7.14
1000          p120mv_upper_shadow/60k_avg              10                7.14
1061     p180mv_(60k_amt/p60mv_60k_amt-1)              10                7.14
1457      p60mv_(20k_avg/p20mv_20k_avg-1)              10                7.14
454                        (high-low)/avg               9                6.43
234               (3k_high-3k_low)/3k_avg               1                0.71
Among 2036 features, 2023 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 100.11s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              14
2025                                  sz_vol              13
430                  (close/p120max_close-1)               8
567                                      amt               6
1705                                  sh_vol               6
986      p120mv_(60k_close-60k_open)/60k_avg               4
895                                hs300_vol               4
1009  p1250mv_(250k_low-250k_close)/250k_avg               4
1449   p600mv_(120k_low-120k_close)/120k_avg               3
454                           (high-low)/avg               3
1268            p360mv_lower_shadow/120k_avg               3
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               3
1347     p480mv_(120k_low/p120mv_120k_low-1)               3
2034                                     vol               3
132            (20k_close/p80mv_20k_close-1)               2
1350            p480mv_lower_shadow/120k_avg               2
1452   p600mv_(120k_open/p120mv_120k_open-1)               2
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               2
83            (120k_high/p120mv_120k_high-1)               2
1000             p120mv_upper_shadow/60k_avg               2

      importance_percent
1865               10.00
2025                9.29
430                 5.71
567                 4.29
1705                4.29
986                 2.86
895                 2.86
1009                2.86
1449                2.14
454                 2.14
1268                2.14
1061                2.14
1347                2.14
2034                2.14
132                 1.43
1350                1.43
1452                1.43
1457                1.43
83                  1.43
1000                1.43
Among 2036 features, 1971 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 96.89s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              30               21.43
2025                              sz_vol              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)              10                7.14
538                 (open/p60min_open-1)              10                7.14
432              (close/p120min_close-1)              10                7.14
134           (20k_high-20k_low)/20k_avg              10                7.14
345         (60k_high/p120mv_60k_high-1)               9                6.43
1138          p20mv_lower_shadow/20k_avg               9                6.43
399                   (avg/p10max_avg-1)               7                5.00
303               (5k_low/p5mv_5k_low-1)               3                2.14
347         (60k_high/p240mv_60k_high-1)               1                0.71
1473  p60mv_(60k_high-60k_close)/60k_avg               1                0.71
Among 2036 features, 2021 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 100.29s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 102.50s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              20
2025                                  sz_vol              14
895                                hs300_vol              12
342               (60k_high-60k_low)/60k_avg              12
452                   (close/p60min_close-1)              10
430                  (close/p120max_close-1)              10
454                           (high-low)/avg              10
567                                      amt               8
1449   p600mv_(120k_low-120k_close)/120k_avg               7
1009  p1250mv_(250k_low-250k_close)/250k_avg               5
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               5
1705                                  sh_vol               4
1526         p80mv_(20k_low/p20mv_20k_low-1)               3
1467       p60mv_(20k_open/p20mv_20k_open-1)               2
1471      p60mv_(60k_close-60k_open)/60k_avg               2
468                   (high/p250mean_high-1)               2
1536            p9mv_(3k_high-3k_low)/3k_avg               2
234                  (3k_high-3k_low)/3k_avg               2
511                       (low/p60min_low-1)               2
1525        p80mv_(20k_low-20k_open)/20k_avg               2

      importance_percent
1865               14.29
2025               10.00
895                 8.57
342                 8.57
452                 7.14
430                 7.14
454                 7.14
567                 5.71
1449                5.00
1009                3.57
1061                3.57
1705                2.86
1526                2.14
1467                1.43
1471                1.43
468                 1.43
1536                1.43
234                 1.43
511                 1.43
1525                1.43
Among 2036 features, 2011 features are not used in the model
2013-01-01 2018-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 169181
<class 'pandas.core.frame.DataFrame'>
Int64Index: 169181 entries, 20130807 to 20180403
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 701.4 MB
None
(70, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170110       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170706       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170303       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170425       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171116       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170227       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170822       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
20171214    NaN
20171115    NaN
20170110    NaN
20170706    NaN
20170303    NaN
20170425    NaN
20170125    NaN
20171116    NaN
20170227    NaN
20170822    NaN
Train dates:20130104-20180530
(166030, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 134.93s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
1865                                sz50_vol              45
1705                                  sh_vol              36
2025                                  sz_vol              36
895                                hs300_vol              33
732                                  cyb_vol              27
2060     layer0_custom_revenue2_y_s_avg_pred              26
2134  layer0_custom_revenue_y_s_decline_pred              16
2122     layer0_custom_revenue2_y_l_avg_pred              16
1094                 p1mv_(open/p1mv_open-1)               7
453                         (high-close)/avg               7
2099    layer0_custom_revenue_y_s_avg_output               7
2074    layer0_custom_revenue2_y_l_rise_pred               6
1493           p6mv_(3k_high/p3mv_3k_high-1)               5
2048  layer0_custom_revenue_y_l_decline_pred               5
937       p100mv_(20k_open/p20mv_20k_open-1)               5
1198            p25mv_(5k_low/p5mv_5k_low-1)               4
958          p10mv_(5k_high-5k_close)/5k_avg               4
519                       (open/p1mv_open-1)               4
2110      layer0_custom_revenue_y_l_avg_pred               4
46               (10k_open/p40mv_10k_open-1)               4

      importance_percent
1865                6.00
1705                4.80
2025                4.80
895                 4.40
732                 3.60
2060                3.47
2134                2.13
2122                2.13
1094                0.93
453                 0.93
2099                0.93
2074                0.80
1493                0.67
2048                0.67
937                 0.67
1198                0.53
958                 0.53
519                 0.53
2110                0.53
46                  0.53
Among 2160 features, 1825 features are not used in the model

Train l2_y_s_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 129.51s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
1705                                     sh_vol              44
895                                   hs300_vol              44
1865                                   sz50_vol              39
2025                                     sz_vol              37
732                                     cyb_vol              28
2060        layer0_custom_revenue2_y_s_avg_pred              28
2134     layer0_custom_revenue_y_s_decline_pred              16
2122        layer0_custom_revenue2_y_l_avg_pred              11
2048     layer0_custom_revenue_y_l_decline_pred               9
2110         layer0_custom_revenue_y_l_avg_pred               8
1526            p80mv_(20k_low/p20mv_20k_low-1)               7
2086       layer0_custom_revenue2_y_s_rise_pred               7
2133  layer0_custom_revenue2_y_l_avg_tree9_leaf               6
433                        (close/p1mv_close-1)               6
937          p100mv_(20k_open/p20mv_20k_open-1)               6
2098         layer0_custom_revenue_y_s_avg_pred               5
938            p100mv_(20k_vol/p20mv_20k_vol-1)               5
1088                        p1mv_(high-low)/avg               5
1099            p20mv_(10k_avg/p10mv_10k_avg-1)               4
1202                  p25mv_upper_shadow/5k_avg               4

      importance_percent
1705                5.87
895                 5.87
1865                5.20
2025                4.93
732                 3.73
2060                3.73
2134                2.13
2122                1.47
2048                1.20
2110                1.07
1526                0.93
2086                0.93
2133                0.80
433                 0.80
937                 0.80
2098                0.67
938                 0.67
1088                0.67
1099                0.53
1202                0.53
Among 2160 features, 1851 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 126.70s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2122        layer0_custom_revenue2_y_l_avg_pred              49
1865                                   sz50_vol              37
2025                                     sz_vol              35
1705                                     sh_vol              30
732                                     cyb_vol              23
2133  layer0_custom_revenue2_y_l_avg_tree9_leaf              16
1526            p80mv_(20k_low/p20mv_20k_low-1)              14
2048     layer0_custom_revenue_y_l_decline_pred              13
895                                   hs300_vol              13
2134     layer0_custom_revenue_y_s_decline_pred              12
1466            p60mv_(20k_low/p20mv_20k_low-1)              11
567                                         amt               9
2128  layer0_custom_revenue2_y_l_avg_tree4_leaf               8
1341     p480mv_(120k_high-120k_close)/120k_avg               8
1450       p600mv_(120k_low-120k_open)/120k_avg               7
2129  layer0_custom_revenue2_y_l_avg_tree5_leaf               7
1350               p480mv_lower_shadow/120k_avg               7
2110         layer0_custom_revenue_y_l_avg_pred               6
1074                p180mv_lower_shadow/60k_avg               6
1238                p300mv_lower_shadow/60k_avg               6

      importance_percent
2122                6.53
1865                4.93
2025                4.67
1705                4.00
732                 3.07
2133                2.13
1526                1.87
2048                1.73
895                 1.73
2134                1.60
1466                1.47
567                 1.20
2128                1.07
1341                1.07
1450                0.93
2129                0.93
1350                0.93
2110                0.80
1074                0.80
1238                0.80
Among 2160 features, 1880 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 126.34s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2122         layer0_custom_revenue2_y_l_avg_pred              41
1865                                    sz50_vol              29
1705                                      sh_vol              28
2025                                      sz_vol              26
2134      layer0_custom_revenue_y_s_decline_pred              15
567                                          amt              14
2060         layer0_custom_revenue2_y_s_avg_pred              14
2074        layer0_custom_revenue2_y_l_rise_pred              13
732                                      cyb_vol              12
2048      layer0_custom_revenue_y_l_decline_pred              12
895                                    hs300_vol              10
1526             p80mv_(20k_low/p20mv_20k_low-1)               9
2083  layer0_custom_revenue2_y_l_rise_tree7_leaf               9
1463           p60mv_(20k_high/p20mv_20k_high-1)               7
568                                         area               7
2110          layer0_custom_revenue_y_l_avg_pred               7
1466             p60mv_(20k_low/p20mv_20k_low-1)               6
81                (120k_high-120k_open)/120k_avg               5
1450        p600mv_(120k_low-120k_open)/120k_avg               5
2128   layer0_custom_revenue2_y_l_avg_tree4_leaf               5

      importance_percent
2122                5.47
1865                3.87
1705                3.73
2025                3.47
2134                2.00
567                 1.87
2060                1.87
2074                1.73
732                 1.60
2048                1.60
895                 1.33
1526                1.20
2083                1.20
1463                0.93
568                 0.93
2110                0.93
1466                0.80
81                  0.67
1450                0.67
2128                0.67
Among 2160 features, 1840 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 125.44s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2086    layer0_custom_revenue2_y_s_rise_pred              32
2060     layer0_custom_revenue2_y_s_avg_pred              24
1705                                  sh_vol              23
2025                                  sz_vol              22
1865                                sz50_vol              18
2122     layer0_custom_revenue2_y_l_avg_pred              16
2074    layer0_custom_revenue2_y_l_rise_pred              15
895                                hs300_vol              13
454                           (high-low)/avg               9
2075  layer0_custom_revenue2_y_l_rise_output               7
453                         (high-close)/avg               6
2134  layer0_custom_revenue_y_s_decline_pred               6
1195          p25mv_(5k_high/p5mv_5k_high-1)               6
732                                  cyb_vol               6
2033                        upper_shadow/avg               6
568                                     area               5
1087                   p1mv_(high-close)/avg               4
2110      layer0_custom_revenue_y_l_avg_pred               4
2034                                     vol               4
1094                 p1mv_(open/p1mv_open-1)               4

      importance_percent
2086                4.27
2060                3.20
1705                3.07
2025                2.93
1865                2.40
2122                2.13
2074                2.00
895                 1.73
454                 1.20
2075                0.93
453                 0.80
2134                0.80
1195                0.80
732                 0.80
2033                0.80
568                 0.67
1087                0.53
2110                0.53
2034                0.53
1094                0.53
Among 2160 features, 1754 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 127.73s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2122         layer0_custom_revenue2_y_l_avg_pred              39
2074        layer0_custom_revenue2_y_l_rise_pred              20
568                                         area              19
1865                                    sz50_vol              17
2060         layer0_custom_revenue2_y_s_avg_pred              15
1705                                      sh_vol              14
567                                          amt              13
2025                                      sz_vol              11
2075      layer0_custom_revenue2_y_l_rise_output              10
2035                                        vol0               9
2148         layer0_custom_revenue_y_l_rise_pred               7
454                               (high-low)/avg               7
895                                    hs300_vol               6
1511         p750mv_(250k_low/p250mv_250k_low-1)               6
2134      layer0_custom_revenue_y_s_decline_pred               5
2110          layer0_custom_revenue_y_l_avg_pred               5
1456             p60mv_(20k_amt/p20mv_20k_amt-1)               5
2086        layer0_custom_revenue2_y_s_rise_pred               5
2083  layer0_custom_revenue2_y_l_rise_tree7_leaf               5
1163          p240mv_(60k_high/p60mv_60k_high-1)               4

      importance_percent
2122                5.20
2074                2.67
568                 2.53
1865                2.27
2060                2.00
1705                1.87
567                 1.73
2025                1.47
2075                1.33
2035                1.20
2148                0.93
454                 0.93
895                 0.80
1511                0.80
2134                0.67
2110                0.67
1456                0.67
2086                0.67
2083                0.67
1163                0.53
Among 2160 features, 1744 features are not used in the model
2013-01-01 2018-06-30
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 169009
<class 'pandas.core.frame.DataFrame'>
Int64Index: 169009 entries, 20130819 to 20180110
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 700.6 MB
None
(67, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170223       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170522       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170515       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170526       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170920       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171120       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170908       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
20170223    NaN
20171222    NaN
20170124    NaN
20170522    NaN
20170515    NaN
20170526    NaN
20170920    NaN
20171120    NaN
20171027    NaN
20171027    NaN
20170908    NaN
Train dates:20130104-20180530
(165884, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 151.14s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=30,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2238                  layer1_l2_y_l_avg_pred             116
2212              layer1_l2_y_l_decline_pred              93
2290                 layer1_l2_y_l_rise_pred              84
2160                  layer1_l2_y_s_avg_pred              39
2122     layer0_custom_revenue2_y_l_avg_pred              37
2186              layer1_l2_y_s_decline_pred              12
1705                                  sh_vol              11
2148     layer0_custom_revenue_y_l_rise_pred              11
2025                                  sz_vol              11
2264                 layer1_l2_y_s_rise_pred              10
1865                                sz50_vol               9
732                                  cyb_vol               9
1241         p30mv_(10k_avg/p10mv_10k_avg-1)               8
2048  layer0_custom_revenue_y_l_decline_pred               7
2134  layer0_custom_revenue_y_s_decline_pred               6
2161            layer1_l2_y_s_avg_tree0_leaf               6
895                                hs300_vol               6
568                                     area               6
2204       layer1_l2_y_s_decline_tree17_leaf               5
2254           layer1_l2_y_l_avg_tree15_leaf               4

      importance_percent
2238               12.89
2212               10.33
2290                9.33
2160                4.33
2122                4.11
2186                1.33
1705                1.22
2148                1.22
2025                1.22
2264                1.11
1865                1.00
732                 1.00
1241                0.89
2048                0.78
2134                0.67
2161                0.67
895                 0.67
568                 0.67
2204                0.56
2254                0.44
Among 2316 features, 1986 features are not used in the model
test range: 20180701 20190101
2018-07-01 2018-12-31
Time slice keys in hdf5: 2018/0101-0101

Current key: 2018/0101-0101
Current slice size(length): 347712

Total concatenating size: 347712
Result dataset size: 178346

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2     -0.121       -0.121     0.006    -0.117    -0.126
(-0.20,-0.15]      9     -0.026       -0.083     0.131     0.209    -0.139
(-0.15,-0.10]     77     -0.050       -0.083     0.120     0.318    -0.220
(-0.10,-0.05]   1589     -0.042       -0.069     0.117     0.647    -0.382
(-0.05,0.00]   25509     -0.032       -0.054     0.106     0.736    -0.612
(0.00,0.05]    98255     -0.013       -0.033     0.101     0.856    -0.617
(0.05,0.10]    38130      0.004       -0.024     0.119     0.989    -0.459
(0.10,0.15]    10684      0.023        0.049     0.143     1.029    -0.387
(0.15,0.20]     2976      0.040        0.067     0.154     0.837    -0.341
(0.20,0.25]      728      0.038        0.074     0.160     0.709    -0.311
(0.25,0.30]      103      0.053        0.107     0.166     0.432    -0.327
(0.30,0.35]       18      0.138        0.156     0.138     0.323    -0.107
(0.35,0.40]        3      0.200        0.161     0.079     0.290     0.148
(0.40,0.45]        2      0.241        0.241     0.027     0.260     0.222
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2     -0.177       -0.177     0.004    -0.174    -0.180
(-0.20,-0.15]      9     -0.038       -0.128     0.190     0.303    -0.177
(-0.15,-0.10]     77     -0.069       -0.134     0.169     0.439    -0.287
(-0.10,-0.05]   1589     -0.064       -0.110     0.160     0.852    -0.455
(-0.05,0.00]   25509     -0.050       -0.091     0.147     0.920    -0.677
(0.00,0.05]    98255     -0.024       -0.062     0.141     1.077    -0.674
(0.05,0.10]    38130     -0.001       -0.060     0.167     1.304    -0.544
(0.10,0.15]    10684      0.025        0.073     0.202     1.328    -0.452
(0.15,0.20]     2976      0.049        0.107     0.219     1.014    -0.409
(0.20,0.25]      728      0.044        0.118     0.231     0.864    -0.402
(0.25,0.30]      103      0.072        0.164     0.239     0.613    -0.381
(0.30,0.35]       18      0.211        0.246     0.190     0.420    -0.194
(0.35,0.40]        3      0.285        0.239     0.096     0.395     0.221
(0.40,0.45]        2      0.348        0.348     0.035     0.372     0.323
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2     -0.066       -0.066     0.009    -0.060    -0.072
(-0.20,-0.15]      9     -0.013       -0.037     0.074     0.136    -0.100
(-0.15,-0.10]     77     -0.031       -0.044     0.075     0.196    -0.167
(-0.10,-0.05]   1589     -0.025       -0.027     0.076     0.443    -0.309
(-0.05,0.00]   25509     -0.019       -0.017     0.067     0.563    -0.564
(0.00,0.05]    98255     -0.007       -0.006     0.064     0.670    -0.570
(0.05,0.10]    38130      0.004       -0.001     0.075     0.721    -0.403
(0.10,0.15]    10684      0.015        0.010     0.090     0.733    -0.346
(0.15,0.20]     2976      0.026        0.020     0.096     0.659    -0.291
(0.20,0.25]      728      0.021        0.023     0.098     0.554    -0.273
(0.25,0.30]      103      0.032        0.041     0.100     0.251    -0.284
(0.30,0.35]       18      0.083        0.078     0.074     0.226    -0.021
(0.35,0.40]        3      0.115        0.082     0.062     0.186     0.075
(0.40,0.45]        2      0.135        0.135     0.020     0.149     0.121
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2      0.024        0.024     0.005     0.027     0.020
(-0.20,-0.15]      9      0.085        0.035     0.118     0.303     0.002
(-0.15,-0.10]     77      0.069        0.036     0.086     0.439     0.000
(-0.10,-0.05]   1589      0.067        0.044     0.084     0.852     0.000
(-0.05,0.00]   25509      0.064        0.044     0.069     0.920     0.000
(0.00,0.05]    98255      0.068        0.048     0.071     1.077     0.000
(0.05,0.10]    38130      0.094        0.069     0.089     1.304     0.000
(0.10,0.15]    10684      0.125        0.098     0.112     1.328     0.000
(0.15,0.20]     2976      0.147        0.116     0.125     1.014     0.000
(0.20,0.25]      728      0.157        0.132     0.119     0.864     0.000
(0.25,0.30]      103      0.175        0.167     0.124     0.613     0.000
(0.30,0.35]       18      0.248        0.246     0.122     0.420     0.065
(0.35,0.40]        3      0.285        0.239     0.096     0.395     0.221
(0.40,0.45]        2      0.348        0.348     0.035     0.372     0.323
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

test period:20190101-20190701
dataset range: 20130101 20190101
2013-01-01 2018-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 186915
<class 'pandas.core.frame.DataFrame'>
Int64Index: 186915 entries, 20130607 to 20180925
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 774.9 MB
None
(95, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161216       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161208       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161025       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161010       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161117       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161014       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161226       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161020       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170324       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170707       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170405       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161202    NaN
20161216    NaN
20161208    NaN
20161205    NaN
20161025    NaN
20161026    NaN
20161010    NaN
20161027    NaN
20161117    NaN
20161102    NaN
20160927    NaN
20161214    NaN
20161014    NaN
20161226    NaN
20161020    NaN
20170324    NaN
20170707    NaN
20170926    NaN
20170926    NaN
20170405    NaN
Train dates:20130104-20181129
(183796, 2036)

----------Train layer 0----------

Train custom_revenue_y_s_rise
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 125.90s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                               feature  importance_raw  importance_percent
134         (20k_high-20k_low)/20k_avg              20               14.29
342         (60k_high-60k_low)/60k_avg              20               14.29
1865                          sz50_vol              14               10.00
2035                              vol0              13                9.29
454                     (high-low)/avg              10                7.14
234            (3k_high-3k_low)/3k_avg              10                7.14
895                          hs300_vol              10                7.14
1107  p20mv_(10k_low-10k_open)/10k_avg               9                6.43
404                (avg/p120min_avg-1)               8                5.71
1495      p6mv_(3k_low-3k_open)/3k_avg               6                4.29
405                   (avg/p1mv_avg-1)               5                3.57
1490    p6mv_(3k_high-3k_close)/3k_avg               4                2.86
1219               p2mv_(low-open)/avg               3                2.14
1210             p2mv_(amt/p1mv_amt-1)               3                2.14
424                 (avg/p60min_avg-1)               2                1.43
243            (3k_low-3k_open)/3k_avg               1                0.71
492                   (low/p1mv_low-1)               1                0.71
567                                amt               1                0.71
Among 2036 features, 2018 features are not used in the model

Train custom_revenue_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 122.07s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
80            (120k_high-120k_low)/120k_avg              20
567                                     amt              20
1446   p600mv_(120k_high-120k_low)/120k_avg              18
1000            p120mv_upper_shadow/60k_avg              16
134              (20k_high-20k_low)/20k_avg              11
1321        p40mv_(20k_avg/p20mv_20k_avg-1)              10
1507  p750mv_(250k_high-250k_open)/250k_avg              10
1509  p750mv_(250k_low-250k_close)/250k_avg               8
1465       p60mv_(20k_low-20k_open)/20k_avg               8
1466        p60mv_(20k_low/p20mv_20k_low-1)               6
538                    (open/p60min_open-1)               6
325              (60k_amt/p240mv_60k_amt-1)               4
895                               hs300_vol               2
26               (10k_high-10k_low)/10k_avg               1

      importance_percent
80                 14.29
567                14.29
1446               12.86
1000               11.43
134                 7.86
1321                7.14
1507                7.14
1509                5.71
1465                5.71
1466                4.29
538                 4.29
325                 2.86
895                 1.43
26                  0.71
Among 2036 features, 2022 features are not used in the model

Train custom_revenue2_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 118.55s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
895                               hs300_vol              20
342              (60k_high-60k_low)/60k_avg              15
1865                               sz50_vol              12
2025                                 sz_vol              12
454                          (high-low)/avg               9
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               5
567                                     amt               5
1705                                 sh_vol               4
22            (10k_close/p30mv_10k_close-1)               4
430                 (close/p120max_close-1)               4
1449  p600mv_(120k_low-120k_close)/120k_avg               4
2034                                    vol               3
462                   (high/p120min_high-1)               2
453                        (high-close)/avg               2
1102     p20mv_(10k_high-10k_close)/10k_avg               2
732                                 cyb_vol               2
1491           p6mv_(3k_high-3k_low)/3k_avg               2
1309     p40mv_(10k_close-10k_open)/10k_avg               2
2035                                   vol0               2
1490         p6mv_(3k_high-3k_close)/3k_avg               2

      importance_percent
895                14.29
342                10.71
1865                8.57
2025                8.57
454                 6.43
1125                3.57
567                 3.57
1705                2.86
22                  2.86
430                 2.86
1449                2.86
2034                2.14
462                 1.43
453                 1.43
1102                1.43
732                 1.43
1491                1.43
1309                1.43
2035                1.43
1490                1.43
Among 2036 features, 1991 features are not used in the model

Train custom_revenue2_y_l_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 122.95s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue2_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 124.93s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
567                                      amt              12
342               (60k_high-60k_low)/60k_avg               9
1705                                  sh_vol               9
2025                                  sz_vol               9
430                  (close/p120max_close-1)               8
1865                                sz50_vol               7
234                  (3k_high-3k_low)/3k_avg               6
2027                   upper_shadow/120k_avg               6
1112         p20mv_(20k_avg/p20mv_20k_avg-1)               5
134               (20k_high-20k_low)/20k_avg               4
1503  p750mv_(250k_close-250k_open)/250k_avg               3
1283           p3mv_(3k_low-3k_close)/3k_avg               3
142              (20k_low-20k_close)/20k_avg               3
438                 (close/p250mean_close-1)               3
1526         p80mv_(20k_low/p20mv_20k_low-1)               3
454                           (high-low)/avg               3
496                      (low/p250max_low-1)               2
895                                hs300_vol               2
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
1341  p480mv_(120k_high-120k_close)/120k_avg               2

      importance_percent
567                 8.57
342                 6.43
1705                6.43
2025                6.43
430                 5.71
1865                5.00
234                 4.29
2027                4.29
1112                3.57
134                 2.86
1503                2.14
1283                2.14
142                 2.14
438                 2.14
1526                2.14
454                 2.14
496                 1.43
895                 1.43
1009                1.43
1341                1.43
Among 2036 features, 1980 features are not used in the model

Train custom_revenue2_y_s_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 130.77s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
895                               hs300_vol              13
342              (60k_high-60k_low)/60k_avg              12
454                          (high-low)/avg              10
134              (20k_high-20k_low)/20k_avg               9
2025                                 sz_vol               9
567                                     amt               8
80            (120k_high-120k_low)/120k_avg               7
234                 (3k_high-3k_low)/3k_avg               6
1865                               sz50_vol               6
1418          p5mv_(5k_low-5k_close)/5k_avg               4
26               (10k_high-10k_low)/10k_avg               4
142             (20k_low-20k_close)/20k_avg               4
1705                                 sh_vol               3
1449  p600mv_(120k_low-120k_close)/120k_avg               3
1125           p20mv_(5k_avg/p5mv_5k_avg-1)               3
468                  (high/p250mean_high-1)               3
482                    (high/p60min_high-1)               2
430                 (close/p120max_close-1)               2
453                        (high-close)/avg               2
1415           p5mv_(5k_high-5k_low)/5k_avg               2

      importance_percent
895                 9.29
342                 8.57
454                 7.14
134                 6.43
2025                6.43
567                 5.71
80                  5.00
234                 4.29
1865                4.29
1418                2.86
26                  2.86
142                 2.86
1705                2.14
1449                2.14
1125                2.14
468                 2.14
482                 1.43
430                 1.43
453                 1.43
1415                1.43
Among 2036 features, 1990 features are not used in the model

Train custom_revenue_y_s_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 126.80s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
895                               hs300_vol              20
2025                                 sz_vol              20
1865                               sz50_vol              16
22            (10k_close/p30mv_10k_close-1)              10
1309     p40mv_(10k_close-10k_open)/10k_avg              10
303                  (5k_low/p5mv_5k_low-1)              10
1138             p20mv_lower_shadow/20k_avg              10
1449  p600mv_(120k_low-120k_close)/120k_avg              10
1490         p6mv_(3k_high-3k_close)/3k_avg               6
372                     (amt/p10mean_amt-1)               6
1361                  p4mv_(close-open)/avg               6
248                  (3k_low/p6mv_3k_low-1)               4
541                     (vol/p10mean_vol-1)               4
1705                                 sh_vol               4
1491           p6mv_(3k_high-3k_low)/3k_avg               4

      importance_percent
895                14.29
2025               14.29
1865               11.43
22                  7.14
1309                7.14
303                 7.14
1138                7.14
1449                7.14
1490                4.29
372                 4.29
1361                4.29
248                 2.86
541                 2.86
1705                2.86
1491                2.86
Among 2036 features, 2021 features are not used in the model

Train custom_revenue_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 122.83s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                    feature  importance_raw  \
2025                                 sz_vol              20
382                    (amt/p250mean_amt-1)              10
1536           p9mv_(3k_high-3k_low)/3k_avg              10
1347    p480mv_(120k_low/p120mv_120k_low-1)              10
430                 (close/p120max_close-1)              10
1865                               sz50_vol              10
1000            p120mv_upper_shadow/60k_avg              10
1509  p750mv_(250k_low-250k_close)/250k_avg              10
1266  p360mv_(120k_open/p120mv_120k_open-1)              10
1457        p60mv_(20k_avg/p20mv_20k_avg-1)              10
1268           p360mv_lower_shadow/120k_avg              10
1396      p50mv_(10k_high/p10mv_10k_high-1)               7
81           (120k_high-120k_open)/120k_avg               5
110                      (20MA/p2mv_20MA-1)               4
1138             p20mv_lower_shadow/20k_avg               2
1321        p40mv_(20k_avg/p20mv_20k_avg-1)               2

      importance_percent
2025               14.29
382                 7.14
1536                7.14
1347                7.14
430                 7.14
1865                7.14
1000                7.14
1509                7.14
1266                7.14
1457                7.14
1268                7.14
1396                5.00
81                  3.57
110                 2.86
1138                1.43
1321                1.43
Among 2036 features, 2020 features are not used in the model

Train custom_revenue2_y_l_avg
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 128.65s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                     feature  importance_raw  \
2025                                  sz_vol              13
1865                                sz50_vol              11
1705                                  sh_vol               7
454                           (high-low)/avg               5
986      p120mv_(60k_close-60k_open)/60k_avg               4
1347     p480mv_(120k_low/p120mv_120k_low-1)               4
1503  p750mv_(250k_close-250k_open)/250k_avg               4
895                                hs300_vol               3
567                                      amt               3
1138              p20mv_lower_shadow/20k_avg               3
1268            p360mv_lower_shadow/120k_avg               3
2034                                     vol               3
1509   p750mv_(250k_low-250k_close)/250k_avg               3
430                  (close/p120max_close-1)               3
1064    p180mv_(60k_close/p60mv_60k_close-1)               2
1009  p1250mv_(250k_low-250k_close)/250k_avg               2
79           (120k_high-120k_close)/120k_avg               2
1457         p60mv_(20k_avg/p20mv_20k_avg-1)               2
1061        p180mv_(60k_amt/p60mv_60k_amt-1)               2
489                      (low/p120max_low-1)               2

      importance_percent
2025                9.29
1865                7.86
1705                5.00
454                 3.57
986                 2.86
1347                2.86
1503                2.86
895                 2.14
567                 2.14
1138                2.14
1268                2.14
2034                2.14
1509                2.14
430                 2.14
1064                1.43
1009                1.43
79                  1.43
1457                1.43
1061                1.43
489                 1.43
Among 2036 features, 1961 features are not used in the model

Train custom_revenue_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 124.99s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
567                                  amt              41               29.29
432              (close/p120min_close-1)              10                7.14
26            (10k_high-10k_low)/10k_avg              10                7.14
1417       p5mv_(5k_high/p5mv_5k_high-1)              10                7.14
80         (120k_high-120k_low)/120k_avg              10                7.14
937   p100mv_(20k_open/p20mv_20k_open-1)               9                6.43
2025                              sz_vol               7                5.00
399                   (avg/p10max_avg-1)               7                5.00
134           (20k_high-20k_low)/20k_avg               7                5.00
441                (close/p3max_close-1)               5                3.57
185       (250k_high-250k_open)/250k_avg               3                2.14
288              (5k_high-5k_low)/5k_avg               3                2.14
1705                              sh_vol               3                2.14
1865                            sz50_vol               3                2.14
550                  (vol/p250max_vol-1)               3                2.14
303               (5k_low/p5mv_5k_low-1)               3                2.14
1120    p20mv_(20k_low-20k_open)/20k_avg               2                1.43
968            p10mv_lower_shadow/5k_avg               1                0.71
1021       p12mv_(3k_high-3k_low)/3k_avg               1                0.71
732                              cyb_vol               1                0.71
Among 2036 features, 2015 features are not used in the model

Train custom_revenue2_y_s_decline
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
Time usage: 124.68s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue2_obj at 0x0000023CFAD018C8>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
Empty DataFrame
Columns: [feature, importance_raw, importance_percent]
Index: []
Among 2036 features, 2036 features are not used in the model

Train custom_revenue_y_l_rise
[LightGBM] [Warning] Using self-defined objective function
[LightGBM] [Warning] Using self-defined objective function
Time usage: 128.04s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=2, max_depth=8, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10,
       n_jobs=-1, num_leaves=15,
       objective=<function custom_revenue_obj at 0x0000023CFAD01730>,
       random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,
       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
                                 feature  importance_raw  importance_percent
1705                              sh_vol              16               11.43
342           (60k_high-60k_low)/60k_avg              16               11.43
2025                              sz_vol              12                8.57
454                       (high-low)/avg              10                7.14
1865                            sz50_vol               8                5.71
430              (close/p120max_close-1)               8                5.71
567                                  amt               7                5.00
2027               upper_shadow/120k_avg               7                5.00
234              (3k_high-3k_low)/3k_avg               6                4.29
468               (high/p250mean_high-1)               6                4.29
496                  (low/p250max_low-1)               5                3.57
1112     p20mv_(20k_avg/p20mv_20k_avg-1)               4                2.86
1350        p480mv_lower_shadow/120k_avg               4                2.86
133         (20k_high-20k_close)/20k_avg               2                1.43
1268        p360mv_lower_shadow/120k_avg               2                1.43
895                            hs300_vol               2                1.43
2035                                vol0               2                1.43
132        (20k_close/p80mv_20k_close-1)               2                1.43
452               (close/p60min_close-1)               2                1.43
1518  p80mv_(20k_close-20k_open)/20k_avg               2                1.43
Among 2036 features, 2002 features are not used in the model
2013-01-01 2018-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 186915
<class 'pandas.core.frame.DataFrame'>
Int64Index: 186915 entries, 20130807 to 20180716
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 774.9 MB
None
(90, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160926       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161121       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161108       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161111       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161228       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161102       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161103       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161128       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171214       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170110       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170706       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170303       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170425       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170125       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171116       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170227       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170822       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161222    NaN
20160926    NaN
20161121    NaN
20161108    NaN
20161111    NaN
20161228    NaN
20161102    NaN
20161125    NaN
20161103    NaN
20161128    NaN
20171214    NaN
20171115    NaN
20170110    NaN
20170706    NaN
20170303    NaN
20170425    NaN
20170125    NaN
20171116    NaN
20170227    NaN
20170822    NaN
Train dates:20130104-20181129
(183769, 2036)

----------Layer 0 predicts----------

----------Train layer 1----------

Train l2_y_s_avg
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 150.17s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
1865                                   sz50_vol              49
895                                   hs300_vol              46
2025                                     sz_vol              42
1705                                     sh_vol              31
2060        layer0_custom_revenue2_y_s_avg_pred              23
2134     layer0_custom_revenue_y_s_decline_pred              21
732                                     cyb_vol              20
2122        layer0_custom_revenue2_y_l_avg_pred              17
937          p100mv_(20k_open/p20mv_20k_open-1)              12
1094                    p1mv_(open/p1mv_open-1)               8
2086       layer0_custom_revenue2_y_s_rise_pred               8
2070  layer0_custom_revenue2_y_s_avg_tree8_leaf               7
453                            (high-close)/avg               6
47                  (10k_open/p50mv_10k_open-1)               6
1437                   p5mv_lower_shadow/5k_avg               6
2048     layer0_custom_revenue_y_l_decline_pred               6
1219                        p2mv_(low-open)/avg               5
2075     layer0_custom_revenue2_y_l_rise_output               5
2099       layer0_custom_revenue_y_s_avg_output               5
1088                        p1mv_(high-low)/avg               5

      importance_percent
1865                6.53
895                 6.13
2025                5.60
1705                4.13
2060                3.07
2134                2.80
732                 2.67
2122                2.27
937                 1.60
1094                1.07
2086                1.07
2070                0.93
453                 0.80
47                  0.80
1437                0.80
2048                0.80
1219                0.67
2075                0.67
2099                0.67
1088                0.67
Among 2160 features, 1844 features are not used in the model

Train l2_y_s_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 155.32s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2025                                     sz_vol              51
1705                                     sh_vol              39
895                                   hs300_vol              38
1865                                   sz50_vol              38
732                                     cyb_vol              27
2060        layer0_custom_revenue2_y_s_avg_pred              26
2134     layer0_custom_revenue_y_s_decline_pred              21
2048     layer0_custom_revenue_y_l_decline_pred              12
937          p100mv_(20k_open/p20mv_20k_open-1)              12
2122        layer0_custom_revenue2_y_l_avg_pred              11
2070  layer0_custom_revenue2_y_s_avg_tree8_leaf               7
2098         layer0_custom_revenue_y_s_avg_pred               7
1189               p25mv_(5k_avg/p5mv_5k_avg-1)               6
1457            p60mv_(20k_avg/p20mv_20k_avg-1)               6
2099       layer0_custom_revenue_y_s_avg_output               6
2132  layer0_custom_revenue2_y_l_avg_tree8_leaf               5
88               (120k_low-120k_close)/120k_avg               5
2086       layer0_custom_revenue2_y_s_rise_pred               5
1454               p600mv_lower_shadow/120k_avg               5
1463          p60mv_(20k_high/p20mv_20k_high-1)               4

      importance_percent
2025                6.80
1705                5.20
895                 5.07
1865                5.07
732                 3.60
2060                3.47
2134                2.80
2048                1.60
937                 1.60
2122                1.47
2070                0.93
2098                0.93
1189                0.80
1457                0.80
2099                0.80
2132                0.67
88                  0.67
2086                0.67
1454                0.67
1463                0.53
Among 2160 features, 1864 features are not used in the model

Train l2_y_l_decline
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 168.36s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2025                                     sz_vol              38
2122        layer0_custom_revenue2_y_l_avg_pred              35
1705                                     sh_vol              31
1865                                   sz50_vol              29
732                                     cyb_vol              21
1526            p80mv_(20k_low/p20mv_20k_low-1)              19
2134     layer0_custom_revenue_y_s_decline_pred              15
895                                   hs300_vol              15
927            p100mv_(20k_avg/p20mv_20k_avg-1)              13
2132  layer0_custom_revenue2_y_l_avg_tree8_leaf              12
2048     layer0_custom_revenue_y_l_decline_pred              12
1466            p60mv_(20k_low/p20mv_20k_low-1)              11
1321            p40mv_(20k_avg/p20mv_20k_avg-1)               9
1454               p600mv_lower_shadow/120k_avg               7
1346       p480mv_(120k_low-120k_open)/120k_avg               7
1238                p300mv_lower_shadow/60k_avg               7
2110         layer0_custom_revenue_y_l_avg_pred               7
937          p100mv_(20k_open/p20mv_20k_open-1)               6
924               p1000mv_lower_shadow/250k_avg               5
1062           p180mv_(60k_avg/p60mv_60k_avg-1)               5

      importance_percent
2025                5.07
2122                4.67
1705                4.13
1865                3.87
732                 2.80
1526                2.53
2134                2.00
895                 2.00
927                 1.73
2132                1.60
2048                1.60
1466                1.47
1321                1.20
1454                0.93
1346                0.93
1238                0.93
2110                0.93
937                 0.80
924                 0.67
1062                0.67
Among 2160 features, 1862 features are not used in the model

Train l2_y_l_avg
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 157.90s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                        feature  importance_raw  \
2122        layer0_custom_revenue2_y_l_avg_pred              32
1705                                     sh_vol              26
2025                                     sz_vol              25
1865                                   sz50_vol              24
567                                         amt              16
2134     layer0_custom_revenue_y_s_decline_pred              14
1466            p60mv_(20k_low/p20mv_20k_low-1)              14
2074       layer0_custom_revenue2_y_l_rise_pred              13
2060        layer0_custom_revenue2_y_s_avg_pred              13
732                                     cyb_vol              13
895                                   hs300_vol              11
2048     layer0_custom_revenue_y_l_decline_pred              11
2148        layer0_custom_revenue_y_l_rise_pred              11
1526            p80mv_(20k_low/p20mv_20k_low-1)              10
937          p100mv_(20k_open/p20mv_20k_open-1)              10
1463          p60mv_(20k_high/p20mv_20k_high-1)               9
1454               p600mv_lower_shadow/120k_avg               7
2075     layer0_custom_revenue2_y_l_rise_output               7
2132  layer0_custom_revenue2_y_l_avg_tree8_leaf               7
2086       layer0_custom_revenue2_y_s_rise_pred               7

      importance_percent
2122                4.27
1705                3.47
2025                3.33
1865                3.20
567                 2.13
2134                1.87
1466                1.87
2074                1.73
2060                1.73
732                 1.73
895                 1.47
2048                1.47
2148                1.47
1526                1.33
937                 1.33
1463                1.20
1454                0.93
2075                0.93
2132                0.93
2086                0.93
Among 2160 features, 1842 features are not used in the model

Train l2_y_s_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 157.86s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2086        layer0_custom_revenue2_y_s_rise_pred              29
2060         layer0_custom_revenue2_y_s_avg_pred              24
1865                                    sz50_vol              19
2025                                      sz_vol              19
1705                                      sh_vol              18
2122         layer0_custom_revenue2_y_l_avg_pred              16
2134      layer0_custom_revenue_y_s_decline_pred              13
895                                    hs300_vol              12
2075      layer0_custom_revenue2_y_l_rise_output              11
2033                            upper_shadow/avg               9
453                             (high-close)/avg               8
732                                      cyb_vol               7
2074        layer0_custom_revenue2_y_l_rise_pred               6
454                               (high-low)/avg               5
2080  layer0_custom_revenue2_y_l_rise_tree4_leaf               5
568                                         area               5
1545                    p9mv_upper_shadow/3k_avg               5
1087                       p1mv_(high-close)/avg               4
2148         layer0_custom_revenue_y_l_rise_pred               4
1437                    p5mv_lower_shadow/5k_avg               4

      importance_percent
2086                3.87
2060                3.20
1865                2.53
2025                2.53
1705                2.40
2122                2.13
2134                1.73
895                 1.60
2075                1.47
2033                1.20
453                 1.07
732                 0.93
2074                0.80
454                 0.67
2080                0.67
568                 0.67
1545                0.67
1087                0.53
2148                0.53
1437                0.53
Among 2160 features, 1760 features are not used in the model

Train l2_y_l_rise
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 157.60s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.2, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                         feature  importance_raw  \
2122         layer0_custom_revenue2_y_l_avg_pred              35
568                                         area              21
2075      layer0_custom_revenue2_y_l_rise_output              18
2025                                      sz_vol              17
1865                                    sz50_vol              16
2074        layer0_custom_revenue2_y_l_rise_pred              15
567                                          amt              15
2148         layer0_custom_revenue_y_l_rise_pred              13
2060         layer0_custom_revenue2_y_s_avg_pred              11
2035                                        vol0               9
895                                    hs300_vol               7
2134      layer0_custom_revenue_y_s_decline_pred               6
2081  layer0_custom_revenue2_y_l_rise_tree5_leaf               5
1705                                      sh_vol               5
995           p120mv_(60k_open/p60mv_60k_open-1)               5
1466             p60mv_(20k_low/p20mv_20k_low-1)               5
1517             p80mv_(20k_avg/p20mv_20k_avg-1)               5
1456             p60mv_(20k_amt/p20mv_20k_amt-1)               5
931            p100mv_(20k_high-20k_low)/20k_avg               5
1235            p300mv_(60k_low/p60mv_60k_low-1)               5

      importance_percent
2122                4.67
568                 2.80
2075                2.40
2025                2.27
1865                2.13
2074                2.00
567                 2.00
2148                1.73
2060                1.47
2035                1.20
895                 0.93
2134                0.80
2081                0.67
1705                0.67
995                 0.67
1466                0.67
1517                0.67
1456                0.67
931                 0.67
1235                0.67
Among 2160 features, 1767 features are not used in the model
2013-01-01 2018-12-31
Time slice keys in hdf5: 2013/0101-0101,2014/0101-0101,2015/0101-0101,2016/0101-0101,2017/0101-0101,2018/0101-0101

Current key: 2013/0101-0101
Current slice size(length): 278061
Current subsample size(length): 27806

Current key: 2014/0101-0101
Current slice size(length): 291574
Current subsample size(length): 29157

Current key: 2015/0101-0101
Current slice size(length): 304680
Current subsample size(length): 30468

Current key: 2016/0101-0101
Current slice size(length): 313767
Current subsample size(length): 31376

Current key: 2017/0101-0101
Current slice size(length): 333371
Current subsample size(length): 33337

Current key: 2018/0101-0101
Current slice size(length): 347712
Current subsample size(length): 34771

Total concatenating size: 186915
Result dataset size: 186915
<class 'pandas.core.frame.DataFrame'>
Int64Index: 186915 entries, 20130819 to 20180110
Columns: 2037 entries, (10MA/20MA-1) to vol0
dtypes: float16(1987), float64(44), int64(1), uint8(5)
memory usage: 774.9 MB
None
(95, 8)
          y_l_rise  y_l_decline  y_l_avg  y_s_rise  y_s_decline  y_s_avg  y_l  \
date
20161201       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161115       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160928       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161202       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20160927       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161209       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161205       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20161026       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170223       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171222       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170124       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170522       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170515       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170526       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170920       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171120       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20171027       0.0          0.0      NaN       0.0          0.0      NaN  0.0
20170908       0.0          0.0      NaN       0.0          0.0      NaN  0.0

          y_l_r
date
20161201    NaN
20161115    NaN
20160928    NaN
20161124    NaN
20161202    NaN
20160927    NaN
20161209    NaN
20161205    NaN
20161026    NaN
20170223    NaN
20171222    NaN
20170124    NaN
20170522    NaN
20170515    NaN
20170526    NaN
20170920    NaN
20171120    NaN
20171027    NaN
20171027    NaN
20170908    NaN
Train dates:20130104-20181129
(183806, 2036)

----------Layer 0 predicts----------

----------Layer 1 predicts----------

----------Train layer 2----------

Train l2_y_l_r
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\lightgbm\basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is ['area', 'exchange', 'is_hs', 'market']
  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
Time usage: 196.03s
LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
       learning_rate=0.1, max_depth=12, min_child_samples=30,
       min_child_weight=0.001, min_split_gain=0.0, n_estimators=30,
       n_jobs=-1, num_leaves=31, objective=None, random_state=0,
       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,
       subsample_for_bin=200000, subsample_freq=0)
                                      feature  importance_raw  \
2238                   layer1_l2_y_l_avg_pred             113
2212               layer1_l2_y_l_decline_pred             102
2290                  layer1_l2_y_l_rise_pred              75
2160                   layer1_l2_y_s_avg_pred              57
2122      layer0_custom_revenue2_y_l_avg_pred              39
2048   layer0_custom_revenue_y_l_decline_pred              25
895                                 hs300_vol              14
2025                                   sz_vol              13
1865                                 sz50_vol              13
2186               layer1_l2_y_s_decline_pred               9
2134   layer0_custom_revenue_y_s_decline_pred               8
2312           layer1_l2_y_l_rise_tree21_leaf               7
568                                      area               7
2148      layer0_custom_revenue_y_l_rise_pred               6
2264                  layer1_l2_y_s_rise_pred               6
2300            layer1_l2_y_l_rise_tree9_leaf               6
1705                                   sh_vol               6
134                (20k_high-20k_low)/20k_avg               5
2240             layer1_l2_y_l_avg_tree1_leaf               4
1005  p1250mv_(250k_high-250k_close)/250k_avg               4

      importance_percent
2238               12.56
2212               11.33
2290                8.33
2160                6.33
2122                4.33
2048                2.78
895                 1.56
2025                1.44
1865                1.44
2186                1.00
2134                0.89
2312                0.78
568                 0.78
2148                0.67
2264                0.67
2300                0.67
1705                0.67
134                 0.56
2240                0.44
1005                0.44
Among 2316 features, 2015 features are not used in the model
test range: 20190101 20190701
2019-01-01 2019-06-30
Time slice keys in hdf5: 2019/0101-0101

Current key: 2019/0101-0101
Current slice size(length): 69168

Total concatenating size: 69168
Result dataset size: 69168

--------------------Predict--------------------
layer 0

----------Layer 0 predicts----------
layer 1

----------Layer 1 predicts----------
layer 2

----------Layer 2 predicts----------

y_l_r
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2      0.098        0.098     0.013     0.108     0.089
(-0.20,-0.15]     28      0.311        0.265     0.258     1.254     0.078
(-0.15,-0.10]   1712      0.151        0.149     0.163     1.231    -0.267
(-0.10,-0.05]   9246      0.126        0.117     0.162     1.586    -0.294
(-0.05,0.00]   18051      0.114        0.101     0.143     1.402    -0.298
(0.00,0.05]     9756      0.135        0.116     0.151     1.331    -0.402
(0.05,0.10]     1122      0.187        0.174     0.218     1.047    -0.278
(0.10,0.15]      287      0.232        0.272     0.243     0.823    -0.283
(0.15,0.20]       58      0.373        0.417     0.248     0.725    -0.211
(0.20,0.25]        2      0.313        0.313     0.166     0.431     0.196
(0.25,0.30]        0        NaN          NaN       NaN       NaN       NaN
(0.30,0.35]        0        NaN          NaN       NaN       NaN       NaN
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN
C:\Users\dell-pc\Anaconda3.6\lib\site-packages\matplotlib\pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)

y_l
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2      0.162        0.162     0.012     0.170     0.153
(-0.20,-0.15]     28      0.430        0.380     0.343     1.672     0.132
(-0.15,-0.10]   1712      0.202        0.213     0.231     1.594    -0.366
(-0.10,-0.05]   9246      0.165        0.171     0.228     2.227    -0.392
(-0.05,0.00]   18051      0.151        0.145     0.199     1.878    -0.429
(0.00,0.05]     9756      0.179        0.161     0.206     1.686    -0.501
(0.05,0.10]     1122      0.238        0.243     0.306     1.329    -0.391
(0.10,0.15]      287      0.298        0.376     0.345     1.194    -0.388
(0.15,0.20]       58      0.508        0.559     0.360     1.097    -0.301
(0.20,0.25]        2      0.446        0.446     0.167     0.565     0.328
(0.25,0.30]        0        NaN          NaN       NaN       NaN       NaN
(0.30,0.35]        0        NaN          NaN       NaN       NaN       NaN
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_avg
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2      0.035        0.035     0.014     0.045     0.025
(-0.20,-0.15]     28      0.192        0.154     0.176     0.836     0.008
(-0.15,-0.10]   1712      0.094        0.084     0.104     0.895    -0.197
(-0.10,-0.05]   9246      0.079        0.062     0.105     0.976    -0.203
(-0.05,0.00]   18051      0.073        0.056     0.093     1.002    -0.196
(0.00,0.05]     9756      0.087        0.069     0.101     0.980    -0.303
(0.05,0.10]     1122      0.126        0.105     0.143     0.824    -0.165
(0.10,0.15]      287      0.155        0.171     0.156     0.574    -0.179
(0.15,0.20]       58      0.234        0.269     0.152     0.498    -0.121
(0.20,0.25]        2      0.181        0.181     0.164     0.297     0.065
(0.25,0.30]        0        NaN          NaN       NaN       NaN       NaN
(0.30,0.35]        0        NaN          NaN       NaN       NaN       NaN
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

y_l_rise
               count  eval_mean  eval_median  eval_std  eval_max  eval_min
pred_range
(-1.00,-0.95]      0        NaN          NaN       NaN       NaN       NaN
(-0.95,-0.90]      0        NaN          NaN       NaN       NaN       NaN
(-0.90,-0.85]      0        NaN          NaN       NaN       NaN       NaN
(-0.85,-0.80]      0        NaN          NaN       NaN       NaN       NaN
(-0.80,-0.75]      0        NaN          NaN       NaN       NaN       NaN
(-0.75,-0.70]      0        NaN          NaN       NaN       NaN       NaN
(-0.70,-0.65]      0        NaN          NaN       NaN       NaN       NaN
(-0.65,-0.60]      0        NaN          NaN       NaN       NaN       NaN
(-0.60,-0.55]      0        NaN          NaN       NaN       NaN       NaN
(-0.55,-0.50]      0        NaN          NaN       NaN       NaN       NaN
(-0.50,-0.45]      0        NaN          NaN       NaN       NaN       NaN
(-0.45,-0.40]      0        NaN          NaN       NaN       NaN       NaN
(-0.40,-0.35]      0        NaN          NaN       NaN       NaN       NaN
(-0.35,-0.30]      0        NaN          NaN       NaN       NaN       NaN
(-0.30,-0.25]      0        NaN          NaN       NaN       NaN       NaN
(-0.25,-0.20]      2      0.162        0.162     0.012     0.170     0.153
(-0.20,-0.15]     28      0.430        0.380     0.343     1.672     0.132
(-0.15,-0.10]   1712      0.243        0.213     0.177     1.594     0.000
(-0.10,-0.05]   9246      0.208        0.172     0.178     2.227     0.000
(-0.05,0.00]   18051      0.185        0.147     0.159     1.878     0.000
(0.00,0.05]     9756      0.205        0.161     0.173     1.686     0.000
(0.05,0.10]     1122      0.297        0.244     0.235     1.329     0.000
(0.10,0.15]      287      0.367        0.376     0.250     1.194     0.000
(0.15,0.20]       58      0.550        0.559     0.282     1.097     0.044
(0.20,0.25]        2      0.446        0.446     0.167     0.565     0.328
(0.25,0.30]        0        NaN          NaN       NaN       NaN       NaN
(0.30,0.35]        0        NaN          NaN       NaN       NaN       NaN
(0.35,0.40]        0        NaN          NaN       NaN       NaN       NaN
(0.40,0.45]        0        NaN          NaN       NaN       NaN       NaN
(0.45,0.50]        0        NaN          NaN       NaN       NaN       NaN
(0.50,0.55]        0        NaN          NaN       NaN       NaN       NaN
(0.55,0.60]        0        NaN          NaN       NaN       NaN       NaN
(0.60,0.65]        0        NaN          NaN       NaN       NaN       NaN
(0.65,0.70]        0        NaN          NaN       NaN       NaN       NaN
(0.70,0.75]        0        NaN          NaN       NaN       NaN       NaN
(0.75,0.80]        0        NaN          NaN       NaN       NaN       NaN
(0.80,0.85]        0        NaN          NaN       NaN       NaN       NaN
(0.85,0.90]        0        NaN          NaN       NaN       NaN       NaN
(0.90,0.95]        0        NaN          NaN       NaN       NaN       NaN
(0.95,1.00]        0        NaN          NaN       NaN       NaN       NaN

Process finished with exit code 0
